{"cells":[{"cell_type":"markdown","metadata":{"id":"l1Fs0GL_mVgn"},"source":["# ***Progetto CV&DL - Alex Giacomini & Denis Bernovschi***"]},{"cell_type":"markdown","metadata":{"id":"ox6IFmrBfpP-"},"source":["## IMPORT"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20572,"status":"ok","timestamp":1656925267354,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"mwFSXnnkfot7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6236562-8407-4823-b5f9-77e3e91a8aeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 585 kB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 72.8 MB/s \n","\u001b[K     |████████████████████████████████| 419 kB 80.2 MB/s \n","\u001b[K     |████████████████████████████████| 140 kB 87.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 76.7 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 74.7 MB/s \n","\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 81.9 MB/s \n","\u001b[?25h"]}],"source":["import sys\n","import subprocess\n","if 'google.colab' in sys.modules:\n","  subprocess.call(\"pip install -U progress\".split())\n","\n","#---- RANDOM ------\n","import random\n","from random import randrange\n","random.seed( 40 )\n","\n","#---- NUMPY -------\n","import pandas as pd\n","import numpy as np\n","import math\n","from pandas.compat._optional import import_optional_dependency\n","\n","#----- OS ---------\n","import os\n","from os import path\n","\n","#---- DRIVE --------\n","from google.colab import drive\n","\n","#----- SCYPY -------\n","import scipy.ndimage\n","\n","#----- MATPLOT LIB -----\n","import matplotlib.pyplot as plt\n","\n","#------- TORCH VISION -------\n","from torchsummary import summary\n","from torchvision import transforms, datasets, utils \n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import models\n","from torchvision.utils import save_image\n","from torch import optim \n","from torch.optim import Adam, SGD, RMSprop \n","import torchvision, torch \n","import torch.nn as nn\n","import torchvision as tv\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import torch.autograd as autograd\n","from torchvision.transforms.transforms import CenterCrop\n","\n","\n","#----- PIL --------\n","from PIL import Image, ImageOps\n","\n","#----- SKLEARN ----- \n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score,classification_report, f1_score, precision_score, recall_score, confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","import sklearn.metrics as metrics\n","from sklearn.utils.class_weight import compute_sample_weight\n","from sklearn.utils import class_weight\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.model_selection import train_test_split\n","import keras.backend as K #------ valutare/testare se serve ancora\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.metrics import confusion_matrix\n","\n","\n","#----- IMPORT  PYTORCH ----------\n","!pip install pytorch-lightning -q \n","from pytorch_lightning.callbacks import Callback, ModelCheckpoint, EarlyStopping\n","from pytorch_lightning import Trainer\n","\n","\n","#----- IMPORT CV2 -----------\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","#----- ALTRO ------------\n","#LINK UTILE : https://github.com/chongwar/vgg16-pytorch/blob/master/vgg16_transfer_learning.py\n","#LINK UTILE : https://www.analyticsvidhya.com/blog/2019/10/how-to-master-transfer-learning-using-pytorch/ \n","from functools import reduce\n","\n","#----- SKIIMAGE -----------\n","from skimage import io\n","from skimage.io import imread\n","from skimage.color import rgb2gray, gray2rgb\n","\n","#WGAN-GP APPROACH - imports\n","#https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py\n","import argparse\n","\n","#---- PROGRESS BAR -------\n","from tqdm import tqdm\n","\n","\n","# per attuare il D.A. secondo il link di Riccardo\n","#!pip install https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip --quiet --ignore-installed #--- qui va in errore, serviva per creare il batch rappresentativo\n","#from torchsampler import ImbalancedDatasetSampler\n","\n","\n","\n","device = torch.device(\"cpu\")"]},{"cell_type":"code","source":["'''vecchio costrutto'''\n","#random.seed( 40 )\n","\n","'''nuovo costrutto'''\n","def fix_seeds(seed: int) -> None:\n","  \"\"\" Fix random seeds for numpy, tensorflow, random, etc.\n","\n","  Parameters\n","  -----------\n","  seed : int.\n","  Random seed.\n","  \"\"\"\n","\n","  np.random.seed(seed) # numpy seed\n","  tf.random.set_seed(seed) # tensorflow seed\n","  random.seed(seed) # random seed\n","  os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n","  os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n","  os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","fix_seeds(40)"],"metadata":{"id":"o4vzL4HozssK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VgAYZM1NDIRj"},"source":["## Custom TO CATEGORICAL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZLOAK3RDLMR"},"outputs":[],"source":["#from tensorflow.keras.utils import to_categorical \n","\n","def custom_to_categorical(y, num_classes):\n","    \"\"\" 1-hot encodes a tensor \"\"\"\n","    return np.eye(num_classes, dtype='uint8')[y]"]},{"cell_type":"markdown","metadata":{"id":"zUvw3mwvfCMA"},"source":["## DRIVE "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21017,"status":"ok","timestamp":1656925288363,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kkWUwK_IfBtx","outputId":"c7095c5c-6a86-43a2-c0d0-a1041b700c2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive', force_remount=True)\n","path_drive = '/content/drive/My Drive/'\n","path_progettoDL = path_drive+'ProgettoDL/'"]},{"cell_type":"markdown","metadata":{"id":"YEzVkjhHbISE"},"source":["## Parametri Immagini "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSyxD4LWbG3J"},"outputs":[],"source":["'''DEFINE VARIABLES AND PARAMETERS TO COLLECT THE INFORMATIONS FROM GOOGLE DRIVE'''\n","'''define a path for the collection of informations (CSV file) for the creation of the dataframe'''\n","os.chdir(path_progettoDL) \n","\n","'''to have always the same sequence of randomized values (numbers)'''\n","random_state = 3  \n","\n","'''some useful parameters and variables'''\n","cnn = \"vgg16\" \n","\n","'''series of production & quality classes of the wood rifle butt'''\n","#classi = ['1','2','3','4']    \n","classi = ['1','2-','2','2+','3-','3','3+','4-','4','4+']          \n","serie = [2,4,8,10,6,9,3,11,12,13,14,15,7] \n","\n","'''size of the images & their paths (location) '''\n","#immg_rows = 270 \n","#immg_cols = 470\n","#immgs = '{}_{}'.format(parte,tipo)\n","#path_imgs = os.path.join(path_drive+'{}'.format(immgs))\n","\n","#CSV loading (reading annotations/attributes/informations)\n","csv = pd.read_csv(('/content/drive/MyDrive/ProgettoDL/20201102_ExportDB.txt'), sep=\";\")\n","\n","#CROP o NO CROP \n","type_img = 'CROP' \n","if type_img == 'CROP':\n","  path_images = '/content/drive/MyDrive/CALCIO_CROP_BASE/'\n","else:\n","  path_images = '/content/drive/MyDrive/CALCIO_NOPRE/'\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xuS19wSXbdE8"},"source":["## SPLIT DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0JSykxbbeCU"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","import custom_split_data \n","from custom_split_data import split_data"]},{"cell_type":"markdown","metadata":{"id":"G4OSrOanfe7I"},"source":["## CUSTOM DATASET + TRANSFORMS D.A. "]},{"cell_type":"markdown","metadata":{"id":"Wnckk5UdrJbn"},"source":["#### TRANSFORMS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBukxX0brIpY"},"outputs":[],"source":["\n","#ESEMPI DI TRASFROMAZIONI (PER D.A.)\n","#https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n","\n","#LINK DI TRASFORMAZIONI\n","#https://pytorch.org/vision/stable/transforms.html\n","\n","\n","_transform_1 = transforms.Compose([\n","  transforms.ToPILImage(),\n","  #transforms.CenterCrop(0.75 * 64),\n","  transforms.Resize((270, 470)),            \n","  #T.RandomResizedCrop(image_size),\n","  transforms.RandomHorizontalFlip(p=1),\n","  transforms.ToTensor()])#,\n","  #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link])\n","\n","_transform_2 = transforms.Compose([\n","  transforms.ToPILImage(),\n","  #transforms.CenterCrop(0.75 * 64),\n","  transforms.Resize((270, 470)),            \n","  #T.RandomResizedCrop(image_size),\n","  transforms.RandomVerticalFlip(p=1),\n","  transforms.ToTensor()])#,\n","  #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link])\n","\n","_transform_3 = transforms.Compose([\n","  transforms.ToPILImage(),\n","  #transforms.CenterCrop(0.75 * 64),\n","  transforms.Resize((270, 470)),  \n","  transforms.ColorJitter(brightness=[.7,1.3]),  \n","  transforms.RandomPosterize(bits=2, p=1),  #ALTERNATIVA1, bit da mantenere per ogni canale RGB, (0-8 range permesso).  -- ALTERNATIVE DA TESTARE\n","  #transforms.RandomRotation(degrees=(-20, 20), expand=True, PIL.Image.NEAREST ),   #ALTERNATIVA2       \n","  #T.RandomResizedCrop(image_size),\n","  #transforms.RandomHorizontalFlip(),\n","  transforms.ToTensor()])#,\n","  #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link])\n","\n","\n","_transform_4 = transforms.Compose([                   #PER ESTRARRE patch di dimensione DIMEZZATA e farla in scala di grigi\n","        transforms.ToPILImage(),\n","        transforms.Resize((270, 470)), \n","        transforms.CenterCrop((135,235)), \n","        transforms.Grayscale(num_output_channels=1),  #VALUTA SE DIMINUIRE IL CROP A MENO PIXELS O ANCHE PER FORME RETTANGOLARI/QUADRATE\n","        transforms.ToTensor()])\n","\n","_transform_5 = transforms.Compose([\n","  transforms.ToPILImage(),\n","  #transforms.CenterCrop(0.75 * 64),\n","  transforms.Resize((270, 470)),            \n","  #T.RandomResizedCrop(image_size),\n","  transforms.RandomRotation((-30,+30)),\n","  transforms.ToTensor()])#,\n","  #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link])\n","\n","\n","\n","# trasform generale per fare l'allenamento (senza GAN)\n","_transform_ = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((270, 470)),                                            \n","        transforms.ToTensor()])#,\n","        #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link\n","\n","\n","\n","#METODO PER PREPARARE LE IMMAGINI ALLA ARCHITETTURA GAN\n","_transform_GAN = transforms.Compose([                     #normalizzazione per GAN\n","        transforms.ToPILImage(),\n","        transforms.Resize((270, 470)), \n","        #transforms.CenterCrop((256,256)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5], [0.5])])              #VALORI DI NORMALIZZAZIONE IN QUESTO MODO PORTANO I VALORI FINALI A DIVENTARE IN UN RANGE TRA [-1 E 1]\n","\n","\n","#TRANSFORM PER TRAIN DOVE OGNUNO SI ATTIVA CON PROBABILITA' DEL 50% COSì DA OTTENERE PIU' COMBINAZIONI POSSIBILI\n","_transform_train = transforms.Compose(   #usato per ricavare i nomi delle nuove immagini generate con la GAN\n","    [transforms.ToPILImage(),\n","     transforms.Resize((270, 470)), \n","     transforms.RandomHorizontalFlip(p=0.5),\n","     transforms.RandomVerticalFlip(p=0.5),\n","     transforms.ColorJitter(brightness=[.7,1.3]),  \n","     transforms.RandomPosterize(bits=2, p=0.4),                                           \n","     transforms.ToTensor(),\n","     transforms.Normalize([0.5], [0.5])]\n","     )"]},{"cell_type":"markdown","metadata":{"id":"sr7QNs-GZ12S"},"source":["NORMALIZE ------\n","* [Mezza spiegazione](https://discuss.pytorch.org/t/understanding-transform-normalize/21730)\n","* [LINK REPO](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize)\n","\n","https://deeplizard.com/learn/video/lu7TCu7HeYc\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1656925308005,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"LKDjjzqtZLNp","outputId":"59b51bdd-f1ef-434e-b064-04d0ee592143"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor before Normalize:\n"," tensor([1., 2., 3., 4., 5.])\n","Mean, Std and Var before Normalize:\n"," tensor(3.) tensor(1.5811) tensor(2.5000)\n","Tensor after Normalize:\n"," tensor([-1.2649, -0.6325,  0.0000,  0.6325,  1.2649])\n","Mean, std and Var after normalize:\n"," tensor(0.) tensor(1.) tensor(1.)\n"]}],"source":["# Python program to normalize a tensor to\n","# 0 mean and 1 variance\n","# Step 1: Importing torch\n","import torch\n","  \n","# Step 2: creating a torch tensor\n","t = torch.tensor([1.,2.,3.,4.,5.])\n","print(\"Tensor before Normalize:\\n\", t)\n","  \n","# Step 3: Computing the mean, std and variance\n","mean, std, var = torch.mean(t), torch.std(t), torch.var(t)\n","print(\"Mean, Std and Var before Normalize:\\n\", \n","      mean, std, var)\n","  \n","# Step 4: Normalizing the tensor\n","t  = (t-mean)/std\n","\n","\n","print(\"Tensor after Normalize:\\n\", t)\n","  \n","# Step 5: Again compute the mean, std and variance\n","# after Normalize\n","mean, std, var = torch.mean(t), torch.std(t), torch.var(t)\n","print(\"Mean, std and Var after normalize:\\n\", \n","      mean, std, var)"]},{"cell_type":"markdown","metadata":{"id":"z6DNhY9LrOZF"},"source":["#### CUSTOM DATASET "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScS3q5MmWMJg"},"outputs":[],"source":["class CustomDataset(Dataset):\n","  def __init__ (self, dataframe, transform_0=None, transform_1 = None, transform_2 = None, transform_3 = None, weight=None, mode = None):\n","    self.transform_0 = transform_0\n","    self.transform_1 = transform_1\n","    self.transform_2 = transform_2\n","    self.transform_3 = transform_3\n","    self.mode = mode\n","    self.weight = weight\n","    self.dataframe = dataframe\n","  def __len__(self):\n","    return len(self.dataframe)\n","  \n","  #ho dovuto aggiungerlo perché l'imbalance dataset sampler lo chiedeva \n","  def get_labels(self):\n","    print(self.dataframe['class'])\n","    return self.dataframe['class']\n","\n","  def __getitem__(self, index):\n","    path = self.dataframe.iloc[index, 2]\n","    img_path = os.path.join(path_images+path)\n","    image = io.imread(img_path)\n","    y_label_class = torch.tensor(int(self.dataframe.iloc[index, 3]))              \n","    y_label_series = torch.tensor(int(self.dataframe.iloc[index, 1]))\n","\n","    #da usare solo se aumentiamo il set offline, ma conviene senza troppe modifiche di creare un transform e mettere quei stessi metodi come probabilità a 0.5 così si possono anche combinare\n","    if self.mode == 'train':                            #questi 3 if servono solo per Augmentation Online se si usano le tecniche base di Computer Visioni di questi transform\n","      if self.dataframe.iloc[index, 4] == 'T0': \n","        #print('transform T1')\n","        image = self.transform_0(image)\n","      if self.dataframe.iloc[index, 4] == 'T1': \n","        #print('transform T1')\n","        image = self.transform_1(image)\n","      if self.dataframe.iloc[index, 4] == 'T2':\n","        #print('transform T2')\n","        image = self.transform_2(image)\n","      if self.dataframe.iloc[index, 4] == 'T3':\n","        #print('transform T3')\n","        image = self.transform_3(image)\n","    else:\n","      image = self.transform_0(image)\n","\n","      \n","    \n","    #plt.imshow(image.numpy()[0], cmap='gray')\n","    \n","    return (image, y_label_class, y_label_series)"]},{"cell_type":"markdown","metadata":{"id":"28i0TbSpzt_Z"},"source":["## NETWORK "]},{"cell_type":"markdown","metadata":{"id":"o3iWx_QMIVQp"},"source":["### Model Pre-Trained - per classificazione"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["38b8c5002b544143b663b2df73691646","8d343692141b458cac84bdf864b816a7","c0b3209ce4834e7a8429844c4eafd6ee","b3dcbcbfd74b4b3f9124a51656494dbc","f2708be5de65451490d7b5d4ad60ca49","7cb573b5fc45423b8b6d2fdf6ac552eb","6067d8bbaaed40dcaea7c94d2859954d","20ef333fa2554c68a91d514af5f593e3","625336ef685c42fbb86f4703590d5d13","c68a3f24d8714f18b10191eb8424cce4","ac304c2ccac64bdb811b36e6d9e9debf"]},"executionInfo":{"elapsed":13793,"status":"ok","timestamp":1656925334328,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"q0Dc7ahEzpWW","outputId":"fe429cce-88f4-496c-ad53-36c40bac1af1"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/528M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b8c5002b544143b663b2df73691646"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): ReLU(inplace=True)\n","  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): ReLU(inplace=True)\n","  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (6): ReLU(inplace=True)\n","  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (8): ReLU(inplace=True)\n","  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (11): ReLU(inplace=True)\n","  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (13): ReLU(inplace=True)\n","  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (15): ReLU(inplace=True)\n","  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (18): ReLU(inplace=True)\n","  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (20): ReLU(inplace=True)\n","  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (22): ReLU(inplace=True)\n","  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (25): ReLU(inplace=True)\n","  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (27): ReLU(inplace=True)\n","  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (29): ReLU(inplace=True)\n","  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 270, 470]           1,792\n","              ReLU-2         [-1, 64, 270, 470]               0\n","            Conv2d-3         [-1, 64, 270, 470]          36,928\n","              ReLU-4         [-1, 64, 270, 470]               0\n","         MaxPool2d-5         [-1, 64, 135, 235]               0\n","            Conv2d-6        [-1, 128, 135, 235]          73,856\n","              ReLU-7        [-1, 128, 135, 235]               0\n","            Conv2d-8        [-1, 128, 135, 235]         147,584\n","              ReLU-9        [-1, 128, 135, 235]               0\n","        MaxPool2d-10         [-1, 128, 67, 117]               0\n","           Conv2d-11         [-1, 256, 67, 117]         295,168\n","             ReLU-12         [-1, 256, 67, 117]               0\n","           Conv2d-13         [-1, 256, 67, 117]         590,080\n","             ReLU-14         [-1, 256, 67, 117]               0\n","           Conv2d-15         [-1, 256, 67, 117]         590,080\n","             ReLU-16         [-1, 256, 67, 117]               0\n","        MaxPool2d-17          [-1, 256, 33, 58]               0\n","           Conv2d-18          [-1, 512, 33, 58]       1,180,160\n","             ReLU-19          [-1, 512, 33, 58]               0\n","           Conv2d-20          [-1, 512, 33, 58]       2,359,808\n","             ReLU-21          [-1, 512, 33, 58]               0\n","           Conv2d-22          [-1, 512, 33, 58]       2,359,808\n","             ReLU-23          [-1, 512, 33, 58]               0\n","        MaxPool2d-24          [-1, 512, 16, 29]               0\n","           Conv2d-25          [-1, 512, 16, 29]       2,359,808\n","             ReLU-26          [-1, 512, 16, 29]               0\n","           Conv2d-27          [-1, 512, 16, 29]       2,359,808\n","             ReLU-28          [-1, 512, 16, 29]               0\n","           Conv2d-29          [-1, 512, 16, 29]       2,359,808\n","             ReLU-30          [-1, 512, 16, 29]               0\n","        MaxPool2d-31           [-1, 512, 8, 14]               0\n","         Identity-32           [-1, 512, 8, 14]               0\n","          Flatten-33                [-1, 57344]               0\n","          Dropout-34                [-1, 57344]               0\n","           Linear-35                 [-1, 4096]     234,885,120\n","             ReLU-36                 [-1, 4096]               0\n","           Linear-37                 [-1, 4096]      16,781,312\n","             ReLU-38                 [-1, 4096]               0\n","      BatchNorm1d-39                 [-1, 4096]           8,192\n","           Linear-40                   [-1, 10]          40,970\n","          Softmax-41                   [-1, 10]               0\n","================================================================\n","Total params: 266,430,282\n","Trainable params: 251,715,594\n","Non-trainable params: 14,714,688\n","----------------------------------------------------------------\n","Input size (MB): 1.45\n","Forward/backward pass size (MB): 549.98\n","Params size (MB): 1016.35\n","Estimated Total Size (MB): 1567.78\n","----------------------------------------------------------------\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/waleedka/hiddenlayer.git@master\n","  Cloning https://github.com/waleedka/hiddenlayer.git (to revision master) to /tmp/pip-req-build-0elk_luj\n","  Running command git clone -q https://github.com/waleedka/hiddenlayer.git /tmp/pip-req-build-0elk_luj\n","Building wheels for collected packages: hiddenlayer\n","  Building wheel for hiddenlayer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hiddenlayer: filename=hiddenlayer-0.2-py3-none-any.whl size=19775 sha256=dfc9a86819d4f419dc9530d6b151dd4581eb74ad20432e4cfb8d11ce548435bf\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ond129bu/wheels/00/09/2f/b6652a7470f4d7a25479fb95342e5e47f1a1b63801c985d568\n","Successfully built hiddenlayer\n","Installing collected packages: hiddenlayer\n","Successfully installed hiddenlayer-0.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return forward_call(*input, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_opset9.py:1672: UserWarning: Dropout is a training op and should not be exported in inference mode. For inference, make sure to call eval() on the model and to export it with param training=False.\n","  warnings.warn(\"Dropout is a training op and should not be exported in inference mode. \"\n"]},{"output_type":"execute_result","data":{"text/plain":["<hiddenlayer.graph.Graph at 0x7fc2e74ca310>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"1805pt\" height=\"116pt\"\n viewBox=\"0.00 0.00 1805.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(72 80)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-72,36 -72,-80 1733,-80 1733,36 -72,36\"/>\n<!-- /outputs/42 -->\n<g id=\"node1\" class=\"node\">\n<title>/outputs/42</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"190,-40 120,-40 120,-4 190,-4 190,-40\"/>\n<text text-anchor=\"start\" x=\"128\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 14617908681454553813 -->\n<g id=\"node12\" class=\"node\">\n<title>14617908681454553813</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"310,-44 226,-44 226,0 310,0 310,-44\"/>\n<text text-anchor=\"start\" x=\"234\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"295\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- /outputs/42&#45;&gt;14617908681454553813 -->\n<g id=\"edge6\" class=\"edge\">\n<title>/outputs/42&#45;&gt;14617908681454553813</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M190.1345,-22C198.237,-22 207.0271,-22 215.6853,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.7397,-25.5001 225.7397,-22 215.7396,-18.5001 215.7397,-25.5001\"/>\n</g>\n<!-- /outputs/47 -->\n<g id=\"node2\" class=\"node\">\n<title>/outputs/47</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"416,-40 346,-40 346,-4 416,-4 416,-40\"/>\n<text text-anchor=\"start\" x=\"354\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 12558322270756888333 -->\n<g id=\"node14\" class=\"node\">\n<title>12558322270756888333</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"536,-44 452,-44 452,0 536,0 536,-44\"/>\n<text text-anchor=\"start\" x=\"460\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"521\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x3</text>\n</g>\n<!-- /outputs/47&#45;&gt;12558322270756888333 -->\n<g id=\"edge10\" class=\"edge\">\n<title>/outputs/47&#45;&gt;12558322270756888333</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M416.1345,-22C424.237,-22 433.0271,-22 441.6853,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"441.7397,-25.5001 451.7397,-22 441.7396,-18.5001 441.7397,-25.5001\"/>\n</g>\n<!-- /outputs/54 -->\n<g id=\"node3\" class=\"node\">\n<title>/outputs/54</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"642,-40 572,-40 572,-4 642,-4 642,-40\"/>\n<text text-anchor=\"start\" x=\"580\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 9654823641575477822 -->\n<g id=\"node15\" class=\"node\">\n<title>9654823641575477822</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"762,-44 678,-44 678,0 762,0 762,-44\"/>\n<text text-anchor=\"start\" x=\"686\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"747\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x3</text>\n</g>\n<!-- /outputs/54&#45;&gt;9654823641575477822 -->\n<g id=\"edge12\" class=\"edge\">\n<title>/outputs/54&#45;&gt;9654823641575477822</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M642.1345,-22C650.237,-22 659.0271,-22 667.6853,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"667.7397,-25.5001 677.7397,-22 667.7396,-18.5001 667.7397,-25.5001\"/>\n</g>\n<!-- /outputs/61 -->\n<g id=\"node4\" class=\"node\">\n<title>/outputs/61</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"868,-40 798,-40 798,-4 868,-4 868,-40\"/>\n<text text-anchor=\"start\" x=\"806\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 18000895836176557862 -->\n<g id=\"node16\" class=\"node\">\n<title>18000895836176557862</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"988,-44 904,-44 904,0 988,0 988,-44\"/>\n<text text-anchor=\"start\" x=\"912\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"973\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x3</text>\n</g>\n<!-- /outputs/61&#45;&gt;18000895836176557862 -->\n<g id=\"edge14\" class=\"edge\">\n<title>/outputs/61&#45;&gt;18000895836176557862</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M868.1345,-22C876.237,-22 885.0271,-22 893.6853,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"893.7397,-25.5001 903.7397,-22 893.7396,-18.5001 893.7397,-25.5001\"/>\n</g>\n<!-- /outputs/68 -->\n<g id=\"node5\" class=\"node\">\n<title>/outputs/68</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1094,-40 1024,-40 1024,-4 1094,-4 1094,-40\"/>\n<text text-anchor=\"start\" x=\"1032\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 15677865886447711823 -->\n<g id=\"node10\" class=\"node\">\n<title>15677865886447711823</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1184,-44 1130,-44 1130,0 1184,0 1184,-44\"/>\n<text text-anchor=\"start\" x=\"1143\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Flatten</text>\n<text text-anchor=\"start\" x=\"1165\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- /outputs/68&#45;&gt;15677865886447711823 -->\n<g id=\"edge3\" class=\"edge\">\n<title>/outputs/68&#45;&gt;15677865886447711823</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1094.1034,-22C1102.3675,-22 1111.2064,-22 1119.5686,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1119.8266,-25.5001 1129.8266,-22 1119.8266,-18.5001 1119.8266,-25.5001\"/>\n</g>\n<!-- /outputs/71/72 -->\n<g id=\"node6\" class=\"node\">\n<title>/outputs/71/72</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1274,-40 1220,-40 1220,-4 1274,-4 1274,-40\"/>\n<text text-anchor=\"start\" x=\"1230\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Dropout</text>\n</g>\n<!-- 14412741954763885589 -->\n<g id=\"node13\" class=\"node\">\n<title>14412741954763885589</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1382,-44 1310,-44 1310,0 1382,0 1382,-44\"/>\n<text text-anchor=\"start\" x=\"1318\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Linear &gt; Relu</text>\n<text text-anchor=\"start\" x=\"1367\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- /outputs/71/72&#45;&gt;14412741954763885589 -->\n<g id=\"edge8\" class=\"edge\">\n<title>/outputs/71/72&#45;&gt;14412741954763885589</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1274.0522,-22C1281.9527,-22 1290.8212,-22 1299.5618,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1299.6982,-25.5001 1309.6982,-22 1299.6982,-18.5001 1299.6982,-25.5001\"/>\n</g>\n<!-- /outputs/77/78/79/80/81 -->\n<g id=\"node7\" class=\"node\">\n<title>/outputs/77/78/79/80/81</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1481,-40 1418,-40 1418,-4 1481,-4 1481,-40\"/>\n<text text-anchor=\"start\" x=\"1426.5\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">BatchNorm</text>\n</g>\n<!-- /outputs/82 -->\n<g id=\"node8\" class=\"node\">\n<title>/outputs/82</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1571,-40 1517,-40 1517,-4 1571,-4 1571,-40\"/>\n<text text-anchor=\"start\" x=\"1531\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n</g>\n<!-- /outputs/77/78/79/80/81&#45;&gt;/outputs/82 -->\n<g id=\"edge1\" class=\"edge\">\n<title>/outputs/77/78/79/80/81&#45;&gt;/outputs/82</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1481.23,-22C1489.464,-22 1498.4053,-22 1506.889,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1506.932,-25.5001 1516.932,-22 1506.9319,-18.5001 1506.932,-25.5001\"/>\n</g>\n<!-- /outputs/83 -->\n<g id=\"node9\" class=\"node\">\n<title>/outputs/83</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1661,-40 1607,-40 1607,-4 1661,-4 1661,-40\"/>\n<text text-anchor=\"start\" x=\"1617\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Softmax</text>\n</g>\n<!-- /outputs/82&#45;&gt;/outputs/83 -->\n<g id=\"edge2\" class=\"edge\">\n<title>/outputs/82&#45;&gt;/outputs/83</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1571.003,-22C1579.0277,-22 1587.9665,-22 1596.5309,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1596.7051,-25.5001 1606.705,-22 1596.705,-18.5001 1596.7051,-25.5001\"/>\n</g>\n<!-- 15677865886447711823&#45;&gt;/outputs/71/72 -->\n<g id=\"edge4\" class=\"edge\">\n<title>15677865886447711823&#45;&gt;/outputs/71/72</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1184.003,-22C1192.0277,-22 1200.9665,-22 1209.5309,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1209.7051,-25.5001 1219.705,-22 1209.705,-18.5001 1209.7051,-25.5001\"/>\n</g>\n<!-- 980930239154923773 -->\n<g id=\"node11\" class=\"node\">\n<title>980930239154923773</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"84,-44 0,-44 0,0 84,0 84,-44\"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"69\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- 980930239154923773&#45;&gt;/outputs/42 -->\n<g id=\"edge5\" class=\"edge\">\n<title>980930239154923773&#45;&gt;/outputs/42</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M84.077,-22C92.4638,-22 101.305,-22 109.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.9138,-25.5001 119.9138,-22 109.9138,-18.5001 109.9138,-25.5001\"/>\n</g>\n<!-- 14617908681454553813&#45;&gt;/outputs/47 -->\n<g id=\"edge7\" class=\"edge\">\n<title>14617908681454553813&#45;&gt;/outputs/47</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M310.077,-22C318.4638,-22 327.305,-22 335.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"335.9138,-25.5001 345.9138,-22 335.9138,-18.5001 335.9138,-25.5001\"/>\n</g>\n<!-- 14412741954763885589&#45;&gt;/outputs/77/78/79/80/81 -->\n<g id=\"edge9\" class=\"edge\">\n<title>14412741954763885589&#45;&gt;/outputs/77/78/79/80/81</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1382.1993,-22C1390.4879,-22 1399.3633,-22 1407.8603,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1407.9723,-25.5001 1417.9722,-22 1407.9722,-18.5001 1407.9723,-25.5001\"/>\n</g>\n<!-- 12558322270756888333&#45;&gt;/outputs/54 -->\n<g id=\"edge11\" class=\"edge\">\n<title>12558322270756888333&#45;&gt;/outputs/54</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M536.077,-22C544.4638,-22 553.305,-22 561.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"561.9138,-25.5001 571.9138,-22 561.9138,-18.5001 561.9138,-25.5001\"/>\n</g>\n<!-- 9654823641575477822&#45;&gt;/outputs/61 -->\n<g id=\"edge13\" class=\"edge\">\n<title>9654823641575477822&#45;&gt;/outputs/61</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M762.077,-22C770.4638,-22 779.305,-22 787.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"787.9138,-25.5001 797.9138,-22 787.9138,-18.5001 787.9138,-25.5001\"/>\n</g>\n<!-- 18000895836176557862&#45;&gt;/outputs/68 -->\n<g id=\"edge15\" class=\"edge\">\n<title>18000895836176557862&#45;&gt;/outputs/68</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M988.077,-22C996.4638,-22 1005.305,-22 1013.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1013.9138,-25.5001 1023.9138,-22 1013.9138,-18.5001 1013.9138,-25.5001\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{},"execution_count":13}],"source":["model = models.vgg16(pretrained=True)\n","#model.classifier = nn.Sequential(*[model.classifier[i] for i in range(5)])\n","print(model.features) # stampa tutto il modello a valle del classificatore \n","#print(model)\n","#print(model.classifier) # stampa tutto il classificatore \n","model.avgpool = nn.Identity() #sostituisco l'avg pool con un layer identità \n","#print(model.avgpool) #stampo quel layer per controllo \n","\n","model.classifier = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Dropout(0.5, inplace=False),\n","        nn.Linear(in_features=57344, out_features=4096,bias=True),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(in_features=4096, out_features=4096,bias=True), \n","        nn.ReLU(inplace=True),\n","        nn.BatchNorm1d(4096, affine=True),\n","        nn.Linear(4096, 10),\n","        nn.Softmax()\n",")\n","#print(model.classifier)\n","#model.classifier[6] = nn.Linear(in_features=4096, out_features=10, bias=True)\n","\n","#Freeze Weights Convolution Layer \n","for name, layer in model.named_modules():\n","  #print(name)\n","  #conv layer \n","  if name == 'features.0':\n","    layer.weight.requires_grad = False \n","  if name == 'features.2': \n","    layer.weight.requires_grad = False \n","  if name == 'features.5':\n","    layer.weight.requires_grad = False \n","  if name == 'features.7': \n","    layer.weight.requires_grad = False \n","  if name == 'features.10':\n","    layer.weight.requires_grad = False \n","  if name == 'features.12': \n","    layer.weight.requires_grad = False \n","  if name == 'features.14':\n","    layer.weight.requires_grad = False \n","  if name == 'features.17': \n","    layer.weight.requires_grad = False \n","  if name == 'features.19': \n","    layer.weight.requires_grad = False\n","  if name == 'features.21':\n","    layer.weight.requires_grad = False \n","  if name == 'features.24': \n","    layer.weight.requires_grad = False \n","  if name == 'features.26':\n","    layer.weight.requires_grad = False \n","  if name == 'features.28':\n","    layer.weight.requires_grad = False \n","\n","model = model.to(device)\n","summary(model,input_size=(3, 270,470))\n","\n","#Plot Model \n","%pip install -U git+https://github.com/waleedka/hiddenlayer.git@master\n","import hiddenlayer as hl\n","hl.build_graph(model, torch.zeros([3, 3, 270, 470]))\n"]},{"cell_type":"markdown","metadata":{"id":"2XV_nhqtTWxr"},"source":["## Metrica Balance Accuracy "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-rg-uNKANyC"},"outputs":[],"source":["\n","'''\n","Funzione per Balance Accuracy \n","-- Link Utile : https://medium.com/@mostafa.m.ayoub/customize-your-keras-metrics-44ac2e2980bd --\n","-- https://medium.com/analytics-vidhya/custom-metrics-for-keras-tensorflow-ae7036654e05 --- \n","-- https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score --- \n","'''\n","\n","'''\n","---- Questo caso funzione solo nel caso di classificazioni binarie ---- \n","def monitor_balance_accuracy ():\n","\tdef bal_acc(y_true, y_pred):\n","\t\ttp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","\t\tfn = K.sum(K.round(K.clip(y_true, 0, 1)))\n","\t\t#sensitivity = tp / (fn + K.epsilon()) #--- primo test : versione trovata sul web, ma non tornano le formule\n","\t\tsensitivity = tp / (fn + tp + K.epsilon()) #--OK\n","\n","\t\ttn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n","\t\tfp = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n","\t\t#specificity = tn / (fp + K.epsilon()) #--- primo test : versione trovata sul web, ma non tornano le formule \n","\t\tspecificity = tn / (fp + tn + K.epsilon()) #--OK \n","\t\n","\t\tBalanced_Accuracy = (sensitivity+specificity)/2 #--OK\n","\t\treturn Balanced_Accuracy \n","\treturn bal_acc\n","'''\n","\t\t\n","'''\n","Funzione per Balance Accuracy \n","'''\n","def monitor_balance_accuracy ():\n","\tdef bal_acc(y_true, y_pred):\n","\t\ty_true = y_true.numpy().argmax(axis=1) #Returns the indices of the maximum values along an axis.\n","\t\ty_pred = y_pred.numpy().argmax(axis=1) #Returns the indices of the maximum values along an axis.\n","\t\tBalanced_Accuracy = balanced_accuracy_score(y_true, y_pred)\n","\t\tBalanced_Accuracy = torch.tensor(Balanced_Accuracy) ##-- va trasformata in tensore (torch.tensor)\n","\t\treturn (Balanced_Accuracy) #--- capire se si può togliere il K. \n","\treturn bal_acc\n","\n","def _bal_acc_(y_true, y_pred):\n","\ty_true = y_true.detach().numpy()\n","\ty_pred = y_pred.detach().numpy()\n","#\tprint(y_pred)\n","#\tprint(len(y_pred))\n","#\tprint(type(y_pred))\n","#\tprint(y_true)\n","#\tprint(len(y_true))\n","#\tprint(type(y_true))\n","#\tprint('categorical')\n","\ty_true = custom_to_categorical(y_true,10)\n","\ty_true = y_true.argmax(axis=1) #Returns the indices of the maximum values along an axis.\n","\ty_pred = y_pred.argmax(axis=1) #Returns the indices of the maximum values along an axis.\n","\tBalanced_Accuracy = balanced_accuracy_score(y_true, y_pred)\n","\tBalanced_Accuracy = torch.tensor(Balanced_Accuracy) ##-- va trasformata in tensore (torch.tensor)\n","\treturn (Balanced_Accuracy) \n"]},{"cell_type":"markdown","metadata":{"id":"VX2SB5uYzzyB"},"source":["## Early Stopping Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvAnOHC2W134"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","import custom_early_stopping \n","from custom_early_stopping import _EarlyStopping"]},{"cell_type":"markdown","metadata":{"id":"VQsYqmoQVJIu"},"source":["## PREPROCESSING & DATA FRAME "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Y0wGA0Ji5Jv"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","import check_for_leakage_function\n","from check_for_leakage_function import check_for_leakage"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6372,"status":"ok","timestamp":1656925341144,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kCvpWRyju3cE","outputId":"ca81f068-3e1f-4abe-b2b7-f267752b1c0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------------\n","DATAFRAME COMPLETO INIZIALE\n","result\n","        ID  series            filename class\n","0        3       2  20201 319 5323.png    3+\n","1        4       2  20201 3110125 .png    3+\n","2        5       2  20201 31101327.png    3+\n","3        6       2  20201 3110161 .png    3+\n","4        7       2  20201 3110177 .png    3+\n","...    ...     ...                 ...   ...\n","1059  2023       7  20201031090549.png    3+\n","1060  2024       7  20201031090855.png    3+\n","1061  2025       7  20201031091127.png    3+\n","1062  2026       7  20201031091720.png    3+\n","1063  2027       7  20201031091941.png    3+\n","\n","[2128 rows x 4 columns]\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","Number of Null values in column 'quality_classes' : 2\n","- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n","mostro quegli elementi che hanno valore nullo\n","       ID  series            filename  class\n","963  1927       3  20200825181909.png    NaN\n","963  1927       3  20200825181932.png    NaN\n","- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n","Rimuovo gli elementi nulli e verifico stampando nuovamente i valori nulli:\n","elementi nulli rimasti: 0\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","elimino i file che non sono presenti in Google Drive anche se ci sono nel CSV\n","CHECK FILE NON PRESENTI NELLA CARTELLA\n","File Non Esiste !!!\n","File : 20202 13101023.png eliminato\n","File Non Esiste !!!\n","File : 20200825180901.png eliminato\n","File Non Esiste !!!\n","File : 20200825181058.png eliminato\n","File Non Esiste !!!\n","File : 20202 13101011.png eliminato\n","File Non Esiste !!!\n","File : 20200825180918.png eliminato\n","File Eliminati : 5 \n","CHECK FILE CON NaN\n","Empty DataFrame\n","Columns: [ID, series, filename, class]\n","Index: []\n","Empty DataFrame\n","Columns: [ID, series, filename, class]\n","Index: []\n","Empty DataFrame\n","Columns: [ID, series, filename, class]\n","Index: []\n","Empty DataFrame\n","Columns: [ID, series, filename, class]\n","Index: []\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","SPLIT DATA\n","train_balance_df\n","        ID series            filename  class\n","0      398      4  20203 3 7 0 38.png    7.0\n","1      398      4  20203 3 7 0 19.png    8.0\n","2     1944     12  20201010071452.png    6.0\n","3     1944     12  20201010071339.png    6.0\n","4      463      4  20203 108 147 .png    8.0\n","...    ...    ...                 ...    ...\n","1269   521      4  20203 10110 49.png    7.0\n","1270   265      2  20202 14123442.png    5.0\n","1271   265      2  20202 14123432.png    5.0\n","1272  1942     12  20201010070727.png    5.0\n","1273  1942     12  20201010070550.png    5.0\n","\n","[1274 rows x 4 columns]\n","test_balance_df\n","       ID series            filename  class\n","0     305      3  20203 2 7 1944.png    0.0\n","1     305      3  20203 2 7 1932.png    0.0\n","2     331      3  20203 2 8 8 22.png    0.0\n","3     331      3  20203 2 8 8 1 .png    4.0\n","4     271      3  20202 2716421 .png    0.0\n","..    ...    ...                 ...    ...\n","419   261      2  20202 14122743.png    5.0\n","420  1936      3  20200929122824.png    6.0\n","421  1936      3  20200929122632.png    6.0\n","422  1917      6  20200609213900.png    5.0\n","423  1917      6  20200609213844.png    5.0\n","\n","[424 rows x 4 columns]\n","val_balance_df\n","       ID series            filename  class\n","0    1683      6  20200525173531.png    4.0\n","1    1683      6  20200525173519.png    4.0\n","2     175      2  20202 138 560 .png    4.0\n","3     175      2  20202 138 5546.png    4.0\n","4     292      3  20203 2 7 4 51.png    0.0\n","..    ...    ...                 ...    ...\n","417  1729      7  20200526173659.png    2.0\n","418  1858     11  20200603131724.png    9.0\n","419  1858     11  20200603131710.png    7.0\n","420  1825      4  20200529181313.png    9.0\n","421  1825      4  20200529181301.png    9.0\n","\n","[422 rows x 4 columns]\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","conta del numero di immagini per speicfica classe in set Train\n","1274\n","1:95\n","2-:83\n","2:132\n","2+:110\n","3-:111\n","3:179\n","3+:206\n","4-:125\n","4:171\n","4+:62\n","conta del numero di immagini per speicfica classe in set Validation\n","422\n","1:33\n","2-:26\n","2:42\n","2+:39\n","3-:33\n","3:64\n","3+:68\n","4-:40\n","4:56\n","4+:21\n","conta del numero di immagini per speicfica classe in set Test\n","424\n","1:37\n","2-:39\n","2:38\n","2+:28\n","3-:35\n","3:64\n","3+:69\n","4-:43\n","4:48\n","4+:23\n","Weight train_balance_df\n","{0: 1.34, 1: 1.53, 2: 0.97, 3: 1.16, 4: 1.15, 5: 0.71, 6: 0.62, 7: 1.02, 8: 0.75, 9: 2.05}\n","Weight val_balance_df\n","{0: 1.28, 1: 1.62, 2: 1.0, 3: 1.08, 4: 1.28, 5: 0.66, 6: 0.62, 7: 1.06, 8: 0.75, 9: 2.01}\n","Weight test_balance_df\n","{0: 1.15, 1: 1.09, 2: 1.12, 3: 1.51, 4: 1.21, 5: 0.66, 6: 0.61, 7: 0.99, 8: 0.88, 9: 1.84}\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","test case 1 - train VS validation\n","Stessi ID in set usati?: False\n","-------------------------------------\n","test case 2 - train VS test\n","Stessi ID in set usati ?: False\n","-------------------------------------\n","test case 3 - validation VS test\n","Stessi ID in set usati?: False\n"]}],"source":["'''PREPROCESSING PHASE OF THE DATAFRAME (CREATIONS OF THE SUBSETS TRAIN/VALIDATION/TEST, CALCULATE WEIGHTS OF ELEMENTS OF THE SUBSETS, VERIFY THAT SAME IDs ARE IN THE SAME SUBSET)'''\n","os.chdir(path_progettoDL)\n","path = os.getcwd()\n","\n","'''reading informations from the CSV'''\n","col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n","dataframe_sx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n","\n","col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n","dataframe_dx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n","\n","'''rename the dataframe columns'''\n","dataframe_sx.columns = ['ID','series', 'filename', 'class']\n","dataframe_dx.columns = ['ID','series', 'filename', 'class']\n","\n","frames = [dataframe_sx, dataframe_dx] \n","result = pd.concat(frames) #concatenate the two dataframes\n","\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"DATAFRAME COMPLETO INIZIALE\")\n","print(\"result\")\n","print(result)\n","\n","#print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","#print(\"STAMPO ELEMENTO/I CON INDICE 1 (elemento tutto a sinistra)\")\n","#print(result.loc[[1]])\n","#print(type(result.loc[[1]]))    #STAMPO IL TIPO DELL'ELEMENTO \n","\n","#print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","#print(\"LUNGHEZZA DATAFRAME COMPLESSIVO : {} \".format(result[result.columns[0]].count()))\n","\n","\n","'''mapping the values used for the classification into integer values'''\n","#version with 10 classes\n","result[\"class\"] = result[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n","result[\"series\"] = result[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)}) \n","\n","\n","'''identification of NULL values that would bring the execution on failing and eliminate those values'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"Number of Null values in column 'quality_classes' : \"+format(result['class'].isnull().sum()))\n","print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n","#print(result.loc[result['class'] == '0'])\n","print(\"mostro quegli elementi che hanno valore nullo\")\n","print(result[result['class'].isnull()])\n","print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n","\n","'''Remove Null elements to avoid failures during executions (data in not useful!)'''\n","print(\"Rimuovo gli elementi nulli e verifico stampando nuovamente i valori nulli:\")\n","result['class'] = pd.to_numeric(result['class'], errors='coerce')\n","result = result.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n","\n","print(\"elementi nulli rimasti: \"+format(result['class'].isnull().sum()))     #stampo per verifica se ci sono elementi nulli\n","\n","\n","'''verify if images exist in the Google Drive folder, when not present it is eliminated from the dataset aswell'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"elimino i file che non sono presenti in Google Drive anche se ci sono nel CSV\")\n","print('CHECK FILE NON PRESENTI NELLA CARTELLA')\n","\n","os.chdir(path_images)\n","i = 0; \n","for index, row in result.iterrows():\n","    filename = row['filename']\n","    if os.path.exists(path_images+filename) == False:\n","      print('File Non Esiste !!!')\n","    \n","    if(os.path.exists(filename) == False):\n","      result = result.drop(result[(result['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","      i = i + 1             \n","print('File Eliminati : {} '.format(i))\n","\n","print('CHECK FILE CON NaN')\n","print(result[result['class'].isnull()])\n","print(result[result['series'].isnull()])\n","print(result[result['filename'].isnull()])\n","print(result[result['ID'].isnull()])\n","result = result[result['class'].notna()]\n","result = result[result['series'].notna()]\n","result = result[result['filename'].notna()]\n","result = result[result['ID'].notna()]\n","\n","\n","\n","#----PER FARE LE PROVE RIDUCO LA DIMENSIONE DI RESULTS#\n","#result = result[:,100]\n","\n","'''creation of masked images (grayscale images) and save them in Google Drive'''\n","'''than create a second dataframe with these new images'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","'''\n","mask_filenames = []\n","IDs = []\n","classes = []\n","for index, row in result.iterrows():\n","    filename = row['filename']\n","    mask_filenames.append(str(\"mask_\"+filename))\n","    IDs.append(row['ID'])\n","    classes.append(row['class'])\n","\n","print(\"DATAFRAME CON MASCHERE\")\n","result2 = result.copy()\n","result2['mask_filename'] = mask_filenames\n","result2.drop('filename', axis='columns', inplace=True)   #rimuovo colonna con path immagini normali\n","\n","column_names = [\"ID\",\"series\", \"mask_filename\", \"class\"]\n","result2 = result2.reindex(columns=column_names)\n","\n","print(\"result2\")\n","print(result2)\n","#stampa della conta delle serie dei calci presenti nel dataframe\n","'''\n","\n","'''performing the splitting of the dataframe into sub-sets'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"SPLIT DATA\")\n","\n","train_balance_df, test_balance_df, val_balance_df  = split_data(result, 0.2, 0.2, 3)  #CUSTOM SPLIT CON ID IN STESSO SET DI DATI\n","#train_mask, test_mask, validation_mask  = split_data(result2, 0.2, 0.2, 3)           #split per test con immagini con maschere\n","\n","print(\"train_balance_df\")\n","print(train_balance_df)\n","print(\"test_balance_df\")\n","print(test_balance_df)\n","print(\"val_balance_df\")\n","print(val_balance_df)\n","\n","\n","#------------------------version with 4 classes (togliere se si lavora con 10 classi)-----------------------------------\n","#train_balance_df[\"class\"] = train_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","#val_balance_df[\"class\"] = val_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","#test_balance_df[\"class\"] = test_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","\n","\"\"\"\n","NOTA: versione dei metodi di tensorflow, che non divide però mantenendo stessi ID in stessi Sub-set\n","train_balance_df, test_balance_df = train_test_split(result, test_size=0.4, stratify=result['class'], random_state=2)\n","test_balance_df, val_balance_df = train_test_split(test_balance_df, test_size=0.5, stratify=test_balance_df['class'],random_state=2)\n","\"\"\"\n","\n","'''verify distibution of classes in the sub-sets and calculate weights of the classes in each sub-set'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","vals, counts = np.unique(train_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Train\")\n","print(len(train_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts[i]))\n","\n","vals2, counts2 = np.unique(val_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Validation\")\n","print(len(val_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts2[i]))\n","\n","vals3, counts3 = np.unique(test_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Test\")\n","print(len(test_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts3[i]))    \n","\n","\n","class_weights_train = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(train_balance_df['class']),y = train_balance_df['class'])\n","weight_train = {i : round(class_weights_train[i], 2) for i in range(len(classi))} \n","print('Weight train_balance_df')\n","print(weight_train)\n","\n","\n","class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(val_balance_df['class']),y = val_balance_df['class'])\n","weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n","print('Weight val_balance_df')\n","print(weight)\n","\n","\n","class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(test_balance_df['class']),y = test_balance_df['class'])\n","weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n","print('Weight test_balance_df')\n","print(weight)\n","\n","\n","'''verify that same IDs are in the same sub-sets'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","\n","#--------verifico che stessi ID siano in stesso set--------\n","print(\"test case 1 - train VS validation\")\n","print(f\"Stessi ID in set usati?: {check_for_leakage(train_balance_df, val_balance_df, 'ID')}\")\n","print(\"-------------------------------------\")\n","print(\"test case 2 - train VS test\")\n","print(f\"Stessi ID in set usati ?: {check_for_leakage(train_balance_df, test_balance_df, 'ID')}\")\n","print(\"-------------------------------------\")\n","print(\"test case 3 - validation VS test\")\n","print(f\"Stessi ID in set usati?: {check_for_leakage(val_balance_df, test_balance_df, 'ID')}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1656925341145,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"QPihVVu9fXJb","outputId":"470b0574-d3f3-4aca-c31f-751c1e0d63fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Verifica Classi Qualità per ogni Serie\n","SOMMA IMG totali: 2120\n"]}],"source":["'''Verifica Classi Qualità per ogni Serie'''\n","print(\"Verifica Classi Qualità per ogni Serie\")\n","result_x_ = result.groupby(['series','class']).size()\n","#print(result_x_)\n","result_class = result.groupby(['class']).size()\n","#print(result_class)\n","result_series = result.groupby(['series']).size()\n","#print(result_series)  \n","print('SOMMA IMG totali: {}'.format(np.sum(result_class)))\n"]},{"cell_type":"markdown","metadata":{"id":"Ks3Fly2oUf3g"},"source":["## HYPERPARAMETERS"]},{"cell_type":"markdown","metadata":{"id":"DXwAzoJE_OIP"},"source":["\n","The **optimization algorithm** (or optimizer) is the main approach used today for training a machine learning model to minimize its error rate. There are *two metrics* to determine the efficacy of an optimizer: **speed of convergence** (the process of reaching a global optimum for gradient descent); and **generalization** (the model’s performance on new data)\n","\n","***SGD*** : Stochastic Gradient Descent \n","\n","Parameters \n","- Learning : learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. ... In setting a learning rate, there is a trade-off between the rate of convergence and overshooting\n","- Momentum : Momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n","- Decay :  We then set our decay to be the learning rate divided by the total number of epochs we are training the network for (a common rule of thumb) ... lr = (lr_iniziale - (1.0/(1-decay*iterations)))\n","- Nesterov: Nesterov which is set to false by default. Nesterov momentum is a different version of the momentum method which has stronger theoretical converge guarantees for convex functions.\n","\n","[1° LINK](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)\n","\n","[2° LINK](https://www.pyimagesearch.com/2019/07/22/keras-learning-rate-schedules-and-decay/)\n","\n","\n","![1*VQkTnjr2VJOz0R2m4hDucQ.jpeg](data:image/jpeg;base64,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)\n","\n","***ADAM*** :  Adaptive Moment Estimation\n","\n","\n","\n","\n","FONTI : Paper for ICLR 2019"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":206,"status":"ok","timestamp":1656925364778,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Bbjwj4QwW6yx","outputId":"bb934c17-c48f-4937-d5e9-662531442077"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nlosses = {\\n\\t\"class_output\": \\'categorical_crossentropy\\'\\n}\\nlossWeights = { \"class_output\": 1}\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["#Definizione hyperparameters                                                                                                                        \n","optimizer_ = optim.SGD(model.parameters(), weight_decay=1e-5, lr=0.001, momentum=0.8)\n","epochs = 30 #100  \n","bs = 64\n","\n","'''\n","losses = {\n","\t\"class_output\": 'categorical_crossentropy'\n","}\n","lossWeights = { \"class_output\": 1}\n","'''\n","#hyper parametri test "]},{"cell_type":"markdown","metadata":{"id":"Utp1VUbSWy_0"},"source":["## CREAZIONE Sub-sets di TRAIN, VAL, TEST (con immagini create della GAN)"]},{"cell_type":"markdown","metadata":{"id":"ZnbL-T3PAGWI"},"source":["#### D.A. offline - GAN images load from drive"]},{"cell_type":"markdown","metadata":{"id":"M08_ckvtTEzl"},"source":["RICERCA DELLE IMMAGINI E SALVATAGGIO DEI NOMI SU UN CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UBAoRewI5X7"},"outputs":[],"source":["#PARTE DOVE SI USA IL METODO ImageFolder PER LEGGERE TUTTE LE IMMAGINI IN UNA CARTELLA SENZA SAPERNE IL NOME E SALVARLE IN CSV E ALLEGARLE AL DATASET '''\n","\n","# Define directory containing images-\n","data_dir = '/content/drive/MyDrive/CALCIO_CROP_BASE/'\n","data_dir_2 = '/content/drive/MyDrive/CALCIO_CROP_BASE/Data_Aug_GAN'\n","\n","# Define datasets-\n","train_data = datasets.ImageFolder(data_dir + 'Data_Aug_GAN', \n","                                  transform = transform_train)\n","\n","print(train_data.imgs[785][0])  #QUESTO è IL NOME DEL FILE\n","print(train_data.imgs[785][1])  #QUESTO è LA CLASSE DI QUALITA' PESCATA DAL NOME DELLA CARTELLA\n","\n","\n","import os\n","#https://www.studytonight.com/python-howtos/how-to-get-the-last-part-of-the-path-in-python\n","path = os.path.basename(os.path.normpath(train_data.imgs[0][0]))  #ESTRARRE L'ULTIMA PARTE DI UN PERCORSO IN STRINGA, QUINDI IL NOME DEL FILE\n","print(path)\n","\n","print(f\"number of train images = {len(train_data)}\")\n","print(f\"number of training classes = {len(train_data.classes)}\")\n","\n","\n","i = 0\n","os.chdir(path_progettoDL)\n","path = os.getcwd()\n","\n","\n","#CREO L'INTESTAZIONE DEL FILE\n","with open(path + '/GAN_DB.txt', 'a') as f:\n","    f.write('ID;COD_COMPONENTE;IMG;CLASSE_CALCIO')\n","\n","#memorizzo ogni riga come il precedente file txt per le immagini vere\n","for i in range(len(train_data)):\n","  with open(path + '/GAN_DB.txt', 'a') as f:\n","    if train_data.imgs[i][1] > 5:\n","      f.write('\\n{};10;Data_Aug_GAN/images_{}/{};{}'.format(i+2028,train_data.imgs[i][1]+1, os.path.basename(os.path.normpath(train_data.imgs[i][0])),train_data.imgs[i][1]+1))  #mi serve anche questo caso perchè la classe 6 non esiste, e lui scorrendo poi dalla classe 7 la considera come la 6\n","    else:\n","      f.write('\\n{};10;Data_Aug_GAN/images_{}/{};{}'.format(i+2028,train_data.imgs[i][1], os.path.basename(os.path.normpath(train_data.imgs[i][0])),train_data.imgs[i][1]))\n"]},{"cell_type":"markdown","source":["CONCATENAZIONE DEL DATASET GAN CON QUELLO ORIGINALE PER FARE L'ALLENAMENTO DELLA VGG16 (BASELINE)"],"metadata":{"id":"HO9Uht5x4xak"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1363,"status":"ok","timestamp":1656926273115,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"5FviH-aLlMPj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e7ed783-fdae-407e-ae8c-9c775c6f7af8"},"outputs":[{"output_type":"stream","name":"stdout","text":["File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3452_1_20_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3760_1_43_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3773_1_28_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3994_1_22_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3996_1_38_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3998_1_17_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3999_1_12_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3999_1_25_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3999_1_34_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3999_1_36_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3999_1_3_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3999_1_41_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3999_1_49_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/3999_1_62_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1760_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1760_2_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1761_2_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1798_1_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1799_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1810_1_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1810_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1812_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1817_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1830_2_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1833_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1836_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1837_2_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1840_2_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1841_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1855_2_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1856_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1856_1_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1857_1_14.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1857_2_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1857_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1859_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1874_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1875_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1879_1_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1884_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1894_2_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1894_2_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1914_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1915_2_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1916_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1930_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1932_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1940_2_1.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1941_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1955_2_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1961_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1962_1_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1962_2_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1963_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1964_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_1999_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2001_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2008_2_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2042_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2053_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2071_2_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2071_2_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2074_2_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2074_2_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2094_1_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2096_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2100_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2111_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2112_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2116_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2143_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2155_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2171_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2213_2_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2216_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2222_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2284_2_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2284_2_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2404_2_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2458_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2458_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2461_2_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2464_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2484_1_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2484_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2484_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2488_1_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2494_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_0/_2494_1_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/2351_1_10_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/2439_1_61_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/2444_1_20_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/3287_1_36_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/3385_1_17_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/3983_1_60_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/3993_1_49_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/3996_1_57_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/3997_1_2_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/3997_1_7_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/3998_1_33_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1757_2_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1768_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1772_2_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1780_2_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1780_2_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1782_1_1.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1788_1_14.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1793_2_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1793_2_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1817_2_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1866_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1887_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1890_2_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1890_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1904_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1911_2_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1918_2_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1919_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1934_1_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1941_2_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1945_2_14.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1946_2_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1976_2_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_1976_2_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2003_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2008_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2014_1_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2042_1_14.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2053_2_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2118_2_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2136_2_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2158_2_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2173_2_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2175_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2175_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2176_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2191_2_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2260_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2263_2_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2267_2_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2287_2_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2312_2_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2398_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2401_1_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2407_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2415_2_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2423_2_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2428_1_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2428_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2430_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2438_2_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2441_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2441_2_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2441_2_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2443_2_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2446_2_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2447_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2448_1_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2452_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2454_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2454_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2457_1_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2458_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2458_2_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2473_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2482_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2483_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2484_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2484_1_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2487_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2489_2_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2490_2_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2490_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2492_2_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2493_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2493_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2494_1_1.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2494_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2494_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2494_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2494_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2494_2_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2497_1_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2497_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2499_2_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_1/_2499_2_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/1585_2_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2260_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2294_2_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2298_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2337_2_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2386_1_14.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2393_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2396_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2416_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2427_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2436_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2440_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2449_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2452_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2454_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2457_2_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2457_2_30.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2461_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2465_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2468_2_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2472_2_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2545_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2572_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2581_2_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2588_2_30.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2588_2_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2593_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2619_1_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2632_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2632_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2640_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2646_1_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2649_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2651_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2653_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2653_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2658_2_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2658_2_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2665_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2668_2_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2675_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2676_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2678_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2680_2_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2682_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2688_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2696_2_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2696_2_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2696_2_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/2698_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/_2858_1_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/_2967_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/_2982_2_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/_2988_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/_2991_2_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_2/_2995_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/1616_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/2888_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/2925_2_44.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/2959_2_40.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/2959_2_41.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/2964_1_56.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1752_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1752_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1756_1_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1760_1_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1763_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1763_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1764_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1764_2_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1766_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1766_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1766_2_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1767_2_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1774_1_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1774_1_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1776_2_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1782_2_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1782_2_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1789_1_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1799_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1811_1_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1814_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1817_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1818_2_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1825_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1827_2_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1837_2_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1837_2_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1852_2_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1855_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1859_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1866_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1876_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1889_2_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1892_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1893_2_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1895_1_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1897_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1909_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1910_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1918_2_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1922_2_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1922_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1954_2_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1954_2_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1956_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1956_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1959_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1963_2_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1985_2_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_1987_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2005_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2009_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2012_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2013_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2041_1_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2102_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2105_1_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2116_2_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2153_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2191_2_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2210_2_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2210_2_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2210_2_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2221_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2221_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2221_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2221_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_3/_2221_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/1297_1_46.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2452_2_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2593_2_40.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2618_1_41.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2641_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2642_1_49.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2645_2_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2912_2_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2998_1_34.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/2998_1_55.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1529_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1588_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1639_2_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1639_2_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1658_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1694_3_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1701_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1720_3_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1726_1_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1738_3_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1755_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1756_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1761_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1763_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1782_3_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1799_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1805_3_1.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1805_3_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1805_3_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1826_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1826_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1826_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1849_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1852_3_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1876_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1905_1_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1905_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1905_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_1920_3_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2016_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2016_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2028_2_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2048_3_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2050_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2068_1_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2117_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2131_3_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2153_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2155_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2182_2_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2219_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2225_3_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2244_3_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2259_3_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2329_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2330_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2332_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2332_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2351_2_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2429_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2429_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2441_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2441_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2472_2_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2472_2_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2476_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2476_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_4/_2476_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_5/3099_1_14.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_5/3999_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_5/_1674_2_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_5/_1914_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_5/_2252_1_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2151_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2276_2_37.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2304_1_47_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2335_1_28.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2335_2_41.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2501_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2501_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2502_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2502_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2503_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2507_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2510_1_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2534_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2580_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2580_1_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2658_1_60.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2809_1_24_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2837_1_34_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2941_1_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/2962_1_50_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3053_1_63_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3054_1_16_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3054_1_40_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3054_1_56_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3123_1_30.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3210_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3226_1_43.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3263_1_47.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3273_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3372_1_50.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3420_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3480_1_39.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3488_1_36.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3492_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3495_1_37.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3495_1_42.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3495_1_48.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3835_1_29.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3864_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3898_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_7/3969_1_56_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/2963_2_8_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/_1910_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/_1991_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/_2023_2_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/_2160_2_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/_2164_2_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/_2171_2_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/_2464_1_10.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_8/_2464_1_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2501_1_18_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2502_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2503_1_14.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2503_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2514_1_27_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2521_1_30_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2531_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2536_1_26_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2544_1_14_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2575_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2594_1_10_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2601_1_15_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2615_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2633_1_13_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2650_1_31_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2733_1_20_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2743_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2766_1_30_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2900_1.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3048_1_12_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3116_1_27_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3147_40.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3210_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3218_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3230_1_2_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3259_1_24_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3281_50.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3358_1_24_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3416_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3418_30.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3419_1_7_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3419_38.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3424_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3425_41.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3428_40.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3430_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3435_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3452_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3470_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3471_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3471_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3471_1_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3473_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3496_1_12_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3496_1_8_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3497_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3497_1_13_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3497_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3497_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3498_1_21_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3498_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3499_1_25_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3499_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3499_1_6_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3532_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3561_41.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3563_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3565_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3569_60.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3570_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3572_39.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3574_42.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3577_54.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3578_55.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3595_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3595_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3599_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3606_36.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3615_53.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3620_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3623_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3625_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3625_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3629_46.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3630_55.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3631_34.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3632_47.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3655_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3657_33.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3667_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3668_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3668_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3670_1_1.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3674_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3676_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3701_43.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3705_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3719_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3720_30.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3721_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3731_1_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3734_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3736_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3744_52.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3749_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3750_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3755_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3777_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3777_47.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3806_35.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3810_55.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3812_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3845_35.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3912_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3923_1_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3941_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3967_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3994_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3995_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3996_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3998_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3999_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3999_1_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3999_1_26.png eliminato\n","File Eliminati : 559 \n","        ID series            filename  class\n","0      398      4  20203 3 7 0 38.png    7.0\n","1      398      4  20203 3 7 0 19.png    8.0\n","2     1944     12  20201010071452.png    6.0\n","3     1944     12  20201010071339.png    6.0\n","4      463      4  20203 108 147 .png    8.0\n","...    ...    ...                 ...    ...\n","1269   521      4  20203 10110 49.png    7.0\n","1270   265      2  20202 14123442.png    5.0\n","1271   265      2  20202 14123432.png    5.0\n","1272  1942     12  20201010070727.png    5.0\n","1273  1942     12  20201010070550.png    5.0\n","\n","[1274 rows x 4 columns]\n","       ID  series                               filename  class\n","1    2029      10  Data_Aug_GAN/images_0/3759_1_17_2.png      0\n","3    2031      10  Data_Aug_GAN/images_0/3767_1_51_2.png      0\n","4    2032      10  Data_Aug_GAN/images_0/3773_1_13_2.png      0\n","6    2034      10  Data_Aug_GAN/images_0/3773_1_36_2.png      0\n","7    2035      10  Data_Aug_GAN/images_0/3843_1_12_2.png      0\n","..    ...     ...                                    ...    ...\n","770  2798      10    Data_Aug_GAN/images_9/3890_1_18.png      9\n","773  2801      10    Data_Aug_GAN/images_9/3925_1_24.png      9\n","776  2804      10    Data_Aug_GAN/images_9/3982_1_31.png      9\n","777  2805      10    Data_Aug_GAN/images_9/3989_1_10.png      9\n","785  2813      10    Data_Aug_GAN/images_9/3999_1_29.png      9\n","\n","[227 rows x 4 columns]\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","DATAFRAME COMPLETO INIZIALE\n","result\n","       ID series                             filename  class transform\n","0     398      4                   20203 3 7 0 38.png    7.0        T0\n","1     398      4                   20203 3 7 0 19.png    8.0        T0\n","2    1944     12                   20201010071452.png    6.0        T0\n","3    1944     12                   20201010071339.png    6.0        T0\n","4     463      4                   20203 108 147 .png    8.0        T0\n","..    ...    ...                                  ...    ...       ...\n","770  2798     10  Data_Aug_GAN/images_9/3890_1_18.png    9.0        T0\n","773  2801     10  Data_Aug_GAN/images_9/3925_1_24.png    9.0        T0\n","776  2804     10  Data_Aug_GAN/images_9/3982_1_31.png    9.0        T0\n","777  2805     10  Data_Aug_GAN/images_9/3989_1_10.png    9.0        T0\n","785  2813     10  Data_Aug_GAN/images_9/3999_1_29.png    9.0        T0\n","\n","[1501 rows x 5 columns]\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","conta del numero di immagini per speicfica classe in set Train\n","1501\n","1:117\n","2-:109\n","2:150\n","2+:132\n","3-:138\n","3:201\n","3+:206\n","4-:165\n","4:197\n","4+:86\n","       ID series                             filename  class transform\n","0     398      4                   20203 3 7 0 38.png    7.0        T0\n","1     398      4                   20203 3 7 0 19.png    8.0        T0\n","2    1944     12                   20201010071452.png    6.0        T0\n","3    1944     12                   20201010071339.png    6.0        T0\n","4     463      4                   20203 108 147 .png    8.0        T0\n","..    ...    ...                                  ...    ...       ...\n","770  2798     10  Data_Aug_GAN/images_9/3890_1_18.png    9.0        T0\n","773  2801     10  Data_Aug_GAN/images_9/3925_1_24.png    9.0        T0\n","776  2804     10  Data_Aug_GAN/images_9/3982_1_31.png    9.0        T0\n","777  2805     10  Data_Aug_GAN/images_9/3989_1_10.png    9.0        T0\n","785  2813     10  Data_Aug_GAN/images_9/3999_1_29.png    9.0        T0\n","\n","[1501 rows x 5 columns]\n"]}],"source":["#READ CSV WITH THE INFOs & CREATE THE FINAL BALANCED DATASET TO BE USED\n","\n","os.chdir(path_progettoDL)\n","path = os.getcwd()\n","\n","'''reading informations from the CSV'''\n","col_list = [\"ID\", \"COD_COMPONENTE\", \"IMG\", \"CLASSE_CALCIO\"]\n","\n","dataframe_GAN = pd.read_csv(os.path.join(path + '/GAN_DB.txt'), usecols=col_list, sep=\";\")\n","dataframe_GAN.columns = ['ID','series', 'filename', 'class']\n","\n","os.chdir(path_images)\n","i = 0; \n","for index, row in dataframe_GAN.iterrows():\n","    filename = row['filename']\n","    if os.path.exists(path_images+filename) == False:\n","      print('File Non Esiste !!!')\n","    \n","    if(os.path.exists(filename) == False):\n","      dataframe_GAN = dataframe_GAN.drop(dataframe_GAN[(dataframe_GAN['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","      i = i + 1             \n","print('File Eliminati : {} '.format(i))\n","\n","\n","data = train_balance_df\n","train_balance_df_copy = pd.DataFrame(data, columns=['ID', 'series', 'filename', 'class', 'transform'])\n","train_balance_df_copy['transform'] = 'T0'\n","print(train_balance_df)\n","\n","\n","\n","data = dataframe_GAN\n","train_balance_df_copy_gan = pd.DataFrame(data, columns=['ID', 'series', 'filename', 'class', 'transform'])\n","train_balance_df_copy_gan['transform'] = 'T0'                                                                   #IN QUESTA RIGA POI PROVARE ANCHE CON ROTAZIONE PER RENDERLE UN PO DIVERSE (T4)\n","print(dataframe_GAN)\n","\n","frames = [train_balance_df_copy, train_balance_df_copy_gan] \n","result_2 = pd.concat(frames) #concatenate the two dataframes\n","\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"DATAFRAME COMPLETO INIZIALE\")\n","print(\"result\")\n","print(result_2)\n","train_balance_df_GAN = result_2\n","\n","'''verify distibution of classes in the sub-sets and calculate weights of the classes in each sub-set'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","vals, counts = np.unique(train_balance_df_GAN['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Train\")\n","print(len(train_balance_df_GAN))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts[i]))\n","print(result_2)\n"]},{"cell_type":"markdown","metadata":{"id":"8nObcOneuOOT"},"source":["#### SETS EXTRACTION FOR DATA AUGEMNTATION WITH GANs\n","\n","NOTA = per rimuovere Data Augmentation, commentare tutto il blocco sotto e utilizzare direttamente il set 'train_balance_df' nel CustomDataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":70,"status":"ok","timestamp":1656869655959,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"y2kXjyROQRan","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6681bea1-1f74-4deb-c01e-0399c516a241"},"outputs":[{"output_type":"stream","name":"stdout","text":["       ID series            filename  class\n","22    309      3  20203 2 7 2249.png    0.0\n","23    309      3  20203 2 7 2239.png    0.0\n","24    320      3  20203 2 7 3533.png    0.0\n","25    320      3  20203 2 7 3521.png    0.0\n","26    366      3  20203 2 9 3946.png    0.0\n","...   ...    ...                 ...    ...\n","1058  278      3  20202 2716493 .png    0.0\n","1059  278      3  20202 27164849.png    0.0\n","1088  348      3  20203 2 8 2951.png    0.0\n","1089  348      3  20203 2 8 2938.png    0.0\n","1160  304      3  20203 2 7 195 .png    0.0\n","\n","[95 rows x 4 columns]\n","95\n","---------------------------------------------------\n","        ID series            filename  class\n","66    1817      4  20200529172017.png    9.0\n","67    1817      4  20200529171958.png    9.0\n","70    1836     10  20200529194441.png    9.0\n","71    1836     10  20200529194427.png    9.0\n","106   1689      8  20200525181453.png    9.0\n","...    ...    ...                 ...    ...\n","1218   637      4  20200506131733.png    9.0\n","1244   473      4  20203 108 2449.png    9.0\n","1245   473      4  20203 108 2438.png    9.0\n","1246   498      4  20203 109 5735.png    9.0\n","1247   498      4  20203 109 5720.png    9.0\n","\n","[62 rows x 4 columns]\n","62\n","---------------------------------------------------\n","       ID series            filename  class\n","1     398      4  20203 3 7 0 19.png    8.0\n","4     463      4  20203 108 147 .png    8.0\n","5     463      4  20203 108 1356.png    8.0\n","8     446      4  20203 3 8 2453.png    8.0\n","9     446      4  20203 3 8 2441.png    8.0\n","...   ...    ...                 ...    ...\n","1216  457      4  20203 108 7 55.png    8.0\n","1217  457      4  20203 108 7 42.png    8.0\n","1219  637      4  20200506131709.png    8.0\n","1228  465      4  20203 108 187 .png    8.0\n","1229  465      4  20203 108 1756.png    8.0\n","\n","[171 rows x 4 columns]\n","171\n","---------------------------------------------------\n","        ID series            filename  class\n","0      398      4  20203 3 7 0 38.png    7.0\n","17     417      4  20203 3 7 4038.png    7.0\n","32     646      4  20200506133512.png    7.0\n","33     646      4  20200506133448.png    7.0\n","69     598      5  20203 19163 6 .png    7.0\n","...    ...    ...                 ...    ...\n","1243   643      4  20200506133133.png    7.0\n","1254  1892     11  20200604132919.png    7.0\n","1255  1892     11  20200604132905.png    7.0\n","1268   521      4  20203 10111 7 .png    7.0\n","1269   521      4  20203 10110 49.png    7.0\n","\n","[125 rows x 4 columns]\n","125\n","---------------------------------------------------\n","        ID series            filename  class\n","2     1944     12  20201010071452.png    6.0\n","3     1944     12  20201010071339.png    6.0\n","15      61      0  20201 31151021.png    6.0\n","18    2021     12  20201031090000.png    6.0\n","19    2021     12  20201031085900.png    6.0\n","...    ...    ...                 ...    ...\n","1211    18      0  20201 31104720.png    6.0\n","1212   542      5  20203 18183 47.png    6.0\n","1213   542      5  20203 18183 33.png    6.0\n","1223   496      4  20203 109 557 .png    6.0\n","1241  2023     12  20201031090549.png    6.0\n","\n","[206 rows x 4 columns]\n","206\n","---------------------------------------------------\n","        ID series            filename  class\n","6      244      2  20202 14113338.png    5.0\n","7      244      2  20202 14113328.png    5.0\n","14      61      0  20201 3115116 .png    5.0\n","34     569      5  20203 18213952.png    5.0\n","42     168      2  20202 138 4846.png    5.0\n","...    ...    ...                 ...    ...\n","1267   245      2  20202 14113942.png    5.0\n","1270   265      2  20202 14123442.png    5.0\n","1271   265      2  20202 14123432.png    5.0\n","1272  1942     12  20201010070727.png    5.0\n","1273  1942     12  20201010070550.png    5.0\n","\n","[179 rows x 4 columns]\n","179\n","---------------------------------------------------\n","        ID series            filename  class\n","36    1805      6  20200528204518.png    4.0\n","37    1805      6  20200528204506.png    4.0\n","51     197      2  20202 139 5211.png    4.0\n","62    1718      6  20200525213412.png    4.0\n","63    1718      6  20200525213403.png    4.0\n","...    ...    ...                 ...    ...\n","1099  1700      6  20200525194103.png    4.0\n","1126  1798      6  20200528203737.png    4.0\n","1127  1798      6  20200528203726.png    4.0\n","1133   249      2  20202 14114618.png    4.0\n","1144  1702      6  20200525194340.png    4.0\n","\n","[111 rows x 4 columns]\n","111\n","---------------------------------------------------\n","        ID series            filename  class\n","10    1648      7  20200519080842.png    3.0\n","20    1883      9  20200604124819.png    3.0\n","79    1736      7  20200526174328.png    3.0\n","114   1603      1  20200515193120.png    3.0\n","115   1603      1  20200515193051.png    3.0\n","...    ...    ...                 ...    ...\n","1250  1663      7  20200519083012.png    3.0\n","1251  1663      7  20200519082959.png    3.0\n","1252  1868      9  20200604084051.png    3.0\n","1256  1645      7  20200518133511.png    3.0\n","1257  1645      7  20200518133456.png    3.0\n","\n","[110 rows x 4 columns]\n","110\n","---------------------------------------------------\n","        ID series            filename  class\n","12     146      1  20202 119 2826.png    2.0\n","13     146      1  20202 119 2815.png    2.0\n","21    1883      9  20200604124805.png    2.0\n","72     355      3  20203 2 8 3924.png    2.0\n","90     160      1  20202 119 5125.png    2.0\n","...    ...    ...                 ...    ...\n","1200    74      1  20202 10141 4 .png    2.0\n","1201    74      1  20202 10140 50.png    2.0\n","1230   107      1  20202 10152451.png    2.0\n","1231   107      1  20202 10152438.png    2.0\n","1253  1868      9  20200604084035.png    2.0\n","\n","[132 rows x 4 columns]\n","132\n","---------------------------------------------------\n","        ID series            filename  class\n","11    1648      7  20200519080822.png    1.0\n","27     366      3  20203 2 9 3932.png    1.0\n","56    1752      7  20200526194818.png    1.0\n","57    1752      7  20200526194809.png    1.0\n","61    1748      7  20200526193405.png    1.0\n","...    ...    ...                 ...    ...\n","1063  1610      1  20200515193828.png    1.0\n","1128  1742      7  20200526192944.png    1.0\n","1129  1742      7  20200526192932.png    1.0\n","1176  1613      1  20200515194253.png    1.0\n","1178  1766      9  20200528174300.png    1.0\n","\n","[83 rows x 4 columns]\n","83\n"]}],"source":["#print(train_balance_df)\n","#print(len(train_balance_df))\n","\n","\n","#Estraggo solo le immagini di un solo lato perchè altrimenti geometrie sono opposte - SE USIAMO LE PATCH NON ABBIAMO BISOGNO DI SPEZZARE IL DATASET PERCHÈ LA GEOMETRIA NON C'È PIÙ\n","#-----PER IMMAGINI INTERE\n","#train_balance_df_lato_1 = train_balance_df[::2]\n","\n","#-----PER IMMAGINI PATCH PRENDO TUTTO IL DATASET CON CLASSI DI QUALITÀ\n","train_balance_df_lato_1 = train_balance_df\n","'''\n","print(len(train_balance_df_lato_1))\n","print(train_balance_df_lato_1) #Prendo solo quelle da un lato, quindi o solo le pari o solo le dispari\n","'''\n","\n","df_class_0 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 0]\n","print(df_class_0)\n","print(len(df_class_0))\n","\n","print(\"---------------------------------------------------\")\n","df_class_9 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 9]\n","print(df_class_9)\n","print(len(df_class_9))\n","\n","print(\"---------------------------------------------------\")\n","df_class_8 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 8]\n","print(df_class_8)\n","print(len(df_class_8))\n","\n","print(\"---------------------------------------------------\")\n","df_class_7 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 7]\n","print(df_class_7)\n","print(len(df_class_7))\n","\n","print(\"---------------------------------------------------\")\n","df_class_6 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 6]\n","print(df_class_6)\n","print(len(df_class_6))\n","\n","print(\"---------------------------------------------------\")\n","df_class_5 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 5]\n","print(df_class_5)\n","print(len(df_class_5))\n","\n","print(\"---------------------------------------------------\")\n","df_class_4 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 4]\n","print(df_class_4)\n","print(len(df_class_4))\n","\n","print(\"---------------------------------------------------\")\n","df_class_3 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 3]\n","print(df_class_3)\n","print(len(df_class_3))\n","\n","print(\"---------------------------------------------------\")\n","df_class_2 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 2]\n","print(df_class_2)\n","print(len(df_class_2))\n","\n","print(\"---------------------------------------------------\")\n","df_class_1 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 1]\n","print(df_class_1)\n","print(len(df_class_1))\n"]},{"cell_type":"markdown","source":["##### Custom Dataset 2\n","\n","(da utilizzare solo per salvare le patch delle immagini in ciascuna cartella)\n","(QUESTO E' STATO FATTO SOLO PER EVITARE DI RIESEGUIRE I PEZZI DI CODICI SOPRA ED ESEGUIRE LA GAN DIRETTAMENTE UTILIZZANDO LA CARTELLA DI PATCH DI UNA CERTA CLASSE DI QUALITÀ)"],"metadata":{"id":"9p_z8eSL7Rvs"}},{"cell_type":"code","source":["#Custom dataset to SAVE PATCHES\n","class CustomDataset2(Dataset):\n","  def __init__ (self, dataframe, transform_0=None, transform_1 = None, transform_2 = None, transform_3 = None, weight=None, mode = None):\n","    self.transform_0 = transform_0\n","    self.transform_1 = transform_1\n","    self.transform_2 = transform_2\n","    self.transform_3 = transform_3\n","    self.mode = mode\n","    self.weight = weight\n","    self.dataframe = dataframe\n","  def __len__(self):\n","    return len(self.dataframe)\n","  \n","  #ho dovuto aggiungerlo perché l'imbalance dataset sampler lo chiedeva - per il SAMPLER di molte versioni di codice fà\n","  def get_labels(self):\n","    print(self.dataframe['class'])\n","    return self.dataframe['class']\n","\n","  def __getitem__(self, index):\n","    path = self.dataframe.iloc[index, 2]\n","    img_path = os.path.join(path_images+path)\n","    image = io.imread(img_path)\n","    y_label_class = torch.tensor(int(self.dataframe.iloc[index, 3]))              \n","    y_label_series = torch.tensor(int(self.dataframe.iloc[index, 1]))\n","\n","    if self.mode == 'train':\n","      if self.dataframe.iloc[index, 4] == 'T1': \n","        #print('transform T1')\n","        image = self.transform_1(image)\n","      if self.dataframe.iloc[index, 4] == 'T2':\n","        #print('transform T2')\n","        image = self.transform_2(image)\n","      if self.dataframe.iloc[index, 4] == 'T3':\n","        #print('transform T3')\n","        image = self.transform_3(image)\n","\n","    if self.transform_0: \n","      #print('transform generale')\n","      image = self.transform_0(image)\n","      #save_image(image, \"patches/class{0}.0/patch_{1}\".format(y_label_class,path), normalize=True, cmap='gray')    #sbloccare quando serve di salvare le patch\n","\n","      #ALTERNATIVA DI SALVATAGGIO IMMAGINE\n","      #torch.save(image, 'data_drive_path{}'.format(idx))\n","    \n","    return (image, y_label_class, y_label_series)\n"],"metadata":{"id":"DW6KP1b07Scb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":547,"status":"ok","timestamp":1656869656494,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"_uElwqndtHlX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3716fc29-5bbe-4038-e196-7c80215ed623"},"outputs":[{"output_type":"stream","name":"stderr","text":["1it [00:00,  1.66it/s]"]},{"output_type":"stream","name":"stdout","text":["0\n","DATAFRAME CON PATCHES\n","        ID series                                   filename  class\n","66    1817      4  patches/class9.0/patch_20200529172017.png    9.0\n","67    1817      4  patches/class9.0/patch_20200529171958.png    9.0\n","70    1836     10  patches/class9.0/patch_20200529194441.png    9.0\n","71    1836     10  patches/class9.0/patch_20200529194427.png    9.0\n","106   1689      8  patches/class9.0/patch_20200525181453.png    9.0\n","...    ...    ...                                        ...    ...\n","1218   637      4  patches/class9.0/patch_20200506131733.png    9.0\n","1244   473      4  patches/class9.0/patch_20203 108 2449.png    9.0\n","1245   473      4  patches/class9.0/patch_20203 108 2438.png    9.0\n","1246   498      4  patches/class9.0/patch_20203 109 5735.png    9.0\n","1247   498      4  patches/class9.0/patch_20203 109 5720.png    9.0\n","\n","[62 rows x 4 columns]\n","        ID series                                   filename  class\n","106   1689      8  patches/class9.0/patch_20200525181453.png    9.0\n","623   1670      4  patches/class9.0/patch_20200525162207.png    9.0\n","562    633      4  patches/class9.0/patch_20200506125957.png    9.0\n","438    449      4  patches/class9.0/patch_20203 3 8 2729.png    9.0\n","573    628      4  patches/class9.0/patch_20200506125045.png    9.0\n","...    ...    ...                                        ...    ...\n","67    1817      4  patches/class9.0/patch_20200529171958.png    9.0\n","107   1689      8  patches/class9.0/patch_20200525181440.png    9.0\n","1188  1826      4  patches/class9.0/patch_20200529181353.png    9.0\n","933    632      4  patches/class9.0/patch_20200506125503.png    9.0\n","1244   473      4  patches/class9.0/patch_20203 108 2449.png    9.0\n","\n","[62 rows x 4 columns]\n","62\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["'''ricavare il dataset di immagini per una specifica classe di qualità'''\n","\n","#SAVE PATCHES EXTRACTED FROM IMAGES\n","#cartelle in cui salvare i patches : /drive/CALCIO_CROP_BASE/patches/...\n","os.makedirs(\"patches/class0.0\", exist_ok=True)\n","os.makedirs(\"patches/class9.0\", exist_ok=True)\n","os.makedirs(\"patches/class8.0\", exist_ok=True)\n","os.makedirs(\"patches/class7.0\", exist_ok=True)\n","os.makedirs(\"patches/class6.0\", exist_ok=True)\n","os.makedirs(\"patches/class5.0\", exist_ok=True)\n","os.makedirs(\"patches/class4.0\", exist_ok=True)\n","os.makedirs(\"patches/class3.0\", exist_ok=True)\n","os.makedirs(\"patches/class2.0\", exist_ok=True)\n","os.makedirs(\"patches/class1.0\", exist_ok=True)\n","\n","#------------------------------------------------------MODIFICARE SEMPRE DA QUA SOTTO---------------------------------------------------\n","#---------------------------------------------------------------------------------------------------------------------------------------\n","\n","#IN QUESTA RIGA SOTTO UTILIZZARE COME PRIMO PARAMETRO IL DATASET CHE SI VUOLE USARE PER ESTRARRE IL PATCH\n","train_c_dataset_patches = CustomDataset2(df_class_9, transform_0=_transform_GAN)                                                  #MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","trainloader_patches = torch.utils.data.DataLoader(dataset = train_c_dataset_patches, batch_size=bs, shuffle=True)#,num_workers=2\n","\n","#ITERO GLI ELEMENTI DEL DATALOADER PER RICAVARE I PATCH\n","for i, (data, targets, targets2) in tqdm(enumerate(trainloader_patches)):\n","  print(i)\n","\n","\n","#CREO IL NUOVO DATASET DA USARE COI PATCH PER LA GAN, SENZA MODIFICARE IL PRECEDENTE\n","mask_filenames = []\n","IDs = []\n","classes = []\n","for index, row in df_class_9.iterrows():                                                                                          #MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","    filename = row['filename']\n","    mask_filenames.append(str(\"patches/class{0}/patch_{1}\".format(row['class'],filename)))\n","\n","    IDs.append(row['ID'])\n","    classes.append(row['class'])\n","\n","print(\"DATAFRAME CON PATCHES\")\n","df_class_new = df_class_9.copy()                                                                                                  #MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","df_class_new.drop('filename', axis='columns', inplace=True)   \n","df_class_new['filename'] = mask_filenames\n","\n","column_names = [\"ID\",\"series\", \"filename\", \"class\"]\n","df_class_new = df_class_new.reindex(columns=column_names)\n","print(df_class_new)\n","\n","\n","#ATTIVA SOLO SE I RISULTATI CON TROPPE IMMAGINI PORTANO A GENERAZIONI NON BUONE, COSì GLI SI DANNO MENO IMMAGINI O SVALVOLA\n","#df_class_new = df_class_new[40:71]\n","df_class_new = df_class_new.sample(n = 62, random_state = 4)                                                                      #MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","print(df_class_new)\n","print(len(df_class_new))\n"]},{"cell_type":"markdown","metadata":{"id":"9rWuwr7bXGLI"},"source":["## SAMPLER "]},{"cell_type":"markdown","metadata":{"id":"8ohcNPBHMrq-"},"source":["#### Classe BalancedBatchSampler \n","\n","SERVE A CREARE IL BATCH RAPPRESENTATIVO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGtHdrBHwQbO"},"outputs":[],"source":["import torch\n","is_torchvision_installed = True\n","try:\n","    import torchvision\n","except:\n","    is_torchvision_installed = False\n","import torch.utils.data\n","import random\n","\n","class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n","    def __init__(self, dataset, labels=None):\n","        self.labels = labels\n","        self.dataset = dict()\n","        self.balanced_max = 0\n","\n","        #prova denis \n","        self.number_of_samples = 0\n","\n","        # Save all the indices for all the classes\n","        for idx in range(0, len(dataset)):\n","            label = self._get_label(dataset, idx)\n","            if label not in self.dataset:\n","                self.dataset[label] = list()\n","            self.dataset[label].append(idx)\n","            self.balanced_max = len(self.dataset[label]) \\\n","                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n","        \n","        # Oversample the classes with fewer elements than the max\n","        for label in self.dataset:\n","            while len(self.dataset[label]) < self.balanced_max:\n","                self.dataset[label].append(random.choice(self.dataset[label]))\n","        self.keys = list(self.dataset.keys())\n","        self.currentkey = 0\n","        self.indices = [-1]*len(self.keys)\n","        self.number_of_samples = self.balanced_max * len(self.dataset)\n","        \n","    def __iter__(self):\n","        while self.indices[self.currentkey] < self.balanced_max - 1:\n","            self.indices[self.currentkey] += 1\n","            yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]\n","            self.currentkey = (self.currentkey + 1) % len(self.keys)\n","        self.indices = [-1]*len(self.keys)\n","    \n","    def _get_label(self, dataset, idx, labels = None):\n","        if self.labels is not None:\n","            return self.labels[idx].item()\n","        else:\n","            # Trying guessing\n","            dataset_type = type(dataset)\n","            if is_torchvision_installed and dataset_type is torchvision.datasets.MNIST:\n","                return dataset.train_labels[idx].item()\n","            elif is_torchvision_installed and dataset_type is torchvision.datasets.ImageFolder:\n","                return dataset.imgs[idx][1]\n","            else:\n","                raise Exception(\"You should pass the tensor of labels to the constructor as second argument\")\n","\n","    def __len__(self):\n","        return self.balanced_max*len(self.keys)\n"]},{"cell_type":"markdown","metadata":{"id":"oTOxIN2g2Jcv"},"source":["#### Imbalanced Dataset Sampler \n","(LINK: https://github.com/ufoym/imbalanced-dataset-sampler) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2928,"status":"ok","timestamp":1656869661318,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"YiTVi6842Bi7","outputId":"ec9d1827-ea5a-4cc5-f841-46322a68022b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchsampler in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsampler) (1.3.5)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/dist-packages (from torchsampler) (0.12.0+cu113)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from torchsampler) (4.11.4)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from torchsampler) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->torchsampler) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->torchsampler) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->torchsampler) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->torchsampler) (2.23.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->torchsampler) (3.8.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsampler) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsampler) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsampler) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5->torchsampler) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5->torchsampler) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5->torchsampler) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5->torchsampler) (2022.6.15)\n"]}],"source":["#!pip install https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip --quiet --ignore-installed #--- qui va in errore, serviva per creare il batch rappresentativo\n","!pip install torchsampler\n","from torchsampler import ImbalancedDatasetSampler\n","\n","#sampler_imbalance = ImbalancedDatasetSampler(train_c_dataset)"]},{"cell_type":"markdown","metadata":{"id":"-gwtHXxECiTK"},"source":["## Creazione DataLoader TRAIN, VAL, TEST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ty1I-hVWryqq"},"outputs":[],"source":["#QUA SOTTO SI TENGONO DA PARTE TUTTI I SET ESTRATTI VOLTA PER VOLTA SE DOVESSERO RISERVIRE PER TESTS\n","\n","#---------NO DATA AUGMENTATION\n","#train_c_dataset = CustomDataset(train_balance_df, transform_0=_transform_)\n"," \n","\n","#---------PER GAN\n","train_c_dataset = CustomDataset(df_class_new, transform_0=_transform_GAN)  \n","val_c_dataset = CustomDataset(val_balance_df, transform_0=_transform_)\n","test_c_dataset = CustomDataset(test_balance_df,transform_0=_transform_ )\n","\n","#---------PER CLASSIFICATORE CON IMMAGINI AUGMENTATION GAN OFFLINE\n","#train_c_dataset = CustomDataset(train_balance_df_GAN, transform_0=_transform_train) \n","#val_c_dataset = CustomDataset(val_balance_df, transform_0=_transform_)\n","#test_c_dataset = CustomDataset(test_balance_df,transform_0=_transform_ )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":73,"status":"ok","timestamp":1656869661320,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"c7g-y1mkd_YM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cfd6b18c-56d1-4b3d-d32f-dc098d5eb95c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([8., 6., 8., 5., 5., 8., 8., 3., 1., 2., 2., 8., 0., 1., 6., 0., 4., 6.,\n","        1., 5., 5., 2., 3., 3., 5., 5., 2., 2., 4., 4., 5., 4., 5., 8., 6., 7.,\n","        1., 1., 6., 3., 5., 5., 6., 6., 5., 2., 6., 6., 3., 3., 0., 8., 8., 3.,\n","        3., 0., 6., 6., 0., 5., 6., 1., 1., 0., 6., 7., 5., 9., 9., 2., 2., 3.,\n","        9., 9., 2., 5., 8., 4., 4., 3., 6., 4., 5., 4., 8., 8., 7., 8., 2., 5.,\n","        6., 8., 5., 5., 8., 9., 4., 3., 2., 9., 9., 6., 2., 1., 6., 4., 5., 2.,\n","        3., 6., 7., 5., 5., 6., 6., 4., 8., 8., 6., 6., 4., 4., 1., 7., 8., 3.,\n","        1., 7., 7., 0., 1., 4., 4., 6., 8., 8., 5., 3., 6., 6., 7., 8., 0., 0.,\n","        6., 7., 2., 2., 5., 2., 6., 6., 6., 8., 8., 2., 1., 1., 6., 8., 6., 7.,\n","        7., 9., 1., 3., 5., 5., 4., 8., 8., 7., 1., 1., 8., 8., 2., 2., 9., 8.,\n","        6., 6., 5., 1., 1., 4., 4., 3., 3., 8., 8., 8., 9., 6., 6., 2., 2., 6.,\n","        6., 8., 5., 1., 5., 6., 1., 1., 2., 6., 1., 5., 4., 1., 3., 1., 8., 5.,\n","        1., 2., 5., 1., 0., 7., 7., 6., 0., 0., 1., 1., 1., 4., 4., 6., 6., 7.,\n","        7., 8., 8., 5., 4., 7., 7., 2., 3., 2., 2., 8., 8., 4., 4., 8., 8., 8.,\n","        8., 7., 7., 8., 8., 8., 8., 6., 6., 4., 4., 5., 5., 6., 6., 4., 4., 8.,\n","        8., 2., 2., 9., 9., 0., 0., 0., 2., 0., 2., 7., 7., 5., 5., 4., 4., 3.,\n","        3., 9., 9., 7., 8., 6., 6., 5., 6., 0., 0., 9., 9., 8., 8., 7., 7., 5.,\n","        6., 8., 8., 5., 5., 0., 0., 5., 5., 5., 5., 0., 0., 8., 8., 2., 4., 3.,\n","        3., 2., 2., 3., 3., 7., 7., 5., 5., 6., 6., 4., 4., 9., 9., 4., 4., 4.,\n","        4., 7., 7., 3., 3., 5., 5., 8., 8., 8., 8., 7., 7., 4., 4., 5., 5., 5.,\n","        5., 6., 6., 4., 4., 3., 3., 7., 7., 2., 2., 6., 6., 8., 9., 9., 9., 0.,\n","        0., 0., 1., 0., 0., 9., 9., 7., 7., 5., 6., 0., 0., 6., 6., 0., 0., 6.,\n","        6., 2., 2., 1., 3., 1., 2., 2., 1., 8., 8., 4., 4., 8., 8., 7., 6., 0.,\n","        0., 3., 3., 3., 2., 5., 5., 3., 3., 4., 4., 7., 7., 4., 4., 6., 5., 5.,\n","        5., 3., 4., 5., 7., 2., 2., 4., 4., 0., 0., 0., 0., 0., 0., 6., 6., 6.,\n","        6., 2., 2., 4., 4., 8., 8., 2., 2., 5., 6., 6., 6., 7., 7., 9., 9., 0.,\n","        0., 6., 6., 0., 0., 8., 8., 0., 0., 2., 2., 8., 8., 0., 0., 3., 3., 6.,\n","        6., 3., 3., 6., 6., 0., 0., 9., 8., 1., 1., 6., 6., 0., 0., 7., 6., 1.,\n","        1., 6., 6., 8., 8., 8., 6., 7., 7., 4., 4., 8., 8., 5., 5., 2., 3., 4.,\n","        5., 8., 8., 4., 5., 4., 4., 0., 0., 6., 6., 6., 6., 5., 5., 6., 6., 4.,\n","        4., 2., 2., 5., 5., 7., 7., 5., 6., 3., 3., 7., 8., 6., 6., 6., 6., 3.,\n","        3., 5., 5., 5., 5., 2., 2., 6., 6., 4., 4., 1., 1., 5., 5., 5., 4., 6.,\n","        6., 2., 2., 8., 8., 7., 7., 8., 8., 4., 5., 7., 9., 3., 3., 8., 8., 7.,\n","        7., 6., 6., 3., 3., 6., 6., 0., 3., 6., 5., 5., 5., 6., 6., 2., 3., 5.,\n","        5., 5., 5., 8., 8., 1., 2., 1., 3., 5., 5., 8., 8., 5., 5., 7., 7., 9.,\n","        9., 8., 8., 3., 3., 2., 2., 8., 8., 5., 5., 2., 2., 3., 3., 8., 8., 5.,\n","        5., 7., 7., 6., 6., 6., 6., 8., 7., 8., 8., 9., 8., 7., 7., 7., 6., 7.,\n","        7., 3., 3., 8., 8., 2., 2., 7., 7., 7., 7., 5., 5., 5., 5., 5., 6., 7.,\n","        7., 9., 9., 9., 9., 5., 5., 3., 3., 3., 2., 7., 7., 3., 3., 5., 5., 5.,\n","        5., 5., 5., 5., 5., 5., 5., 7., 7., 5., 5., 5., 5., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n","        2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n","        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 4., 4., 4., 4., 4., 4., 4.,\n","        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n","        4., 4., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n","        5., 5., 5., 5., 5., 5., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n","        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n","        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 8., 8., 8., 8., 8., 8., 8., 8.,\n","        8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.,\n","        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n","        9., 9., 9., 9., 9., 9.], dtype=torch.float64)\n","torch.Size([942])\n"]}],"source":["# creating tensor from targets_df \n","torch_tensor = torch.tensor(train_balance_df_GAN['class'].values)\n","\n","# printing out result\n","print(torch_tensor)\n","print(torch_tensor.shape)\n","\n","#HO RICAVATO LE LABELS DEGLI ELEMENTI IN TENSORE DA PASSARE AL BALANCED BATCH SAMPLER"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":66,"status":"ok","timestamp":1656869661321,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Un908UAc2bmH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d360a581-bcd6-47ed-f61e-62194facb427"},"outputs":[{"output_type":"stream","name":"stdout","text":["0       7.0\n","1       8.0\n","2       6.0\n","3       6.0\n","4       8.0\n","       ... \n","2055    9.0\n","2056    9.0\n","2057    9.0\n","2058    9.0\n","2059    9.0\n","Name: class, Length: 2060, dtype: float64\n"]}],"source":["#DATALOADERS WITH SAMPLERS TECHNIQUES\n","\n","'''#BALANCED DATA SAMPLER\n","trainloader = torch.utils.data.DataLoader(\n","    train_c_dataset,\n","    sampler=BalancedBatchSampler(train_c_dataset, torch_tensor),\n","    batch_size=bs\n",")\n","'''\n","\n","#IMBALANCED DATA SAMPLER\n","trainloader = torch.utils.data.DataLoader(\n","    train_c_dataset,\n","    sampler=ImbalancedDatasetSampler(train_c_dataset),\n","    batch_size=bs\n",")\n","\n","\n","\n","#trainloader senza BALANCED Batch Sampler / iMBALANCED\n","#trainloader = torch.utils.data.DataLoader(dataset = train_c_dataset, batch_size=64, shuffle=True)#,num_workers=2\n","\n","valloader = torch.utils.data.DataLoader(dataset = val_c_dataset, batch_size=bs, shuffle=True)#,num_workers=2\n","testloader = torch.utils.data.DataLoader(dataset = test_c_dataset, batch_size=bs, shuffle=True)#,num_workers=2"]},{"cell_type":"markdown","metadata":{"id":"oow7OM0obIF4"},"source":["##GAN EVALUATION METRICS - IMAGE QUALITY "]},{"cell_type":"markdown","metadata":{"id":"DcsBnqNhd10V"},"source":["Quello che si potrbbe fare per avere un valore di riferimento da controllare con le metriche è calcolare la metrica stessa solo su immagini reali così da avere un valore di qualità di riferimento, così come quando applicheremo sotto le metriche per la diversità, da avere anche un rifeirmento sul quanto vengano univoche e quindi di valore .\n","\n","\n","PYTORCH LIBRARY GAN METRICS (FID & IS)\n","https://pypi.org/project/pytorch-gan-metrics/"]},{"cell_type":"markdown","metadata":{"id":"zbbX2PVYy2q5"},"source":["#### FID \n","[GITHUB](https://github.com/hukkelas/pytorch-frechet-inception-distance)\n","\n","- Il valore deve essere il PIÙ BASSO possibile\n","- è un miglioramento dell'IS quindi valuta se sotituire IS con un'altra metrica"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbW-8G0fxDh6"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","from custom_FID import load_images, calculate_fid\n"]},{"cell_type":"markdown","metadata":{"id":"BAc0bXoskKkK"},"source":["#### IS - Inception Score \n","[GITHUB](https://github.com/sbarratt/inception-score-pytorch/blob/master/inception_score.py)\n","\n","- il valore deve essere PIÙ ALTO possibile\n","- serve il CLASSIFICATORE per valutare quante immagini riconosce bene, solo sul set di immagini create\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1N3IGC9DycxK"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","import custom_IS \n","from custom_IS import inception_score"]},{"cell_type":"markdown","metadata":{"id":"lXgnkNAZmc0z"},"source":["####MMD - Maximum Mean Discrepancy \n","\n","maximum_mean_discrepancy\n","[LINK](https://www.kaggle.com/code/onurtunali/maximum-mean-discrepancy/notebook)\n","\n","- il valore deve essere PIÙ BASSO possibile, perchè indica similarità dalle imaggini RAW, ma troppo basso vorrebbe anche dire che potrebbe generarle troppo simili alle originali"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8_qXyVJzFj2"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","#import custom_MMD\n","#from custom_MMD import MMD\n","from custom_MMD import custom_MMD as MMD"]},{"cell_type":"markdown","metadata":{"id":"pMvexlvYe5xD"},"source":["##GAN EVALUATION METRICS - IMAGE DIVERSITY\n","\n","(se fosse la metrica migliore con quelle generate significherebbe che la GAN genera immagini significative, e quindi limiterebbe anche overfitting in partenza)"]},{"cell_type":"markdown","metadata":{"id":"MG_zlz7PfzZC"},"source":["####NDB -\n","\n","- Per valautare la diversità delle immagini e se c'è MODE COLLAPSE\n","\n","- per avere un risultato buono, l'andamento deve abbassarsi durante l'allenamento e comunque tendere a 0 il pià possibile"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4279,"status":"ok","timestamp":1656869665556,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kcusRj12f5K_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"687273a4-634d-444a-cd37-39148e975f3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Seed:  999\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.20)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.6.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n"]}],"source":["'''IMPORTS NEEDED FOR THE NDB IMPLEMENTATION'''\n","\n","#https://github.com/yhlleo/GAN-Metrics/blob/master/scores/ndb_jsd.py - VERSIONE CHE NON MI CONVINCE \n","\n","#https://colab.research.google.com/drive/1fGrFl5UzYc3upShr25Hv8VfqyzhZOPTM?usp=sharing\n","\n","#https://wandb.ai/authors/DCGAN-ndb-test/reports/Draft--VmlldzoxNDcyMjc - INFO\n","\n","from __future__ import print_function\n","#%matplotlib inline\n","import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import time\n","import gc\n","\n","# Set random seed for reproducibility\n","manualSeed = 999\n","#manualSeed = random.randint(1, 10000) # use if you want new results\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","np.random.seed(999)\n","\n","#Install + import Weights and Biases\n","!pip install --upgrade wandb\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_ICp_tG3ymk"},"outputs":[],"source":["'''CLASS DEFINITION NDB'''\n","\n","import os\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from scipy.stats import norm\n","from matplotlib import pyplot as plt\n","import pickle as pkl\n","\n","\n","'''The NDB score is one way to measure the effects of mode collapse quantitatively.\n","\n","We show that logging the score during training can be a good way to detect mode collapse if we don't see a substantial decrease in the score over time. \n","The key takeaway is that it's useful to have another quantitative metric to measure such an important issue such as mode collapse instead of purely relying on visual methods or IS/FID scores\n","'''\n","\n","\n","class NDB:\n","    def __init__(self, training_data=None, number_of_bins=100, significance_level=0.05, z_threshold=None,\n","                 whitening=False, max_dims=None, cache_folder=None):\n","        \"\"\"\n","        NDB Evaluation Class\n","        :param training_data: Optional - the training samples - array of m x d floats (m samples of dimension d)\n","        :param number_of_bins: Number of bins (clusters) default=100\n","        :param significance_level: The statistical significance level for the two-sample test\n","        :param z_threshold: Allow defining a threshold in terms of difference/SE for defining a bin as statistically different\n","        :param whitening: Perform data whitening - subtract mean and divide by per-dimension std\n","        :param max_dims: Max dimensions to use in K-means. By default derived automatically from d\n","        :param bins_file: Optional - file to write / read-from the clusters (to avoid re-calculation)\n","        \"\"\"\n","        \n","        #Added\n","        self.original_bin_centers = None\n","        self.count = None\n","        self.bin_order = None\n","        \n","        #Original\n","        self.number_of_bins = number_of_bins\n","        self.significance_level = significance_level\n","        self.z_threshold = z_threshold\n","        self.whitening = whitening\n","        self.ndb_eps = 1e-6\n","        self.training_mean = 0.0\n","        self.training_std = 1.0\n","        self.max_dims = max_dims\n","        self.cache_folder = cache_folder\n","        self.bin_centers = None\n","        self.bin_proportions = None\n","        self.ref_sample_size = None\n","        self.used_d_indices = None\n","        self.results_file = None\n","        self.test_name = 'ndb_{}_bins_{}'.format(self.number_of_bins, 'whiten' if self.whitening else 'orig')\n","        self.cached_results = {}\n","        if self.cache_folder:\n","            self.results_file = os.path.join(cache_folder, self.test_name+'_results.pkl')\n","            if os.path.isfile(self.results_file):\n","                # print('Loading previous results from', self.results_file, ':')\n","                self.cached_results = pkl.load(open(self.results_file, 'rb'))\n","                # print(self.cached_results.keys())\n","        if training_data is not None or cache_folder is not None:\n","                bins_file = None\n","                if cache_folder:\n","                    os.makedirs(cache_folder, exist_ok=True)\n","                    bins_file = os.path.join(cache_folder, self.test_name+'.pkl')\n","                self.construct_bins(training_data, bins_file)\n","\n","    def construct_bins(self, training_samples, bins_file):\n","        \"\"\"\n","        Performs K-means clustering of the training samples\n","        :param training_samples: An array of m x d floats (m samples of dimension d)\n","        \"\"\"\n","\n","        if self.__read_from_bins_file(bins_file):\n","            return\n","        n, d = training_samples.shape\n","        k = self.number_of_bins\n","        if self.whitening:\n","            self.training_mean = np.mean(training_samples, axis=0)\n","            self.training_std = np.std(training_samples, axis=0) + self.ndb_eps\n","\n","        if self.max_dims is None and d > 1000:\n","            # To ran faster, perform binning on sampled data dimension (i.e. don't use all channels of all pixels)\n","            self.max_dims = d//6\n","\n","        whitened_samples = (training_samples-self.training_mean)/self.training_std\n","        d_used = d if self.max_dims is None else min(d, self.max_dims)\n","        self.used_d_indices = np.random.choice(d, d_used, replace=False)\n","\n","        #print('Performing K-Means clustering of {} samples in dimension {} / {} to {} clusters ...'.format(n, d_used, d, k))\n","        #print('Can take a couple of minutes...')\n","        if n//k > 1000:\n","            print('Training data size should be ~500 times the number of bins (for reasonable speed and accuracy)')\n","\n","        clusters = KMeans(n_clusters=k, max_iter=100).fit(whitened_samples[:, self.used_d_indices])\n","        #clusters = KMeans(n_clusters=k, max_iter=100, n_jobs=-1).fit(whitened_samples[:, self.used_d_indices])\n","\n","\n","        bin_centers = np.zeros([k, d])\n","        for i in range(k):\n","            bin_centers[i, :] = np.mean(whitened_samples[clusters.labels_ == i, :], axis=0)\n","        \n","        self.original_bin_centers = bin_centers\n","        #print(\"Bin centers: \", bin_centers.shape)\n","        # Organize bins by size (largest bin -> smallest bin)\n","        label_vals, label_counts = np.unique(clusters.labels_, return_counts=True)\n","        self.count = list(zip(label_vals, label_counts))\n","        self.count.sort(key=lambda tup: tup[1], reverse=True)\n","        bin_order = np.argsort(-label_counts)\n","        self.bin_order = bin_order\n","        self.bin_proportions = label_counts[bin_order] / np.sum(label_counts)\n","        self.bin_centers = bin_centers[bin_order, :]\n","        self.ref_sample_size = n\n","        self.__write_to_bins_file(bins_file)\n","        print('Done.')\n","\n","    def evaluate(self, query_samples, model_label=None):\n","        \"\"\"\n","        Assign each sample to the nearest bin center (in L2). Pre-whiten if required. and calculate the NDB\n","        (Number of statistically Different Bins) and JS divergence scores.\n","        :param query_samples: An array of m x d floats (m samples of dimension d)\n","        :param model_label: optional label string for the evaluated model, allows plotting results of multiple models\n","        :return: results dictionary containing NDB and JS scores and array of labels (assigned bin for each query sample)\n","        \"\"\"\n","        n = query_samples.shape[0]\n","        query_bin_proportions, query_bin_assignments = self.__calculate_bin_proportions(query_samples)\n","        # print(query_bin_proportions)\n","        different_bins = NDB.two_proportions_z_test(self.bin_proportions, self.ref_sample_size, query_bin_proportions,\n","                                                    n, significance_level=self.significance_level,\n","                                                    z_threshold=self.z_threshold)\n","        ndb = np.count_nonzero(different_bins)\n","        js = NDB.jensen_shannon_divergence(self.bin_proportions, query_bin_proportions)\n","        results = {'NDB': ndb,\n","                   'JS': js,\n","                   'Proportions': query_bin_proportions,\n","                   'N': n,\n","                   'Bin-Assignment': query_bin_assignments,\n","                   'Different-Bins': different_bins}\n","\n","        if model_label:\n","            #print('Results for {} samples from {}: '.format(n, model_label), end='')\n","            self.cached_results[model_label] = results\n","            if self.results_file:\n","                # print('Storing result to', self.results_file)\n","                pkl.dump(self.cached_results, open(self.results_file, 'wb'))\n","\n","        #print('NDB =', ndb, 'NDB/K =', ndb/self.number_of_bins, ', JS =', js)\n","        return results\n","\n","    def print_results(self):\n","        print('NSB results (K={}{}):'.format(self.number_of_bins, ', data whitening' if self.whitening else ''))\n","        for model in sorted(list(self.cached_results.keys())):\n","            res = self.cached_results[model]\n","            print('%s: NDB = %d, NDB/K = %.3f, JS = %.4f' % (model, res['NDB'], res['NDB']/self.number_of_bins, res['JS']))\n","\n","    def plot_results(self, models_to_plot=None):\n","        \"\"\"\n","        Plot the binning proportions of different methods\n","        :param models_to_plot: optional list of model labels to plot\n","        \"\"\"\n","        K = self.number_of_bins\n","        w = 1.0 / (len(self.cached_results)+1)\n","        assert K == self.bin_proportions.size\n","        assert self.cached_results\n","\n","        # Used for plotting only\n","        def calc_se(p1, n1, p2, n2):\n","            p = (p1 * n1 + p2 * n2) / (n1 + n2)\n","            return np.sqrt(p * (1 - p) * (1/n1 + 1/n2))\n","\n","        if not models_to_plot:\n","            models_to_plot = sorted(list(self.cached_results.keys()))\n","\n","        # Visualize the standard errors using the train proportions and size and query sample size\n","        train_se = calc_se(self.bin_proportions, self.ref_sample_size,\n","                           self.bin_proportions, self.cached_results[models_to_plot[0]]['N'])\n","        plt.bar(np.arange(0, K)+0.5, height=train_se*2.0, bottom=self.bin_proportions-train_se,\n","                width=1.0, label='Train$\\pm$SE', color='gray')\n","\n","        ymax = 0.0\n","        for i, model in enumerate(models_to_plot):\n","            results = self.cached_results[model]\n","            label = '%s (%i : %.4f)' % (model, results['NDB'], results['JS'])\n","            ymax = max(ymax, np.max(results['Proportions']))\n","            if K <= 70:\n","                plt.bar(np.arange(0, K)+(i+1.0)*w, results['Proportions'], width=w, label=label)\n","            else:\n","                plt.plot(np.arange(0, K)+0.5, results['Proportions'], '--*', label=label)\n","        plt.legend(loc='best')\n","        plt.ylim((0.0, min(ymax, np.max(self.bin_proportions)*4.0)))\n","        plt.grid(True)\n","        plt.title('Binning Proportions Evaluation Results for {} bins (NDB : JS)'.format(K))\n","        plt.show()\n","\n","    def __calculate_bin_proportions(self, samples):\n","        if self.bin_centers is None:\n","            print('First run construct_bins on samples from the reference training data')\n","        assert samples.shape[1] == self.bin_centers.shape[1]\n","        n, d = samples.shape\n","        k = self.bin_centers.shape[0]\n","        D = np.zeros([n, k], dtype=samples.dtype)\n","\n","        #print('Calculating bin assignments for {} samples...'.format(n))\n","        whitened_samples = (samples-self.training_mean)/self.training_std\n","        for i in range(k):\n","            print('.', end='', flush=True)\n","            D[:, i] = np.linalg.norm(whitened_samples[:, self.used_d_indices] - self.bin_centers[i, self.used_d_indices],\n","                                     ord=2, axis=1)\n","        print()\n","        labels = np.argmin(D, axis=1)\n","        probs = np.zeros([k])\n","        label_vals, label_counts = np.unique(labels, return_counts=True)\n","        probs[label_vals] = label_counts / n\n","        return probs, labels\n","\n","    def __read_from_bins_file(self, bins_file):\n","        if bins_file and os.path.isfile(bins_file):\n","            print('Loading binning results from', bins_file)\n","            bins_data = pkl.load(open(bins_file,'rb'))\n","            self.bin_proportions = bins_data['proportions']\n","            self.bin_centers = bins_data['centers']\n","            self.ref_sample_size = bins_data['n']\n","            self.training_mean = bins_data['mean']\n","            self.training_std = bins_data['std']\n","            self.used_d_indices = bins_data['d_indices']\n","            return True\n","        return False\n","\n","    def __write_to_bins_file(self, bins_file):\n","        if bins_file:\n","            print('Caching binning results to', bins_file)\n","            bins_data = {'proportions': self.bin_proportions,\n","                         'centers': self.bin_centers,\n","                         'n': self.ref_sample_size,\n","                         'mean': self.training_mean,\n","                         'std': self.training_std,\n","                         'd_indices': self.used_d_indices}\n","            pkl.dump(bins_data, open(bins_file, 'wb'))\n","\n","    @staticmethod\n","    def two_proportions_z_test(p1, n1, p2, n2, significance_level, z_threshold=None):\n","        # Per http://stattrek.com/hypothesis-test/difference-in-proportions.aspx\n","        # See also http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/binotest.htm\n","        p = (p1 * n1 + p2 * n2) / (n1 + n2)\n","        se = np.sqrt(p * (1 - p) * (1/n1 + 1/n2))\n","        z = (p1 - p2) / se\n","        # Allow defining a threshold in terms as Z (difference relative to the SE) rather than in p-values.\n","        if z_threshold is not None:\n","            return abs(z) > z_threshold\n","        p_values = 2.0 * norm.cdf(-1.0 * np.abs(z))    # Two-tailed test\n","        return p_values < significance_level\n","\n","    @staticmethod\n","    def jensen_shannon_divergence(p, q):\n","        \"\"\"\n","        Calculates the symmetric Jensen–Shannon divergence between the two PDFs\n","        \"\"\"\n","        m = (p + q) * 0.5\n","        return 0.5 * (NDB.kl_divergence(p, m) + NDB.kl_divergence(q, m))\n","\n","    @staticmethod\n","    def kl_divergence(p, q):\n","        \"\"\"\n","        The Kullback–Leibler divergence.\n","        Defined only if q != 0 whenever p != 0.\n","        \"\"\"\n","        assert np.all(np.isfinite(p))\n","        assert np.all(np.isfinite(q))\n","        assert not np.any(np.logical_and(p != 0, q == 0))\n","\n","        p_pos = (p > 0)\n","        return np.sum(p[p_pos] * np.log(p[p_pos] / q[p_pos]))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1656869665908,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"qV6UxgXT6AXI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"340798f2-6d51-4f44-a615-a3c1031bb82b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef test_ndb():  \\n    dim=100\\n    k=50\\n    n_train = k*100\\n    n_test = k*10\\n\\n    train_samples = np.random.uniform(size=[n_train, dim])\\n    ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\\n\\n    test_samples = np.random.uniform(high=1.0, size=[n_test, dim])\\n    results = ndb.evaluate(test_samples, model_label=\\'Test\\')\\n    print(results[\\'Bin-Assignment\\'])\\n    print\\n    print(ndb.bin_order)\\n    print()\\n    print(ndb.count)\\n    print()\\n    unique, counts = np.unique(results[\"Bin-Assignment\"], return_counts=True)\\n    zipped = list(zip(unique, counts))\\n    zipped.sort(key=lambda tup: tup[1], reverse=True)\\n    print(zipped)\\n\\n    \\ntest_ndb()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":216}],"source":["'''TEST DI NDB SU MATRICI A CASO'''\n","\n","'''\n","def test_ndb():  \n","    dim=100\n","    k=50\n","    n_train = k*100\n","    n_test = k*10\n","\n","    train_samples = np.random.uniform(size=[n_train, dim])\n","    ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\n","\n","    test_samples = np.random.uniform(high=1.0, size=[n_test, dim])\n","    results = ndb.evaluate(test_samples, model_label='Test')\n","    print(results['Bin-Assignment'])\n","    print\n","    print(ndb.bin_order)\n","    print()\n","    print(ndb.count)\n","    print()\n","    unique, counts = np.unique(results[\"Bin-Assignment\"], return_counts=True)\n","    zipped = list(zip(unique, counts))\n","    zipped.sort(key=lambda tup: tup[1], reverse=True)\n","    print(zipped)\n","\n","    \n","test_ndb()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZ65bxNmdQ7a"},"outputs":[],"source":["'''DEFINIZIONE PARAMETRI NDB'''\n","\n","#wandb.init(entity=\"authors\", project=\"GAN-ndb-test\")\n","\n","#NBD Scoring\n","\n","numTrainBatches = 1 #156  \n","\n","numTestBatches = 1 #39\n","\n","#number of bins for NBD (number of clusters)\n","k = 5\n","\n","#number of color channels\n","nc = 3 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoadtFIpZmDe"},"outputs":[],"source":["'''DEFINIZIONE METODO PER CREARE UN SET DI BATCHES DI IMMAGINI COL 'GENERATOR'  '''\n","\n","def generated_fakes(numTestBatches, netG, image_size1, image_size2, batch_size, lat_space_t=None):\n","\n","    generated_batches = []\n","\n","    '''questo parametro serve per fare le elaborazioni con Optuna, quindi se non si esegue riportiamo lat_space a dimensione fissa'''\n","    if lat_space_t == None:\n","        lat_space_t=100\n","\n","    for i in range(numTestBatches):\n","        '''- MEMORIZZO IL SET DI IMMAGINI GENERATE PER USARLO NELLA METRICA 'NDB' -'''\n","        z = Variable(Tensor(np.random.normal(0, 1, (data.shape[0], lat_space_t)))) \n","        gen_imgs = netG(z)\n","        fake_imgs = gen_imgs.detach().cpu().numpy()\n","        generated_batches.append(fake_imgs)\n","        '''------------------------------------------------------------------------'''\n","\n","    generated_batches = np.array(generated_batches)\n","    # (1, 62, 3, 256, 256)\n","\n","\n","    #Display a sample\n","    #plt.imshow(np.transpose(generated_batches[0][0], (1,2,0)))\n","    \n","    gen_combined = generated_batches.reshape(numTestBatches*batch_size, 3, image_size1, image_size2)     #62 perchè il batch non è completo\n","    # (62, 3, 256, 256)\n","  \n","    del generated_batches\n","    gc.collect()\n","\n","\n","\n","    # gen_combined = generated_batches[0]\n","    # for i in range(1,len(generated_batches)):\n","    #     gen_combined = np.concatenate((gen_combined, generated_batches[i]))\n","    \n","    return gen_combined\n","\n","\n","def real_samples(numTrainBatches, dataloader, nc, image_size1, image_size2, batch_size):\n","    #Get real samples (to reduce training time take 40% of original data - 80000 samples)\n","\n","    '''--------------------parametri per calcolare NDB score-----------------'''\n","    batches_done = 0\n","    real_batches = [] #to save real images to be used with NDB METRIC\n","    generated_batches = [] #to save real images to be used with NDB METRIC\n","    iters = 0\n","    img_list = []\n","    metrics = []\n","    ndb_scores = []\n","    real_batches = []\n","\n","\n","    for i, (data, targets, targets2) in tqdm(enumerate(trainloader)):\n","\n","        if(i >= numTrainBatches):\n","            break\n","\n","        '''- MEMORIZZO IL SET DI IMMAGINI REALI PER USARLO NELLA METRICA 'NDB' -'''\n","        real = data.numpy()\n","        real_batches.append(real)\n","\n","    '''RIADATTAMENTO DELLE IMMAGINI REALI & GENERATE PER METRICA 'NDB' '''\n","    real_batches = np.array(real_batches)\n","\n","    print(len(real_batches))\n","    #Display a sample\n","    #plt.imshow(np.transpose(real_batches[0][0], (1,2,0)))\n","\n","    real_combined = real_batches.reshape(numTrainBatches*batch_size, nc, image_size1, image_size2)                #QUANDO NON TORNERANNO LE MISURE AL POSTO DI 62 CI VA LA DIMENSIONE DEL BATCH\n","    del real_batches\n","    gc.collect()\n","    \n","    return real_combined\n"]},{"cell_type":"markdown","metadata":{"id":"zghXE8FCfueE"},"source":["####SSIM \n","((SSIM) index provides a measure of the similarity by comparing two images based on luminance similarity, contrast similarity and structural similarity information)\n","\n","- il valore deve essere PIÙ BASSO POSSIBILE (indicherebbe più diversità), ma allo stesso tempo, se valore è troppo basso potrebbe voler dire che le immagini non sono di qualità accettabile (rumorose)\n","\n","\n","- modeling any image distortion as a combination of three factors: (loss of correlation, loss of luminance, loss of contrast distortions)\n","\n","- Serve di passargli come argomento l'insieme di immagini reali e un altro insieme di immagini generate (tendere ad avere insiemi di stessa dimensione)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1656869665910,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kK5WvQNFOK2m","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a916964a-eb96-4742-9184-ab1c06486915"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `SSIM` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)\n"]}],"source":["'''link utili alternativi'''\n","\n","#https://medium.com/srm-mic/all-about-structural-similarity-index-ssim-theory-code-in-pytorch-6551b455541e\n","\n","#https://cvnote.ddlee.cc/2019/09/12/psnr-ssim-python\n","\n","#https://www.programcreek.com/python/?CodeExample=compute+ssim - NON MI CONVINCE\n","\n","\n","\n","\n","#https://torchmetrics.readthedocs.io/en/stable/image/structural_similarity.html\n","from torchmetrics import StructuralSimilarityIndexMeasure     \n","import torch\n","\n","ssim = StructuralSimilarityIndexMeasure()\n"]},{"cell_type":"markdown","metadata":{"id":"hUVliMM4zT4F"},"source":["#### LPIPS (NON RIESCE AD INSTALLARE QUASI MAI LA LIBRERIA DA USARE UNA VOLTA HA FUNZIONATO)\n","\n","- A low LPIPS score means that image patches are perceptual similar"]},{"cell_type":"markdown","metadata":{"id":"DAANUrOJ4HiS"},"source":["##### Ho utilizzato il GITHUB : [GITHUB](https://github.com/richzhang/PerceptualSimilarity#a-basic-usage)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1656869665910,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"qID_Z-_h1qDQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b39defa0-f06e-4f4d-fda6-4deff94082c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["trovi tutto nella cartella /content/drive/MyDrive/Colab Notebooks/algoritmi_custom\n"]}],"source":["#------ BASTA ESEGUIRLO SOLO UNA VOLTA PER SALVARLO, MA è GIA' STATO FATTO \n","'''\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","os.mkdir('GIT-LPIP') #https://github.com/richzhang/PerceptualSimilarity#a-basic-usage\n","!git clone https://github.com/richzhang/PerceptualSimilarity.git GIT-LPIP\n","'''\n","print('trovi tutto nella cartella /content/drive/MyDrive/Colab Notebooks/algoritmi_custom')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4FDAciJ2gUg"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GIT-LPIP')\n","!pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2294,"status":"ok","timestamp":1656869672391,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"ZpRRVwBr3Eov","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5c1452e-4067-4c1b-fede-862d0956ee13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GIT-LPIP/lpips/weights/v0.1/alex.pth\n","Traceback (most recent call last):\n","  File \"lpips_2imgs.py\", line 19, in <module>\n","    img0 = lpips.im2tensor(lpips.load_image(opt.path0)) # RGB image from [-1,1]\n","  File \"/content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GIT-LPIP/lpips/__init__.py\", line 74, in load_image\n","    return cv2.imread(path)[:,:,::-1]\n","TypeError: 'NoneType' object is not subscriptable\n"]}],"source":["!python lpips_2imgs.py -p0 /content/drive/MyDrive/CALCIO_CROP_BASE/images/990.png -p1 /content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/patch_20200506123315.png #--use_gpu"]},{"cell_type":"markdown","metadata":{"id":"TaiR3ZXv4QpX"},"source":["##### Ho utilizzato il GITHUB di [TORCH METRICS ](https://github.com/PyTorchLightning/metrics.git)\n","\n","Continua a dare lo stesso errore \n","\n","```\n","LPIPS metric requires that lpips is installed. Either install as `pip install torchmetrics[image]` or `pip install lpips`.\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1656869672393,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"MeGmfv55OaLX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"33e646cd-0b0d-4286-9e6a-9bb02945fd0d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n\\nimg1 = torch.rand(10, 3, 100, 100)\\nimg2 = torch.rand(10, 3, 100, 100)\\n#x = lpips(img1, img2)\\n#print(x)\\n\\nloss_fn = lpips.LPIPS(net='vgg')\\nd = loss_fn.forward(img1,img2)\\nprint(d)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":223}],"source":["'''SE A VOLTE NON FUNZIONA, RIPROVA AD ESEGUIRLO, PERCHÈ SI BLOCCA IL 'PIP INSTALL' '''\n","#https://pypi.org/project/lpips/\n","\n","#https://torchmetrics.readthedocs.io/en/stable/image/learned_perceptual_image_patch_similarity.html\n","\n","#import lpips\n","#!pip install --ignore-installed torchmetrics[image]\n","#!pip install lpips==0.1.4\n","\n","#from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n","#lpips = LearnedPerceptualImagePatchSimilarity_(net_type='vgg')\n","\n","\n","'''ESEMPIO CALCOLO LPIPS'''\n","#_ = torch.manual_seed(123)\n","\n","'''\n","\n","img1 = torch.rand(10, 3, 100, 100)\n","img2 = torch.rand(10, 3, 100, 100)\n","#x = lpips(img1, img2)\n","#print(x)\n","\n","loss_fn = lpips.LPIPS(net='vgg')\n","d = loss_fn.forward(img1,img2)\n","print(d)\n","'''\n"]},{"cell_type":"markdown","metadata":{"id":"JCHj6IT9fnoi"},"source":["#### GEOMETRY SCORE\n","\n","- non capisco perché mi da anche questo sklearn.metrics.fowlkes_mallows_score tra i vari risultati \n","- [PAPER](https://arxiv.org/pdf/1802.02664.pdf)\n","- [LINK](https://github.com/KhrulkovV/geometry-score)\n","\n","((NON FUNZIONA))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1656869672394,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Iyp_qwzkDBJX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"811e83db-10bc-409a-eca2-06107c4b2e24"},"outputs":[{"output_type":"stream","name":"stdout","text":["trovi tutto nella cartella /content/drive/MyDrive/Colab Notebooks/algoritmi_custom\n"]}],"source":["#------ BASTA ESEGUIRLO SOLO UNA VOLTA PER SALVARLO, MA è GIA' STATO FATTO \n","'''\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","os.mkdir('GEOM-SCORE') #https://github.com/KhrulkovV/geometry-score.git\n","!git clone https://github.com/KhrulkovV/geometry-score.git GEOM-SCORE\n","'''\n","print('trovi tutto nella cartella /content/drive/MyDrive/Colab Notebooks/algoritmi_custom')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3388,"status":"ok","timestamp":1656869676090,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"7X3puAh2D-Be","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df1859ee-4ec6-4006-e403-6fbe46e0bf3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gudhi in /usr/local/lib/python3.7/dist-packages (3.5.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from gudhi) (1.21.6)\n"]}],"source":["!pip install gudhi"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":323,"status":"ok","timestamp":1656869676403,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"WcCUCJbrC7ZW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"db4c49b9-5811-451f-8af7-21bdc1a16283"},"outputs":[{"output_type":"stream","name":"stdout","text":["assets\t\t     examples-basic.ipynb  images     setup.py\n","example-mnist.ipynb  gs\t\t\t   README.md\n"]}],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GEOM-SCORE')\n","!ls\n","import gs\n","from gs import geom_score\n","#!pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":536,"status":"ok","timestamp":1656869676934,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"NdfwLvvff6xs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2f41278-f363-4e52-c4af-b8ca6ef1e191"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7fa377c585d0>"]},"metadata":{},"execution_count":227},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["x = gs.circle()\n","plt.scatter(x[:, 0], x[:, 1], s=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20013,"status":"ok","timestamp":1656869696936,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"8fX3IXroEpbH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a6d255e-cb25-49f6-d143-9e36f711934b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Done 0/100\n","Done 10/100\n","Done 20/100\n","Done 30/100\n","Done 40/100\n","Done 50/100\n","Done 60/100\n","Done 70/100\n","Done 80/100\n","Done 90/100\n"]}],"source":["rltx = gs.rlts(x, n=100, L_0=32, i_max=10, gamma=1.0/8)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1656869697312,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"meelOooEEub8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d70c028d-4022-4622-b5ea-2ad1c4e18abc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["mrltx = np.mean(rltx, axis=0)\n","gs.fancy_plot(mrltx[:3])\n","plt.xticks(np.arange(3));"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1656869697313,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"WnB18x4eE1LP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4defb902-8798-4c96-b3dd-c93a9a9fc975"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7fa377c584d0>"]},"metadata":{},"execution_count":230},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Get a second dataset for comparison...\n","\n","y = gs.filled_circle()\n","plt.scatter(y[:, 0], y[:, 1], s=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25852,"status":"ok","timestamp":1656869723154,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"lDZQdRrUE6Ke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5cf41718-8669-440e-dd26-b812fafb30a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Done 0/100\n","Done 10/100\n","Done 20/100\n","Done 30/100\n","Done 40/100\n","Done 50/100\n","Done 60/100\n","Done 70/100\n","Done 80/100\n","Done 90/100\n"]}],"source":["rlty = gs.rlts(y, n=100, L_0=32, i_max=10, gamma=1.0/8)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1013,"status":"ok","timestamp":1656869724130,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"N5Wbc4NvFCbp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d112ef53-a0cd-4da1-c90b-25ba5683b975"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["mrlty = np.mean(rlty, axis=0)\n","gs.fancy_plot(mrlty[:3])\n","plt.xticks(np.arange(3));"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"ok","timestamp":1656869724131,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"vTydWGB-FGHE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a893428d-3789-43f1-d7c4-8fc53acc9b22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.8534918319930305"]},"metadata":{},"execution_count":233}],"source":["# Compute score\n","gs.geom_score(rltx, rlty)"]},{"cell_type":"markdown","metadata":{"id":"H2p8tTB2fv7b"},"source":["#### KLD\n","\n","- non so se può essere utile ma c'è questo link che fa uso di Scipy \n","[link](https://machinelearningmastery.com/divergence-between-probability-distributions/) , l'ho provato ad implementare "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":53,"status":"ok","timestamp":1656869724132,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"p-1ye3R8fyOW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"69d001c7-702f-4c62-b2b9-2c5164d44715"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0863)\n","tensor(-0.4772)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2887: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"]}],"source":["#https://torchmetrics.readthedocs.io/en/stable/classification/kl_divergence.html ++++\n","\n","#https://machinelearningmastery.com/divergence-between-probability-distributions/\n","\n","#https://stackoverflow.com/questions/49886369/kl-divergence-for-two-probability-distributions-in-pytorch  +++++++\n","\n","#https://pytorch.org/docs/stable/generated/torch.nn.functional.kl_div.html\n","\n","\n","\n","'''LOG PROBABILITIES INFOs - servono delle probabilità per calcolare il KL-DIV, quindi forse prima tocca allenare pure il classificatore, e dare le probabilità come input in formato vettoriale, altrimenti i conti non tornano'''\n","\n","#https://stackoverflow.com/questions/58742766/how-to-get-log-probabilities-in-tensorflow\n","\n","#https://machinelearningmastery.com/divergence-between-probability-distributions/\n","\n","#https://discuss.pytorch.org/t/how-to-extract-probabilities/2720\n","\n","#https://stackoverflow.com/questions/58766519/how-to-get-probability-of-each-image-belonging-to-a-class\n","\n","#https://discuss.pytorch.org/t/kl-divergence-for-multi-label-classification/118884/2\n","\n","\n","\n","\n","import torch.nn.functional as F\n","\n","'''prova calcolo KL-div - alternativa 1''' #DOVREBBE ESSERE L'ALTERNATIVA PIÙ CORRETTA\n","P = torch.Tensor([0.36, 0.48, 0.16])\n","Q = torch.Tensor([0.333, 0.333, 0.333])\n","\n","(P * (P / Q).log()).sum()\n","# tensor(0.0863), 10.2 µs ± 508\n","\n","ris1 = F.kl_div(Q.log(), P, None, None, 'sum')\n","# tensor(0.0863), 14.1 µs ± 408 ns\n","print(ris1)\n","\n","'''prova calcolo KL-div - alternativa 2'''\n","ris2 = F.kl_div(P, Q)\n","print(ris2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":52,"status":"ok","timestamp":1656869724133,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Vehz6degGssF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3cf9ca5c-6f17-422f-8e5f-7a22e2779e81"},"outputs":[{"output_type":"stream","name":"stdout","text":["KL(P || Q): 0.086 nats\n","KL(Q || P): 0.096 nats\n"]}],"source":["#https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.rel_entr.html\n","\n","# example of calculating the kl divergence (relative entropy) with scipy\n","from scipy.special import rel_entr\n","# define distributions\n","#p = [0.10, 0.40, 0.50]\n","#q = [0.80, 0.15, 0.05]\n","\n","p = [0.36, 0.48, 0.16]\n","q = [0.333, 0.333, 0.333]\n","\n","# calculate (P || Q)\n","kl_pq = rel_entr(p, q)\n","print('KL(P || Q): %.3f nats' % sum(kl_pq))\n","# calculate (Q || P)\n","kl_qp = rel_entr(q, p)\n","print('KL(Q || P): %.3f nats' % sum(kl_qp))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdOqyzgPHEIm"},"outputs":[],"source":["from math import log2\n","# calculate the kl divergence\n","def kl_divergence(p, q):\n","\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n","\n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":52,"status":"ok","timestamp":1656869724136,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"qjtGOve_HOG4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f7117d0f-88f8-43ef-d5c0-3c1e24e783ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.13901528369128108\n","0.1245047286814874\n"]}],"source":["print(kl_divergence(q,p))\n","\n","print(kl_divergence(p,q))"]},{"cell_type":"markdown","metadata":{"id":"bfJ3LnbAZhQx"},"source":["## TESTING MODEL GAN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EHk5JrKefID"},"outputs":[],"source":["os.chdir(path_images)"]},{"cell_type":"markdown","metadata":{"id":"MEhIQS3lx0jL"},"source":["#### GAN \n","(https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)"]},{"cell_type":"markdown","metadata":{"id":"YWzjz2dEFoHr"},"source":["##### GAN ORIGINAL (WITHOUT OPTUNA HYPERPARAMETERS TUNING)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1656869724696,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"IzKfqHpqS8X9","colab":{"base_uri":"https://localhost:8080/","height":109},"outputId":"a6eefbe7-9615-44d0-f66e-c0a6c4483ee9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n'''stampa degli istogrammi delle immagini reali'''\\n\\ncuda = True if torch.cuda.is_available() else False\\n\\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\\n\\n\\ndataloader = trainloader\\nfor i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\\n\\n        data = data.to(device=device)\\n        targets = targets.to(device = device) #classes\\n        targets2 = targets2.to(device = device) #series\\n\\n\\n        # Adversarial ground truths\\n        valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\\n        fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\\n\\n        # Configure input\\n        real_imgs = Variable(data.type(Tensor))\\n        for counter in range(len(real_imgs)):\\n          x = torch.histc(real_imgs.data[counter], bins=4)\\n          #print(x[1])\\n          print(torch.histc(real_imgs.data[counter], bins=4))\\n\\n\\n\\n\\n'''------------------------------------------------------------DA FARE-----------------------------------------------------------------------------------------------------------------------------------------------'''\\n  #PROVA A CALCOLARE LA METRICA MMD TRA BATCH INTERO DI REALI E SINGOLA IMMAGINE GENERATA CICLANDO.\\n\\n  #OPPURE CALCOLA L'ISTOGRAMMA PER TUTTE LE IMMAGINI REALI E VEDI I VALORI DI RIFERIMENTO CHE DEVONO AVERE PER FARE LE CONDIZIONI DI SALVATAGGIO DI IMMAGINE SOPRA\\n'''-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":247}],"source":["\n","'''stampa degli istogrammi delle immagini reali'''\n","'''L'HO USATO PER FARE DELLE CONDIZIONI SUI VALORI DEGLI ISTOGRAMMI PER CAPIRE QUANDO SALVARE LE IMMAGINI, PERCHE' RETE GAN ATTUALE NON FA MOLTO BENE PER IL CASO DI STUDIO'''\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","\n","dataloader = trainloader\n","for i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\n","\n","        data = data.to(device=device)\n","        targets = targets.to(device = device) #classes\n","        targets2 = targets2.to(device = device) #series\n","\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n","        fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n","\n","        # Configure input\n","        real_imgs = Variable(data.type(Tensor))\n","        for counter in range(len(real_imgs)):\n","          x = torch.histc(real_imgs.data[counter], bins=4)\n","          #print(x[1])\n","          print(torch.histc(real_imgs.data[counter], bins=4))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5Ea9NVNx2xm","executionInfo":{"status":"ok","timestamp":1656869724697,"user_tz":-120,"elapsed":45,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/","height":109},"outputId":"581f87bf-1fee-41b5-d402-f1b93fbfae9e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\n\\n\\'\\'\\'ADDESTRAMENTO GAN\\n\\n\\n\\nNOTE PER LE MODIFICHE DEGLI  HYPERPARAMETRI DURANTE LA FASE DI TEST DELLA GAN PER MIGLIOR SETTAGGIO\\n\\n\\n########################\\nMODE COLLAPSE = il generatore produce solo pochi dettagli, quindi DIVERSITÀ BASSA (SE METRICHE DIVERSITÀ AVRANNO VALORE BASSO) = C\\'È OSCILLAZIONE NELLA LOSS DEL GENERATORE TIPICAMENTE (DISCRIMINATOR RICONOSCE CHE SONO FINTE)\\n\\n- LEARNING RATE: modificare il learning rate solo abbassandolo (se immagini non hanno molti dettagli)\\n\\n- LATENT SPACE : come per il caso precedente, (abbassandolo diminuiamo l\\'insieme dei possibili output che genera ogni iterazione e quindi non si allena molto in diversità)\\n\\n########################\\n\\n\\n\\n########################\\nCONVERGENCE FAILURE = quando la loss non fa al caso del modello utilizzato di generatore, e che con le immagini che si stanno usando la LOSS GENERATORE OSCILLA/HA VALORI ALTI/CONTINUA A CRESCERE e/o LOSS DISCIMINATORE TENDE A 0\\n\\n- LOSS TYPE: modificarla potrebbe migliorare la generazione \\n\\n- MODIFICARE LA CAPACITÀ DEI LAYERS DEL MODELLO: aumentare la dimensione dei layers potrebbe aumentare la QUALITÀ del risultato\\n########################\\n\\'\\'\\'\\n\\n\\nimport argparse\\nimport os\\nimport numpy as np\\nimport math\\n\\nimport torchvision.transforms as transforms\\nfrom torchvision.utils import save_image\\n\\nfrom torch.utils.data import DataLoader\\nfrom torchvision import datasets\\nfrom torch.autograd import Variable\\n\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch\\n\\n\\n\\n\\nos.makedirs(\"images\", exist_ok=True)\\nos.makedirs(\"images_to_check\", exist_ok=True)\\n\\ngan_history = {}\\ngan_history[\\'DLOSS\\'] = []\\ngan_history[\\'GLOSS\\'] = []\\ngan_history[\\'MMD\\'] = []\\ngan_history[\\'SSIM\\'] = []\\ngan_history[\\'EPOCH\\'] = []\\n\\nopt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\\n      [2500,              32,                0.0002,   0.5,      0.999,    8,           100,                128*2,             3,              4,              0.01,              20]]\\n\\n\\'\\'\\'\\nopt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\\n      [2000,              64,                0.0002,   0.5,      0.999,    8,           100,                128*2,             3,              4,              0.01,              20]]\\n\\'\\'\\'\\n\\n\\n#inutile cambiare il batch_size, perchè tanto prende quello dato al DataLoader!!!!!!\\n#print(\"ok {} \".format(config))\\n\\n\\nimg_shape = (opt[1][8], 270, 470)\\n\\n\\ncuda = True if torch.cuda.is_available() else False\\n\\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\\n\\n\\n#print(opt[1][7])\\n\\nclass Generator(nn.Module):\\n    def __init__(self):\\n        super(Generator, self).__init__()\\n\\n        def block(in_feat, out_feat, normalize=True):\\n            layers = [nn.Linear(in_feat, out_feat)]\\n            if normalize:\\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\\n            return layers\\n\\n        self.model = nn.Sequential(\\n            *block(opt[1][6], 128, normalize=False),\\n            *block(128, 256),\\n            *block(256, 512),\\n            *block(512, 1024),\\n            nn.Linear(1024, int(np.prod([img_shape]))),\\n            nn.Tanh()\\n        )\\n\\n    def forward(self, z):\\n        img = self.model(z)\\n        img = img.view(img.size(0), *img_shape)\\n        return img\\n\\n\\nclass Discriminator(nn.Module):\\n    def __init__(self):\\n        super(Discriminator, self).__init__()\\n\\n        self.model = nn.Sequential(\\n            nn.Linear(int(np.prod(img_shape)), 512),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Linear(512, 256),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Linear(256, 1),\\n            nn.Sigmoid(),\\n        )\\n\\n    def forward(self, img):\\n        img_flat = img.view(img.size(0), -1)\\n        validity = self.model(img_flat)\\n\\n        return validity\\n\\n\\n# Loss function\\nadversarial_loss = torch.nn.BCELoss()\\n\\n# Initialize generator and discriminator\\ngenerator = Generator()\\n\\n#hl.build_graph(generator, torch.zeros([3, 3, 270, 470]))\\n\\ndiscriminator = Discriminator()\\n\\n#hl.build_graph(discriminator, torch.zeros([3, 3, 270, 470]))\\n\\nif cuda:\\n    generator.cuda()\\n    discriminator.cuda()\\n    adversarial_loss.cuda()\\n\\n# Configure data loader\\ndataloader = trainloader\\n\\n# ---------Optimizers\\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\\n#optimizer_G = torch.optim.Adam(generator.parameters(), lr=config[\\'lr_\\'], betas=(config[\"b1_\"],  config[\"b2_\"]))\\n\\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\\n#optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=config[\\'lr_\\'], betas=(config[\"b1_\"],  config[\"b2_\"]))\\n\\n#---------\\n#optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=0.00002) #VA MA NON CREA IMMAGINI MOLTO BUONE\\n\\n#optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=0.00002) #VA MA NON CREA IMMAGINI MOLTO BUONE\\n\\n\\n\\n\\n\\n\\n# ----------\\n#  Training\\n# ----------\\n\\n\\n#--------------------parametri per calcolare NDB score-----------------\\n\\nbatches_done = 0\\ncontatore_stabilita = 0\\nreal_batches = [] #to save real images to be used with NDB METRIC\\ngenerated_batches = [] #to save real images to be used with NDB METRIC\\niters = 0\\nimg_list = []\\nmetrics = []\\nndb_scores = []\\nreal_batches2 = []\\nreal_batches_np = 0\\nreal_batches2_np = 0\\nreal_concatenate = 0\\n\\n#per metrica FID\\nBatchSize = 8 \\nUseMultiprocessing = False \\n\\n\\n\\'\\'\\'estrazione batch di immagini reali per NDB\\'\\'\\'\\n\\n\\nfor i, (data, targets, targets2) in tqdm(enumerate(trainloader)):\\n\\n    #- MEMORIZZO IL SET DI IMMAGINI REALI PER USARLO NELLA METRICA \\'NDB\\' -\\n    real = data.numpy()\\n    if len(trainloader)==1:\\n      real_batches.append(real)\\n      real_concatenate = np.array(real_batches)\\n    else:\\n      if i==0:\\n        real_batches.append(real)\\n        real_batches_np = np.array(real_batches)\\n        print(real_batches_np.shape)\\n        real_batches = []\\n      if i==1:\\n        real_batches.append(real)\\n        real_batches2_np = np.array(real_batches)\\n        print(real_batches2_np.shape)\\n        real_batches = []\\n        real_concatenate = np.concatenate((real_batches_np,real_batches2_np), axis=1)\\n      if i>1:\\n        real_batches.append(real)\\n        real_batches2_np = np.array(real_batches)\\n        print(real_batches2_np.shape)\\n        real_batches = []\\n        real_concatenate = np.concatenate((real_concatenate,real_batches2_np), axis=1)\\n    \\n\\n#RIADATTAMENTO DELLE IMMAGINI REALI & GENERATE PER METRICA \\'NDB\\'\\n#real_batches = np.array(real_batches)\\nreal_batches = real_concatenate\\n\\n#print(real_batches)\\n#print(real_batches.shape)\\n#print(len(real_batches))\\n\\n#Display a sample\\n#plt.imshow(np.transpose(real_batches[0][0], (1,2,0)))\\n\\nreal_combined = real_batches.reshape(numTrainBatches*len(df_class_new), 3, img_shape[1], img_shape[2])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\\n#real_combined = real_batches.reshape(numTrainBatches*len(df_class_new), 3, opt[1][7], opt[1][7])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\\n\\n\\ndel real_batches\\ngc.collect()\\nreal_batches = []\\nreal_batches_np = []\\n\\n\\'\\'\\'alternativa\\'\\'\\'\\n#real_combined = real_samples(numTrainBatches, trainloader, nc, opt[1][7])\\n\\'\\'\\'----------------------------------------------------------------------\\'\\'\\'   \\n\\n\\n#il KERNEL_TYPE = \"rbf\" serve per il MMD \\nKERNEL_TYPE = \"rbf\"\\n\\nfor epoch in range( opt[1][0]):\\n    for i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\\n\\n        data = data.to(device=device)\\n        targets = targets.to(device = device) #classes\\n        targets2 = targets2.to(device = device) #series\\n\\n\\n        # Adversarial ground truths\\n        valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\\n        fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\\n\\n        # Configure input\\n        real_imgs = Variable(data.type(Tensor))\\n\\n        # -----------------\\n        #  Train Generator\\n        # -----------------\\n\\n        optimizer_G.zero_grad()\\n\\n        # Sample noise as generator input\\n        z = Variable(Tensor(np.random.normal(5, 10, (data.shape[0], opt[1][6]))))         #PRIMA INVECE DI -5,10- C\\'ERA -0,1- MA ERA MENO EFFICACE NEL COMPLESSO DEL BATCH\\n\\n        # Generate a batch of images\\n        gen_imgs = generator(z)\\n\\n\\n\\n        # Loss measures generator\\'s ability to fool the discriminator\\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\\n\\n        g_loss.backward()\\n        optimizer_G.step()\\n\\n        # ---------------------\\n        #  Train Discriminator\\n        # ---------------------\\n\\n        optimizer_D.zero_grad()\\n\\n        # Measure discriminator\\'s ability to classify real from generated samples\\n        real_loss = adversarial_loss(discriminator(real_imgs), valid)\\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\\n        d_loss = (real_loss + fake_loss) / 2\\n\\n        d_loss.backward()\\n        optimizer_D.step()\\n\\n        \\n\\n        print(\\n            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\\n            % (epoch, opt[1][0], i+1, len(dataloader), d_loss.item(), g_loss.item())\\n        )\\n\\n        batches_done = epoch * len(dataloader) + i\\n        \\n        #questo per vedere ogni 10 epoche come sono le immagini che genera\\n        #if batches_done % 100 == 0 : \\n          #save_image(gen_imgs.data[:30], \"images_batch/{}_{}.png\".format(epoch, i+1), nrow=6, normalize=True)\\n\\n        if d_loss.item() < 1 and d_loss.item() > 0.3 and epoch>=1750 and g_loss.item() < 1.5:\\n          for counter in range(len(gen_imgs)):\\n            x = torch.histc(gen_imgs.data[counter], bins=4)\\n            #if epoch % 10 == 0:\\n              #print(torch.histc(gen_imgs.data[counter], bins=4))\\n            \\n            if x[0]>110000 and x[0]<210000 and x[1]>70000 and x[1]<120000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<100000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 9 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\n              save_image(gen_imgs.data[counter], \"images9/_{}_{}_{}.png\".format(epoch, i+1, counter), nrow=1, normalize=True)                    #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\\n              \\n\\n            \\'\\'\\'PARTE DI CALCOLO DELL\\' MMD\\'\\'\\'\\n            \\'\\'\\'\\n            real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\\n            #print(real_imgs2.shape)\\n            #print(type(real_imgs2))\\n\\n            gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\\n            #print(gen_imgs2.shape)\\n            #print(type(gen_imgs2))\\n\\n            mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all\\'interno dell\\'file \\n            \\'\\'\\'\\n\\n        #------------------------------------------stampo i batch in cartelle e salvo i modelli nei punti di stabilità\\n        \\'\\'\\'\\n        if epoch >2000  and g_loss<1.5:  #epoch>1750 quasi sempre con batch da 64, anche con batch da 96 (32*3)\\n          \\n          if contatore_stabilita >=5 and d_loss>0.30:\\n            torch.save(generator.state_dict(),\\'/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_generator_4_{}.pth\\'.format(epoch))                      #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\\n            #torch.save(discriminator.state_dict(),\\'/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_discriminator_2__{}.pth\\'.format(epoch))                          #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICAR\\n            contatore_stabilita = 0\\n            save_image(gen_imgs.data[:30], \"images_batch_4/_{}_{}_{}.png\".format(epoch, i+1, d_loss.item()), nrow=6, normalize=True)\\n\\n          if d_loss > 0.25:\\n            contatore_stabilita = contatore_stabilita + 1\\n          \\n          if contatore_stabilita>0 and d_loss<0.25:\\n            contatore_stabilita = 0\\n          \\n        \\n        if epoch % 25 == 0:\\n          save_image(gen_imgs.data[:30], \"images_batch_2/_{}_{}_{}.png\".format(epoch, i+1, d_loss.item()), nrow=6, normalize=True)                        #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\\n        \\'\\'\\'\\n        #------------------------------------------\\n\\n        #SUCCESSIVAMENTE IMPLEMENTARE UN VETTORE CHE MEMORIZZA I VALORI DI LOSS DEL GENERATOR E MEMORIZZARE MODELLO QUANDO LA VARIANZA è BASSA COSì COME QUANDO è IL SUO VALORE BASSO, SOTTO 1.5 (ALMENO)\\n        #siamo sempre in situazione di mode collapse, ovvero il generator oscilla troppo e non riesce a generare troppa diversità tra le immagini, con qualsiasi dimensione di batch, questo perchè forse ha pochi elementi, ma comunque usando un batch superiore a 64 collassa dopo un pò.\\n        #provare comunque a verificare in futuro usando le immagini Flippate e Brightness ad usare un batch più grande e capire se riese a generare le immagini con meno epoche e quando più o meno collassa se ha già generato cose di sufficiente qualità\\n\\n        \\'\\'\\'\\n        #PARTE DI CALCOLO DELL\\' NDB\\n        # Check how the generator is doing by saving G\\'s output on fixed_noise\\n        if (iters % 10 == 0) or ((epoch == opt[1][0]-1) and (i == len(dataloader)-1)):\\n            with torch.no_grad():\\n                fake = generator(z).detach().cpu()\\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\\n            \\n\\n            #genero fake batches images per calcolare metrica NDB\\n            gen_combined = generated_fakes(numTestBatches, generator, img_shape[1], img_shape[2], opt[1][6])\\n      \\n            #Calculate NDB\\n            train_size = numTrainBatches*62     #perchè il set ha 62 immagini e non riempe neanche il batch\\n            test_size = numTestBatches*62\\n            dim = 270*470*3\\n            #dim = img_shape[1]*img_shape[2]*3\\n\\n            train_samples = real_combined.reshape(train_size, dim)\\n            test_samples = gen_combined.reshape(test_size, dim)\\n            \\n            ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\\n            results = ndb.evaluate(test_samples)\\n                \\n            ndb_k = float(results[\"NDB\"])/ndb.number_of_bins\\n            #wandb.log({\"ndb_k\" : ndb_k, \"JS\": results[\"JS\"]})   \\n\\n        iters += 1\\n\\n\\n    #memorizzo info su NDB\\n    #specifically track NDB\\n    metrics.append(results)\\n    ndb_scores.append(ndb_k)\\n    \\n    #print(\"NDB_K: \", ndb_k)\\n    \\'\\'\\'\\n\\n    #--------------------------------------------------------------------------------------------\\n    #--------------------------------------------------------------------------------------------\\n    \\n\\n    \\n    \\'\\'\\'\\n    #PARTE DI CALCOLO DELL\\' MMD\\n    #---- Maximum Mean Discrepancy (MMD) (non capisco come farlo funzionare anche con le img a colori, se ti da qualche errore, prova prima a rieseguire il blocco subito sopra -----\\n    \\n    real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\\n    #print(real_imgs2.shape)\\n    #print(type(real_imgs2))\\n\\n    gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\\n    #print(gen_imgs2.shape)\\n    #print(type(gen_imgs2))\\n\\n    \\n    #mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all\\'interno dell\\'file \\n    #if epoch > 1000 and epoch % 10 == 0 and mmd_average<2:\\n    #  print(\\'MMD AVERAGE : {}\\'.format(mmd_average))\\n    \\'\\'\\'\\n\\n\\n    \\'\\'\\'\\n    #CALCOLO SSIM DA MONITORARE\\n    if epoch > 1000 and epoch % 10 == 0:\\n      #real_combined_ssim = torch.from_numpy(real_combined)\\n      real = data.numpy()\\n      real_batches.append(real)\\n      real_batches_np = np.array(real_batches)\\n      #print(real_batches_np.shape)\\n      real_combined_ssim = real_batches_np.reshape(len(data), 3, img_shape[1], img_shape[2])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\\n      real_combined_ssim = torch.from_numpy(real_combined_ssim)\\n      #print(real_combined_ssim.shape)\\n\\n      #gen_combined_ssim = torch.from_numpy(gen_combined)\\n      #print(gen_combined_ssim.shape)\\n    \\n      #calcolo metrica\\n      ris_ssim = ssim(real_combined_ssim,gen_imgs)\\n      real_batches = []\\n\\n      #print(\\'MMD AVERAGE : {}\\'.format(mmd_average))\\n      print(\"ssim :{}\".format(ris_ssim))\\n      gan_history[\\'SSIM\\'].append(ris_ssim.item())\\n      \\'\\'\\'\\n    \\n    \\n\\n    \\n\\n\\n    \\'\\'\\'memorizzo info su addestramento GAN\\'\\'\\'\\n    gan_history[\\'DLOSS\\'].append(d_loss.item())\\n    gan_history[\\'GLOSS\\'].append(g_loss.item()) \\n    gan_history[\\'EPOCH\\'].append(epoch)\\n    #gan_history[\\'MMD\\'].append(mmd_average.item())\\n    \\n\\n\\n        \\n\\n\\n\\n\\'\\'\\'As such, the practice of systematically generating images and saving models during training can and should continue to be used to allow post-hoc model selection\\'\\'\\'\\n\\n\\'\\'\\'CONVIENE SALVARE I MODELLI GENERATORE E DISCRIMINATORE, PERCHÈ POTREBBE AVERE I SUOI MASSIMI DI RISULTATI ANCHE IN MEZZO ALL\\'ALLENAMENTO\\'\\'\\'\\n\\n\\'\\'\\'\\nclasse 0 - ok - if  x[0]>40000 and x[0]<180000 and x[1]>60000 and x[1]<130000 and x[2]>55000 and x[2]<100000 and x[3]>10000 and x[3]<2000000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 0 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\nclasse 1 - ok - if x[0]>80000 and x[0]<200000 and x[1]>50000 and x[1]<150000 and x[2]>50000 and x[2]<100000 and x[3]>10000 and x[3]<150000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 1 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\nclasse 2 - ok - if x[0]>50000 and x[0]<220000 and x[1]>60000 and x[1]<130000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<130000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 2 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\nclasse 3 - ok - if x[0]>70000 and x[0]<200000 and x[1]>50000 and x[1]<150000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<130000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 3 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\nclasse 4 - ok - if x[0]>70000 and x[0]<200000 and x[1]>40000 and x[1]<130000 and x[2]>50000 and x[2]<100000 and x[3]>10000 and x[3]<170000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 4 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\nclasse 5 - ok - if x[0]>90000 and x[0]<200000 and x[1]>50000 and x[1]<130000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<160000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 5 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\nclasse 7 - if x[0]>100000 and x[0]<200000 and x[1]>70000 and x[1]<120000 and x[2]>50000 and x[2]<90000 and x[3]>20000 and x[3]<150000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 7 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte \\nclasse 8 - ok - if x[0]>100000 and x[0]<200000 and x[1]>80000 and x[1]<140000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<120000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 8 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\nclasse 9 - ok - if x[0]>110000 and x[0]<210000 and x[1]>70000 and x[1]<120000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<100000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 9 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\\n\\n\\'\\'\\'\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":248}],"source":["'''ADDESTRAMENTO GAN\n","\n","NOTE PER LE MODIFICHE DEGLI  HYPERPARAMETRI DURANTE LA FASE DI TEST DELLA GAN PER MIGLIOR SETTAGGIO\n","\n","########################\n","MODE COLLAPSE = il generatore produce solo pochi dettagli, quindi DIVERSITÀ BASSA (SE METRICHE DIVERSITÀ AVRANNO VALORE BASSO) = C'È OSCILLAZIONE NELLA LOSS DEL GENERATORE TIPICAMENTE (DISCRIMINATOR RICONOSCE CHE SONO FINTE)\n","\n","- LEARNING RATE: modificare il learning rate solo abbassandolo (se immagini non hanno molti dettagli)\n","\n","- LATENT SPACE : come per il caso precedente, (abbassandolo diminuiamo l'insieme dei possibili output che genera ogni iterazione e quindi non si allena molto in diversità)\n","\n","########################\n","\n","########################\n","CONVERGENCE FAILURE = quando la loss non fa al caso del modello utilizzato di generatore, e che con le immagini che si stanno usando la LOSS GENERATORE OSCILLA/HA VALORI ALTI/CONTINUA A CRESCERE e/o LOSS DISCIMINATORE TENDE A 0\n","\n","- LOSS TYPE: modificarla potrebbe migliorare la generazione \n","\n","- MODIFICARE LA CAPACITÀ DEI LAYERS DEL MODELLO: aumentare la dimensione dei layers potrebbe aumentare la QUALITÀ del risultato\n","########################\n","'''\n","\n","\n","import argparse\n","import os\n","import numpy as np\n","import math\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","\n","\n","\n","os.makedirs(\"images\", exist_ok=True)\n","os.makedirs(\"images_to_check\", exist_ok=True)\n","\n","gan_history = {}\n","gan_history['DLOSS'] = []\n","gan_history['GLOSS'] = []\n","gan_history['MMD'] = []\n","gan_history['SSIM'] = []\n","gan_history['EPOCH'] = []\n","\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","      [2500,              32,                0.0002,   0.5,      0.999,    8,           100,                128*2,             3,              4,              0.01,              20]]\n","\n","#inutile cambiare il batch_size QUI, perchè tanto prende quello dato al DataLoader!!!!!!\n","\n","\n","\n","img_shape = (opt[1][8], 270, 470)\n","\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","\n","#print(opt[1][7])\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        def block(in_feat, out_feat, normalize=True):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *block(opt[1][6], 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod([img_shape]))),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        img = img.view(img.size(0), *img_shape)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(int(np.prod(img_shape)), 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, img):\n","        img_flat = img.view(img.size(0), -1)\n","        validity = self.model(img_flat)\n","\n","        return validity\n","\n","\n","# Loss function\n","adversarial_loss = torch.nn.BCELoss()\n","\n","# Initialize generator and discriminator\n","generator = Generator()\n","\n","#hl.build_graph(generator, torch.zeros([3, 3, 270, 470]))\n","\n","discriminator = Discriminator()\n","\n","#hl.build_graph(discriminator, torch.zeros([3, 3, 270, 470]))\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","    adversarial_loss.cuda()\n","\n","# Configure data loader\n","dataloader = trainloader\n","\n","# ---------Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","#optimizer_G = torch.optim.Adam(generator.parameters(), lr=config['lr_'], betas=(config[\"b1_\"],  config[\"b2_\"]))\n","\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","#optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=config['lr_'], betas=(config[\"b1_\"],  config[\"b2_\"]))\n","\n","#---------\n","#optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=0.00002) #VA MA NON CREA IMMAGINI MOLTO BUONE\n","\n","#optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=0.00002) #VA MA NON CREA IMMAGINI MOLTO BUONE\n","\n","\n","\n","\n","\n","\n","# ----------\n","#  Training\n","# ----------\n","\n","\n","#--------------------parametri per calcolare NDB score-----------------\n","\n","batches_done = 0\n","contatore_stabilita = 0\n","real_batches = [] #to save real images to be used with NDB METRIC\n","generated_batches = [] #to save real images to be used with NDB METRIC\n","iters = 0\n","img_list = []\n","metrics = []\n","ndb_scores = []\n","real_batches2 = []\n","real_batches_np = 0\n","real_batches2_np = 0\n","real_concatenate = 0\n","\n","#per metrica FID\n","BatchSize = 8 \n","UseMultiprocessing = False \n","\n","\n","'''estrazione batch di immagini reali per NDB'''\n","\n","\n","for i, (data, targets, targets2) in tqdm(enumerate(trainloader)):\n","\n","    #- MEMORIZZO IL SET DI IMMAGINI REALI PER USARLO NELLA METRICA 'NDB' -\n","    real = data.numpy()\n","    if len(trainloader)==1:\n","      real_batches.append(real)\n","      real_concatenate = np.array(real_batches)\n","    else:\n","      if i==0:\n","        real_batches.append(real)\n","        real_batches_np = np.array(real_batches)\n","        print(real_batches_np.shape)\n","        real_batches = []\n","      if i==1:\n","        real_batches.append(real)\n","        real_batches2_np = np.array(real_batches)\n","        print(real_batches2_np.shape)\n","        real_batches = []\n","        real_concatenate = np.concatenate((real_batches_np,real_batches2_np), axis=1)\n","      if i>1:\n","        real_batches.append(real)\n","        real_batches2_np = np.array(real_batches)\n","        print(real_batches2_np.shape)\n","        real_batches = []\n","        real_concatenate = np.concatenate((real_concatenate,real_batches2_np), axis=1)\n","    \n","\n","#RIADATTAMENTO DELLE IMMAGINI REALI & GENERATE PER METRICA 'NDB'\n","#real_batches = np.array(real_batches)\n","real_batches = real_concatenate\n","\n","#print(real_batches)\n","#print(real_batches.shape)\n","#print(len(real_batches))\n","\n","#Display a sample\n","#plt.imshow(np.transpose(real_batches[0][0], (1,2,0)))\n","\n","real_combined = real_batches.reshape(numTrainBatches*len(df_class_new), 3, img_shape[1], img_shape[2])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","#real_combined = real_batches.reshape(numTrainBatches*len(df_class_new), 3, opt[1][7], opt[1][7])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","\n","\n","del real_batches\n","gc.collect()\n","real_batches = []\n","real_batches_np = []\n","\n","'''alternativa'''\n","#real_combined = real_samples(numTrainBatches, trainloader, nc, opt[1][7])\n","'''----------------------------------------------------------------------'''   \n","\n","\n","#il KERNEL_TYPE = \"rbf\" serve per il MMD \n","KERNEL_TYPE = \"rbf\"\n","\n","for epoch in range( opt[1][0]):\n","    for i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\n","\n","        data = data.to(device=device)\n","        targets = targets.to(device = device) #classes\n","        targets2 = targets2.to(device = device) #series\n","\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n","        fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n","\n","        # Configure input\n","        real_imgs = Variable(data.type(Tensor))\n","\n","        # -----------------\n","        #  Train Generator\n","        # -----------------\n","\n","        optimizer_G.zero_grad()\n","\n","        # Sample noise as generator input\n","        z = Variable(Tensor(np.random.normal(5, 10, (data.shape[0], opt[1][6]))))         #PRIMA INVECE DI -5,10- C'ERA -0,1- MA ERA MENO EFFICACE NEL COMPLESSO DEL BATCH\n","\n","        # Generate a batch of images\n","        gen_imgs = generator(z)\n","\n","\n","\n","        # Loss measures generator's ability to fool the discriminator\n","        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n","\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","\n","        # Measure discriminator's ability to classify real from generated samples\n","        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n","        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n","        d_loss = (real_loss + fake_loss) / 2\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        \n","\n","        print(\n","            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n","            % (epoch, opt[1][0], i+1, len(dataloader), d_loss.item(), g_loss.item())\n","        )\n","\n","        batches_done = epoch * len(dataloader) + i\n","        \n","        #questo per vedere ogni 10 epoche come sono le immagini che genera\n","        #if batches_done % 100 == 0 : \n","          #save_image(gen_imgs.data[:30], \"images_batch/{}_{}.png\".format(epoch, i+1), nrow=6, normalize=True)\n","\n","        if d_loss.item() < 1 and d_loss.item() > 0.3 and epoch>=1750 and g_loss.item() < 1.5:\n","          for counter in range(len(gen_imgs)):\n","            x = torch.histc(gen_imgs.data[counter], bins=4)\n","            #if epoch % 10 == 0:\n","              #print(torch.histc(gen_imgs.data[counter], bins=4))\n","            \n","            if x[0]>110000 and x[0]<210000 and x[1]>70000 and x[1]<120000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<100000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 9 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","              save_image(gen_imgs.data[counter], \"images9/_{}_{}_{}.png\".format(epoch, i+1, counter), nrow=1, normalize=True)                    #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","              \n","\n","            '''PARTE DI CALCOLO DELL' MMD'''\n","            '''\n","            real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\n","            #print(real_imgs2.shape)\n","            #print(type(real_imgs2))\n","\n","            gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\n","            #print(gen_imgs2.shape)\n","            #print(type(gen_imgs2))\n","\n","            mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all'interno dell'file \n","            '''\n","\n","        #------------------------------------------stampo i batch in cartelle e salvo i modelli nei punti di stabilità\n","        '''\n","        if epoch >2000  and g_loss<1.5:  #epoch>1750 quasi sempre con batch da 64, anche con batch da 96 (32*3)\n","          \n","          if contatore_stabilita >=5 and d_loss>0.30:\n","            torch.save(generator.state_dict(),'/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_generator_4_{}.pth'.format(epoch))                      #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","            #torch.save(discriminator.state_dict(),'/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_discriminator_2__{}.pth'.format(epoch))                          #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICAR\n","            contatore_stabilita = 0\n","            save_image(gen_imgs.data[:30], \"images_batch_4/_{}_{}_{}.png\".format(epoch, i+1, d_loss.item()), nrow=6, normalize=True)\n","\n","          if d_loss > 0.25:\n","            contatore_stabilita = contatore_stabilita + 1\n","          \n","          if contatore_stabilita>0 and d_loss<0.25:\n","            contatore_stabilita = 0\n","          \n","        \n","        if epoch % 25 == 0:\n","          save_image(gen_imgs.data[:30], \"images_batch_2/_{}_{}_{}.png\".format(epoch, i+1, d_loss.item()), nrow=6, normalize=True)                        #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","        '''\n","        #------------------------------------------\n","\n","        #SUCCESSIVAMENTE IMPLEMENTARE UN VETTORE CHE MEMORIZZA I VALORI DI LOSS DEL GENERATOR E MEMORIZZARE MODELLO QUANDO LA VARIANZA è BASSA COSì COME QUANDO è IL SUO VALORE BASSO, SOTTO 1.5 (ALMENO)\n","        #siamo sempre in situazione di mode collapse, ovvero il generator oscilla troppo e non riesce a generare troppa diversità tra le immagini, con qualsiasi dimensione di batch, questo perchè forse ha pochi elementi, ma comunque usando un batch superiore a 64 collassa dopo un pò.\n","        #provare comunque a verificare in futuro usando le immagini Flippate e Brightness ad usare un batch più grande e capire se riese a generare le immagini con meno epoche e quando più o meno collassa se ha già generato cose di sufficiente qualità\n","\n","        '''\n","        #PARTE DI CALCOLO DELL' NDB\n","        # Check how the generator is doing by saving G's output on fixed_noise\n","        if (iters % 10 == 0) or ((epoch == opt[1][0]-1) and (i == len(dataloader)-1)):\n","            with torch.no_grad():\n","                fake = generator(z).detach().cpu()\n","            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n","            \n","\n","            #genero fake batches images per calcolare metrica NDB\n","            gen_combined = generated_fakes(numTestBatches, generator, img_shape[1], img_shape[2], opt[1][6])\n","      \n","            #Calculate NDB\n","            train_size = numTrainBatches*62     #perchè il set ha 62 immagini e non riempe neanche il batch\n","            test_size = numTestBatches*62\n","            dim = 270*470*3\n","            #dim = img_shape[1]*img_shape[2]*3\n","\n","            train_samples = real_combined.reshape(train_size, dim)\n","            test_samples = gen_combined.reshape(test_size, dim)\n","            \n","            ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\n","            results = ndb.evaluate(test_samples)\n","                \n","            ndb_k = float(results[\"NDB\"])/ndb.number_of_bins\n","            #wandb.log({\"ndb_k\" : ndb_k, \"JS\": results[\"JS\"]})   \n","\n","        iters += 1\n","\n","\n","    #memorizzo info su NDB\n","    #specifically track NDB\n","    metrics.append(results)\n","    ndb_scores.append(ndb_k)\n","    \n","    #print(\"NDB_K: \", ndb_k)\n","    '''\n","\n","    #--------------------------------------------------------------------------------------------\n","    #--------------------------------------------------------------------------------------------\n","    \n","\n","    \n","    '''\n","    #PARTE DI CALCOLO DELL' MMD\n","    #---- Maximum Mean Discrepancy (MMD) (non capisco come farlo funzionare anche con le img a colori, se ti da qualche errore, prova prima a rieseguire il blocco subito sopra -----\n","    \n","    real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\n","    #print(real_imgs2.shape)\n","    #print(type(real_imgs2))\n","\n","    gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\n","    #print(gen_imgs2.shape)\n","    #print(type(gen_imgs2))\n","\n","    \n","    #mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all'interno dell'file \n","    #if epoch > 1000 and epoch % 10 == 0 and mmd_average<2:\n","    #  print('MMD AVERAGE : {}'.format(mmd_average))\n","    '''\n","\n","\n","    '''\n","    #CALCOLO SSIM DA MONITORARE\n","    if epoch > 1000 and epoch % 10 == 0:\n","      #real_combined_ssim = torch.from_numpy(real_combined)\n","      real = data.numpy()\n","      real_batches.append(real)\n","      real_batches_np = np.array(real_batches)\n","      #print(real_batches_np.shape)\n","      real_combined_ssim = real_batches_np.reshape(len(data), 3, img_shape[1], img_shape[2])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","      real_combined_ssim = torch.from_numpy(real_combined_ssim)\n","      #print(real_combined_ssim.shape)\n","\n","      #gen_combined_ssim = torch.from_numpy(gen_combined)\n","      #print(gen_combined_ssim.shape)\n","    \n","      #calcolo metrica\n","      ris_ssim = ssim(real_combined_ssim,gen_imgs)\n","      real_batches = []\n","\n","      #print('MMD AVERAGE : {}'.format(mmd_average))\n","      print(\"ssim :{}\".format(ris_ssim))\n","      gan_history['SSIM'].append(ris_ssim.item())\n","      '''\n","    \n","\n","    '''memorizzo info su addestramento GAN'''\n","    gan_history['DLOSS'].append(d_loss.item())\n","    gan_history['GLOSS'].append(g_loss.item()) \n","    gan_history['EPOCH'].append(epoch)\n","    #gan_history['MMD'].append(mmd_average.item())\n","    \n","\n","\n","'''As such, the practice of systematically generating images and saving models during training can and should continue to be used to allow post-hoc model selection'''\n","\n","'''CONVIENE SALVARE I MODELLI GENERATORE E DISCRIMINATORE, PERCHÈ POTREBBE AVERE I SUOI MASSIMI DI RISULTATI ANCHE IN MEZZO ALL'ALLENAMENTO'''\n","\n","'''\n","classe 0 - ok - if  x[0]>40000 and x[0]<180000 and x[1]>60000 and x[1]<130000 and x[2]>55000 and x[2]<100000 and x[3]>10000 and x[3]<2000000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 0 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 1 - ok - if x[0]>80000 and x[0]<200000 and x[1]>50000 and x[1]<150000 and x[2]>50000 and x[2]<100000 and x[3]>10000 and x[3]<150000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 1 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 2 - ok - if x[0]>50000 and x[0]<220000 and x[1]>60000 and x[1]<130000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<130000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 2 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 3 - ok - if x[0]>70000 and x[0]<200000 and x[1]>50000 and x[1]<150000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<130000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 3 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 4 - ok - if x[0]>70000 and x[0]<200000 and x[1]>40000 and x[1]<130000 and x[2]>50000 and x[2]<100000 and x[3]>10000 and x[3]<170000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 4 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 5 - ok - if x[0]>90000 and x[0]<200000 and x[1]>50000 and x[1]<130000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<160000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 5 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 7 - if x[0]>100000 and x[0]<200000 and x[1]>70000 and x[1]<120000 and x[2]>50000 and x[2]<90000 and x[3]>20000 and x[3]<150000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 7 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte \n","classe 8 - ok - if x[0]>100000 and x[0]<200000 and x[1]>80000 and x[1]<140000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<120000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 8 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 9 - ok - if x[0]>110000 and x[0]<210000 and x[1]>70000 and x[1]<120000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<100000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 9 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","\n","'''\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXvQynrkw3lZ","executionInfo":{"status":"ok","timestamp":1656869724700,"user_tz":-120,"elapsed":44,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"303683cb-3f71-430c-96ac-26c753e925ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\'\\'\\'#-----------------Generate images out of training--------------------\\'\\'\\'\\n\\nprint(#da cancellare quando vuoi usarlo\\ngenerator_off = Generator()\\ngenerator_off.load_state_dict(torch.load(\"/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_generator_7_3200.pth\"))\\n\\nfor i in range(20):\\n  z = Variable(Tensor(np.random.normal(0, i, (data.shape[0], opt[1][6]))))\\n  imgs_off = generator_off(z)\\n  save_image(imgs_off.data[:30], \"images_batch_7/{}_{}.png\".format(i,i), nrow=6, normalize=True)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":249}],"source":["\"\"\"\n","'''#-----------------Generate images out of training--------------------'''\n","\n","print(#da cancellare quando vuoi usarlo\n","generator_off = Generator()\n","generator_off.load_state_dict(torch.load(\"/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_generator_7_3200.pth\"))\n","\n","for i in range(20):\n","  z = Variable(Tensor(np.random.normal(0, i, (data.shape[0], opt[1][6]))))\n","  imgs_off = generator_off(z)\n","  save_image(imgs_off.data[:30], \"images_batch_7/{}_{}.png\".format(i,i), nrow=6, normalize=True)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"XnUgaUmoFykZ"},"source":["##### GAN with OPTUNA (HYPERPARAMETERS TUNING)\n","attualmente non va perchè servono un sacco di risorse per riuscire a farlo funzionare, e con colab non si riesce."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16wFitvBFGga"},"outputs":[],"source":["!pip install optuna -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tv8ztDxtb6Hq","executionInfo":{"status":"ok","timestamp":1656869728542,"user_tz":-120,"elapsed":380,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9bd4802d-2342-4582-f0cb-ffea04c18a36"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport os\\nimport optuna\\nfrom optuna.trial import TrialState\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nimport torch.utils.data\\nfrom torchvision import datasets\\nfrom torchvision import transforms\\n\\nos.makedirs(\"images\", exist_ok=True)\\nos.makedirs(\"images_to_check\", exist_ok=True)\\n\\n\\n\\n\\nKERNEL_TYPE = \"rbf\"\\n\\ngan_history = {}\\ngan_history[\\'DLOSS\\'] = []\\ngan_history[\\'GLOSS\\'] = []\\ngan_history[\\'MMD\\'] = []\\ngan_history[\\'SSIM\\'] = []\\ngan_history[\\'EPOCH\\'] = []\\n\\nopt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\\n      [400,              64,                0.0002,   0.5,      0.999,    8,           100,                128*2,             3,              4,              0.01,              20]]\\n\\nimg_shape = (opt[1][8], 270, 470)\\ncuda = True if torch.cuda.is_available() else False\\n\\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\\n\\ndef objective2(trial):\\n    #--------------------------------------------------------------------------------------------------------------------------------------------\\n    # Loss function\\n    adversarial_loss = torch.nn.BCELoss()\\n\\n\\n    class GeneratorXXX(nn.Module):\\n        def __init__(self, trial_):\\n          super(GeneratorXXX, self).__init__()\\n          self.xxx=trial_#\\n          \\n          def block(in_feat, out_feat, normalize=True):     #forse aggiungi self come parametro\\n            layers = [nn.Linear(in_feat, out_feat)]\\n            if normalize:\\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\\n            return layers\\n          \\n          self.model = nn.Sequential(\\n            *block(self.xxx, 128, normalize=False), #.suggest_int(\"lat_dim_\", 1, 100)\\n            #*block(100, 128, normalize=False),\\n            *block(128, 256),\\n            *block(256, 512),\\n            *block(512, 1024),\\n            nn.Linear(1024, int(np.prod([img_shape]))),\\n            nn.Tanh())\\n        \\n        def forward(self, zy):\\n          img = self.model(zy)\\n          img = img.view(img.size(0), *img_shape)\\n          return img\\n\\n    \\'\\'\\'----------------------------------------------\\'\\'\\'\\n\\n\\n    class Discriminator(nn.Module):\\n        def __init__(self):\\n            super(Discriminator, self).__init__()\\n\\n            self.model = nn.Sequential(\\n                nn.Linear(int(np.prod(img_shape)), 512),\\n                nn.LeakyReLU(0.2, inplace=True),\\n                nn.Linear(512, 256),\\n                nn.LeakyReLU(0.2, inplace=True),\\n                nn.Linear(256, 1),\\n                nn.Sigmoid(),\\n            )\\n\\n        def forward(self, img):\\n            img_flat = img.view(img.size(0), -1)\\n            validity = self.model(img_flat)\\n\\n            return validity\\n\\n    \\'\\'\\'----------------------------------------------\\'\\'\\'\\n    # Initialize generator and discriminator  \\n    #generator2 = define_model_Gen1(trial) \\n    trial_lat_space = trial.suggest_int(\"lat_dim_\", 50, 500) #di solito viene impostato tra 1 e 100\\n    #print(trial_lat_space)\\n    #print(\\'trial_lat_space {}\\'.format(trial_lat_space))\\n    #print(\\'trial_lat_space {}\\'.format(type(trial_lat_space)))\\n    generator2 =  GeneratorXXX(trial_lat_space)\\n    #generator2 = Generator()\\n    discriminator2 = Discriminator()\\n\\n    if cuda:\\n        generator2.cuda()\\n        discriminator2.cuda()\\n        adversarial_loss.cuda()\\n\\n    # Configure data loader\\n    dataloader = trainloader\\n\\n    # Optimizers\\n\\n    # Generate the optimizers.\\n    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\\n    lr_trial = trial.suggest_float(\"lr_\", 1e-6, 1e-4, log=True)                                     #di solito andare sopra a lr=0.0001 non porta buoni addestramenti\\n    optimizer_gg = getattr(optim, optimizer_name)(generator2.parameters(), lr=lr_trial)\\n    optimizer_dd = getattr(optim, optimizer_name)(discriminator2.parameters(), lr=lr_trial)\\n\\n\\n    if optimizer_name == \"Adam\":\\n        optimizer_G2 = torch.optim.Adam(generator2.parameters(), lr=lr_trial, betas=(trial.suggest_float(\"b1_\", 0.4, 0.6, log=True),  trial.suggest_float(\"b2_\", 0.8, 0.999, log=True)))\\n        optimizer_D2 = torch.optim.Adam(discriminator2.parameters(), lr=lr_trial, betas=(trial.suggest_float(\"b1_\", 0.4, 0.6, log=True),  trial.suggest_float(\"b2_\", 0.8, 0.999, log=True)))\\n    else:\\n        optimizer_G2=optimizer_gg\\n        optimizer_D2=optimizer_dd\\n\\n    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\\n\\n    # ----------\\n    #  Training\\n    # ----------\\n\\n\\n    \\'\\'\\'--------------------parametri per calcolare NDB score-----------------\\'\\'\\'\\n    batches_done = 0\\n    real_batches = [] #to save real images to be used with NDB METRIC\\n    generated_batches = [] #to save real images to be used with NDB METRIC\\n    iters = 0\\n    img_list = []\\n    metrics = []\\n    ndb_scores = []\\n    jj = 0 #contatore per salvare tutte le immagini del batch da usare poi con metrica FID\\n\\n    gen_loss_tensor = torch.tensor([])\\n\\n\\n    \\'\\'\\'estrazione batch di immagini reali per NDB\\'\\'\\'\\n    \\n    real_combined = real_samples(numTrainBatches, dataloader, nc, img_shape[1],img_shape[2])      # bs x 3 x width x height\\n    \\'\\'\\'----------------------------------------------------------------------\\'\\'\\'  \\n\\n    #il KERNEL_TYPE = \"rbf\" serve per il MMD \\n    KERNEL_TYPE = \"rbf\"\\n\\n    for epoch in range(opt[1][0]):\\n        for i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\\n\\n            data = data.to(device=device)\\n            targets = targets.to(device = device) #classes\\n            targets2 = targets2.to(device = device) #series\\n\\n\\n            # Adversarial ground truths\\n            valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\\n            fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\\n\\n            # Configure input\\n            real_imgs = Variable(data.type(Tensor))\\n\\n            # -----------------\\n            #  Train Generator\\n            # -----------------\\n\\n            optimizer_G2.zero_grad()\\n\\n            # Sample noise as generator input\\n            zy = Variable(Tensor(np.random.normal(0, 1, (data.shape[0],  trial_lat_space)))) \\n           \\n\\n            # Generate a batch of images\\n            gen_imgs = generator2(zy)\\n\\n\\n            # Loss measures generator\\'s ability to fool the discriminator\\n            g_loss = adversarial_loss(discriminator2(gen_imgs), valid)\\n\\n            g_loss.backward()\\n            optimizer_G2.step()\\n\\n            # ---------------------\\n            #  Train Discriminator\\n            # ---------------------\\n\\n            optimizer_D2.zero_grad()\\n\\n            # Measure discriminator\\'s ability to classify real from generated samples\\n            real_loss = adversarial_loss(discriminator2(real_imgs), valid)\\n            fake_loss = adversarial_loss(discriminator2(gen_imgs.detach()), fake)\\n            d_loss = (real_loss + fake_loss) / 2\\n\\n            d_loss.backward()\\n            optimizer_D2.step()\\n\\n            \\n\\n            print(\\n                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\\n                % (epoch, opt[1][0], i, len(dataloader), d_loss.item(), g_loss.item())\\n            )\\n\\n            batches_done = epoch * len(dataloader) + i\\n            if batches_done % opt[1][11] == 0: \\n              #save_image(gen_imgs.data[:1], \"images/%d.png\" % batches_done, nrow=1, normalize=True)\\n              \\n              \\'\\'\\'QUESTO SOTTO PER FARE DEI CONFRONTI SU IMMAGINI SALVATE PIÙ DI UNA INSIEME\\'\\'\\'\\n              #save_image(gen_imgs.data[:5], \"images_to_check_tuning_hyperp/%d.png\" % batches_done, nrow=5, normalize=True)   #nrow = numero immagini per ciascuna riga\\n\\n\\n\\n        #--------------------------------------------------------------------------------------------\\n        #--------------------------------------------------------------------------------------------\\n\\n        \\'\\'\\'PARTE DI CALCOLO DELL\\' MMD\\'\\'\\'\\n        #---- Maximum Mean Discrepancy (MMD) (non capisco come farlo funzionare anche con le img a colori, se ti da qualche errore, prova prima a rieseguire il blocco subito sopra -----\\n        \\n        \\n        real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\\n        #print(real_imgs2.shape)\\n        #print(type(real_imgs2))\\n\\n        gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\\n        #print(gen_imgs2.shape)\\n        #print(type(gen_imgs2))\\n\\n        mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all\\'interno dell\\'file \\n        #print(\\'MMD AVERAGE : {}\\'.format(mmd_average))\\n\\n        #--------------------------------------------------------------------------------------------\\n        #--------------------------------------------------------------------------------------------\\n\\n        \\'\\'\\'PARTE DI CALCOLO DELLA FID\\'\\'\\'\\n        \\'\\'\\'SALVO TUTTE LE IMMAGINI GENERATE DAL BATCH COSÌ DA USARLE CON LA METRICA \\'FID\\' \\'\\'\\'\\n        \\'\\'\\'\\n        for jj in range(len(gen_imgs)):\\n          save_image(gen_imgs.data[jj], \"images/%d.png\" % jj, nrow=1, normalize=True)   #nrow = numero immagini per ciascuna riga\\n        \\n        images1 = load_images(\\'/content/drive/MyDrive/CALCIO_CROP_BASE/images/\\')\\n        images2 = load_images(\\'/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0\\')\\n        BatchSize = 8 \\n        UseMultiprocessing = False \\n        \\n        #con queste 2 sotto non mi va\\n        #reals_ = np.transpose(real_combined, (0,2,3,1))     #la metrica FID vuole le immagini in ordine (bs x width x height x 3)\\n        #fakes_ = np.transpose(gen_combined, (0,2,3,1))     #la metrica FID vuole le immagini in ordine (bs x width x height x 3)\\n\\n        fid_value = calculate_fid(images1, images2, UseMultiprocessing, BatchSize)\\n        #print(\"fid_value :{}\".format(fid_value))\\n        \\'\\'\\'\\n\\n\\n        \\'\\'\\'AVERAGE LOSS DA MONITORARE\\'\\'\\'\\n        average_loss = (d_loss + g_loss)/2\\n        b = torch.tensor([g_loss])\\n\\n        gen_loss_tensor = torch.cat((gen_loss_tensor, b), \\n\\n\\n\\n        \\'\\'\\'memorizzo info su addestramento GAN\\'\\'\\'\\n        gan_history[\\'DLOSS\\'].append(d_loss.item())\\n        gan_history[\\'GLOSS\\'].append(g_loss.item()) \\n        gan_history[\\'EPOCH\\'].append(epoch)\\n        gan_history[\\'MMD\\'].append(mmd_average.item())\\n        #gan_history[\\'SSIM\\'].append(ris_ssim.item())\\n        \\n        \\n        \\'\\'\\'queste 4 righe sotto non vanno se si monitorano più metriche insieme\\'\\'\\'\\n        #trial.report(mmd_average, epoch)\\n\\n        # Handle pruning based on the intermediate value.\\n        #if trial.should_prune():\\n            #raise optuna.exceptions.TrialPruned()\\n\\n\\n\\n    #print(\"tensore di generator loss\")\\n    #print(gen_loss_tensor)\\n    var_gen_loss = torch.var(gen_loss_tensor, unbiased=False)\\n    diff_loss = abs(g_loss - d_loss)\\n    return var_gen_loss, g_loss, diff_loss        #fid_value (ci mette troppo, per usarlo togliere i load delle immagini se riesci ad adattare i tensori)\\n\\n\\n\\n\\'\\'\\'MONITORARE MOLTEPLICI METRICHE PER VALUTARE LA GAN\\'\\'\\'\\n#https://stackoverflow.com/questions/69071684/how-to-optimize-for-multiple-metrics-in-optuna\\n\\n\\nstudy = optuna.create_study(directions=[\"maximize\", \"minimize\", \"minimize\"])   #aggiungere un\\'altra metrica della loss, e anche altre 2 tipo LPIPS e SSIM/KL-Div\\n\\'\\'\\'NOTE\\'\\'\\'\\n#forse la varianza da massimizzare perchè altrimenti significa che non si sta allendando per niente\\n#aggiungi al posto della loss da minimizzare perchè prenderebbe l\\'ulitmo valore (la media della loss G)\\n\\n\\n\\n\\nstudy.optimize(objective2, n_trials=10)\\n\\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\\n\\nprint(\"Study statistics: \")\\nprint(\"  Number of finished trials: \", len(study.trials))\\nprint(\"  Number of pruned trials: \", len(pruned_trials))\\nprint(\"  Number of complete trials: \", len(complete_trials))\\n\\n\\'\\'\\'#per singolo parametro monitorato\\nprint(\"Best trial:\")\\ntrial = study.best_trial\\n\\nprint(\"  Value: \", trial.value)\\n\\nprint(\"  Params: \")\\nfor key, value in trial.params.items():\\n  print(\"    {}: {}\".format(key, value))\\n\\'\\'\\'\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":251}],"source":["\"\"\"\n","import os\n","import optuna\n","from optuna.trial import TrialState\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data\n","from torchvision import datasets\n","from torchvision import transforms\n","\n","os.makedirs(\"images\", exist_ok=True)\n","os.makedirs(\"images_to_check\", exist_ok=True)\n","\n","\n","\n","\n","KERNEL_TYPE = \"rbf\"\n","\n","gan_history = {}\n","gan_history['DLOSS'] = []\n","gan_history['GLOSS'] = []\n","gan_history['MMD'] = []\n","gan_history['SSIM'] = []\n","gan_history['EPOCH'] = []\n","\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","      [400,              64,                0.0002,   0.5,      0.999,    8,           100,                128*2,             3,              4,              0.01,              20]]\n","\n","img_shape = (opt[1][8], 270, 470)\n","cuda = True if torch.cuda.is_available() else False\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","def objective2(trial):\n","    #--------------------------------------------------------------------------------------------------------------------------------------------\n","    # Loss function\n","    adversarial_loss = torch.nn.BCELoss()\n","\n","\n","    class GeneratorXXX(nn.Module):\n","        def __init__(self, trial_):\n","          super(GeneratorXXX, self).__init__()\n","          self.xxx=trial_#\n","          \n","          def block(in_feat, out_feat, normalize=True):     #forse aggiungi self come parametro\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","          \n","          self.model = nn.Sequential(\n","            *block(self.xxx, 128, normalize=False), #.suggest_int(\"lat_dim_\", 1, 100)\n","            #*block(100, 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod([img_shape]))),\n","            nn.Tanh())\n","        \n","        def forward(self, zy):\n","          img = self.model(zy)\n","          img = img.view(img.size(0), *img_shape)\n","          return img\n","\n","    '''----------------------------------------------'''\n","\n","\n","    class Discriminator(nn.Module):\n","        def __init__(self):\n","            super(Discriminator, self).__init__()\n","\n","            self.model = nn.Sequential(\n","                nn.Linear(int(np.prod(img_shape)), 512),\n","                nn.LeakyReLU(0.2, inplace=True),\n","                nn.Linear(512, 256),\n","                nn.LeakyReLU(0.2, inplace=True),\n","                nn.Linear(256, 1),\n","                nn.Sigmoid(),\n","            )\n","\n","        def forward(self, img):\n","            img_flat = img.view(img.size(0), -1)\n","            validity = self.model(img_flat)\n","\n","            return validity\n","\n","    '''----------------------------------------------'''\n","    # Initialize generator and discriminator  \n","    #generator2 = define_model_Gen1(trial) \n","    trial_lat_space = trial.suggest_int(\"lat_dim_\", 50, 500) #di solito viene impostato tra 1 e 100\n","    #print(trial_lat_space)\n","    #print('trial_lat_space {}'.format(trial_lat_space))\n","    #print('trial_lat_space {}'.format(type(trial_lat_space)))\n","    generator2 =  GeneratorXXX(trial_lat_space)\n","    #generator2 = Generator()\n","    discriminator2 = Discriminator()\n","\n","    if cuda:\n","        generator2.cuda()\n","        discriminator2.cuda()\n","        adversarial_loss.cuda()\n","\n","    # Configure data loader\n","    dataloader = trainloader\n","\n","    # Optimizers\n","\n","    # Generate the optimizers.\n","    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n","    lr_trial = trial.suggest_float(\"lr_\", 1e-6, 1e-4, log=True)                                     #di solito andare sopra a lr=0.0001 non porta buoni addestramenti\n","    optimizer_gg = getattr(optim, optimizer_name)(generator2.parameters(), lr=lr_trial)\n","    optimizer_dd = getattr(optim, optimizer_name)(discriminator2.parameters(), lr=lr_trial)\n","\n","\n","    if optimizer_name == \"Adam\":\n","        optimizer_G2 = torch.optim.Adam(generator2.parameters(), lr=lr_trial, betas=(trial.suggest_float(\"b1_\", 0.4, 0.6, log=True),  trial.suggest_float(\"b2_\", 0.8, 0.999, log=True)))\n","        optimizer_D2 = torch.optim.Adam(discriminator2.parameters(), lr=lr_trial, betas=(trial.suggest_float(\"b1_\", 0.4, 0.6, log=True),  trial.suggest_float(\"b2_\", 0.8, 0.999, log=True)))\n","    else:\n","        optimizer_G2=optimizer_gg\n","        optimizer_D2=optimizer_dd\n","\n","    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","    # ----------\n","    #  Training\n","    # ----------\n","\n","\n","    '''--------------------parametri per calcolare NDB score-----------------'''\n","    batches_done = 0\n","    real_batches = [] #to save real images to be used with NDB METRIC\n","    generated_batches = [] #to save real images to be used with NDB METRIC\n","    iters = 0\n","    img_list = []\n","    metrics = []\n","    ndb_scores = []\n","    jj = 0 #contatore per salvare tutte le immagini del batch da usare poi con metrica FID\n","\n","    gen_loss_tensor = torch.tensor([])\n","\n","\n","    '''estrazione batch di immagini reali per NDB'''\n","    \n","    real_combined = real_samples(numTrainBatches, dataloader, nc, img_shape[1],img_shape[2])      # bs x 3 x width x height\n","    '''----------------------------------------------------------------------'''  \n","\n","    #il KERNEL_TYPE = \"rbf\" serve per il MMD \n","    KERNEL_TYPE = \"rbf\"\n","\n","    for epoch in range(opt[1][0]):\n","        for i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\n","\n","            data = data.to(device=device)\n","            targets = targets.to(device = device) #classes\n","            targets2 = targets2.to(device = device) #series\n","\n","\n","            # Adversarial ground truths\n","            valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n","            fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n","\n","            # Configure input\n","            real_imgs = Variable(data.type(Tensor))\n","\n","            # -----------------\n","            #  Train Generator\n","            # -----------------\n","\n","            optimizer_G2.zero_grad()\n","\n","            # Sample noise as generator input\n","            zy = Variable(Tensor(np.random.normal(0, 1, (data.shape[0],  trial_lat_space)))) \n","           \n","\n","            # Generate a batch of images\n","            gen_imgs = generator2(zy)\n","\n","\n","            # Loss measures generator's ability to fool the discriminator\n","            g_loss = adversarial_loss(discriminator2(gen_imgs), valid)\n","\n","            g_loss.backward()\n","            optimizer_G2.step()\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            optimizer_D2.zero_grad()\n","\n","            # Measure discriminator's ability to classify real from generated samples\n","            real_loss = adversarial_loss(discriminator2(real_imgs), valid)\n","            fake_loss = adversarial_loss(discriminator2(gen_imgs.detach()), fake)\n","            d_loss = (real_loss + fake_loss) / 2\n","\n","            d_loss.backward()\n","            optimizer_D2.step()\n","\n","            \n","\n","            print(\n","                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n","                % (epoch, opt[1][0], i, len(dataloader), d_loss.item(), g_loss.item())\n","            )\n","\n","            batches_done = epoch * len(dataloader) + i\n","            if batches_done % opt[1][11] == 0: \n","              #save_image(gen_imgs.data[:1], \"images/%d.png\" % batches_done, nrow=1, normalize=True)\n","              \n","              '''QUESTO SOTTO PER FARE DEI CONFRONTI SU IMMAGINI SALVATE PIÙ DI UNA INSIEME'''\n","              #save_image(gen_imgs.data[:5], \"images_to_check_tuning_hyperp/%d.png\" % batches_done, nrow=5, normalize=True)   #nrow = numero immagini per ciascuna riga\n","\n","\n","\n","        #--------------------------------------------------------------------------------------------\n","        #--------------------------------------------------------------------------------------------\n","\n","        '''PARTE DI CALCOLO DELL' MMD'''\n","        #---- Maximum Mean Discrepancy (MMD) (non capisco come farlo funzionare anche con le img a colori, se ti da qualche errore, prova prima a rieseguire il blocco subito sopra -----\n","        \n","        \n","        real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\n","        #print(real_imgs2.shape)\n","        #print(type(real_imgs2))\n","\n","        gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\n","        #print(gen_imgs2.shape)\n","        #print(type(gen_imgs2))\n","\n","        mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all'interno dell'file \n","        #print('MMD AVERAGE : {}'.format(mmd_average))\n","\n","        #--------------------------------------------------------------------------------------------\n","        #--------------------------------------------------------------------------------------------\n","\n","        '''PARTE DI CALCOLO DELLA FID'''\n","        '''SALVO TUTTE LE IMMAGINI GENERATE DAL BATCH COSÌ DA USARLE CON LA METRICA 'FID' '''\n","        '''\n","        for jj in range(len(gen_imgs)):\n","          save_image(gen_imgs.data[jj], \"images/%d.png\" % jj, nrow=1, normalize=True)   #nrow = numero immagini per ciascuna riga\n","        \n","        images1 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/images/')\n","        images2 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0')\n","        BatchSize = 8 \n","        UseMultiprocessing = False \n","        \n","        #con queste 2 sotto non mi va\n","        #reals_ = np.transpose(real_combined, (0,2,3,1))     #la metrica FID vuole le immagini in ordine (bs x width x height x 3)\n","        #fakes_ = np.transpose(gen_combined, (0,2,3,1))     #la metrica FID vuole le immagini in ordine (bs x width x height x 3)\n","\n","        fid_value = calculate_fid(images1, images2, UseMultiprocessing, BatchSize)\n","        #print(\"fid_value :{}\".format(fid_value))\n","        '''\n","\n","\n","        '''AVERAGE LOSS DA MONITORARE'''\n","        average_loss = (d_loss + g_loss)/2\n","        b = torch.tensor([g_loss])\n","\n","        gen_loss_tensor = torch.cat((gen_loss_tensor, b), \n","\n","\n","\n","        '''memorizzo info su addestramento GAN'''\n","        gan_history['DLOSS'].append(d_loss.item())\n","        gan_history['GLOSS'].append(g_loss.item()) \n","        gan_history['EPOCH'].append(epoch)\n","        gan_history['MMD'].append(mmd_average.item())\n","        #gan_history['SSIM'].append(ris_ssim.item())\n","        \n","        \n","        '''queste 4 righe sotto non vanno se si monitorano più metriche insieme'''\n","        #trial.report(mmd_average, epoch)\n","\n","        # Handle pruning based on the intermediate value.\n","        #if trial.should_prune():\n","            #raise optuna.exceptions.TrialPruned()\n","\n","\n","\n","    #print(\"tensore di generator loss\")\n","    #print(gen_loss_tensor)\n","    var_gen_loss = torch.var(gen_loss_tensor, unbiased=False)\n","    diff_loss = abs(g_loss - d_loss)\n","    return var_gen_loss, g_loss, diff_loss        #fid_value (ci mette troppo, per usarlo togliere i load delle immagini se riesci ad adattare i tensori)\n","\n","\n","\n","'''MONITORARE MOLTEPLICI METRICHE PER VALUTARE LA GAN'''\n","#https://stackoverflow.com/questions/69071684/how-to-optimize-for-multiple-metrics-in-optuna\n","\n","\n","study = optuna.create_study(directions=[\"maximize\", \"minimize\", \"minimize\"])   #aggiungere un'altra metrica della loss, e anche altre 2 tipo LPIPS e SSIM/KL-Div\n","'''NOTE'''\n","#forse la varianza da massimizzare perchè altrimenti significa che non si sta allendando per niente\n","#aggiungi al posto della loss da minimizzare perchè prenderebbe l'ulitmo valore (la media della loss G)\n","\n","\n","\n","\n","study.optimize(objective2, n_trials=10)\n","\n","pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","print(\"Study statistics: \")\n","print(\"  Number of finished trials: \", len(study.trials))\n","print(\"  Number of pruned trials: \", len(pruned_trials))\n","print(\"  Number of complete trials: \", len(complete_trials))\n","\n","'''#per singolo parametro monitorato\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","  print(\"    {}: {}\".format(key, value))\n","'''\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EtlNBL0aIV7","executionInfo":{"status":"ok","timestamp":1656869728544,"user_tz":-120,"elapsed":34,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4cee9c2b-f49f-4e29-9e99-9d9d0f9ecf39"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#PARTE DI RICERCA EFFETTIVA DELLA COMBINAZIONE MIGLIORE DEGLI IPERPARAMETRI\\ntrial = study.best_trials\\n\\nfor count_trial in range(len(trial)):\\n  print(\"G_VARIANCE - G_MIN_LOSS - G-D LOSS\")\\n  print(\"  Values: {}\".format(trial[count_trial].values))\\n\\n  print(\"  Params: \")\\n  for key, value in trial[count_trial].params.items():\\n    print(\"    {}: {}\".format(key, value))\\n    print(\"\")\\n  print(\"-------------------------------------------\")\\n\\nprint(trial)\\nprint(\"\")\\nprint(\"\")\\nprint(trial[0].values)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":252}],"source":["\"\"\"\n","#PARTE DI RICERCA EFFETTIVA DELLA COMBINAZIONE MIGLIORE DEGLI IPERPARAMETRI\n","trial = study.best_trials\n","\n","for count_trial in range(len(trial)):\n","  print(\"G_VARIANCE - G_MIN_LOSS - G-D LOSS\")\n","  print(\"  Values: {}\".format(trial[count_trial].values))\n","\n","  print(\"  Params: \")\n","  for key, value in trial[count_trial].params.items():\n","    print(\"    {}: {}\".format(key, value))\n","    print(\"\")\n","  print(\"-------------------------------------------\")\n","\n","print(trial)\n","print(\"\")\n","print(\"\")\n","print(trial[0].values)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"LXKsNF5hSloT"},"source":["##### PLOT GAN LOSS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgPY7gnjvdDX","executionInfo":{"status":"ok","timestamp":1656869728546,"user_tz":-120,"elapsed":31,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ffd77a2-359c-405f-e97d-44b2e3a748b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nDLOSS = gan_history['DLOSS']\\nGLOSS = gan_history['GLOSS']\\nEPOCH = gan_history['EPOCH']\\n\\nplt.figure(figsize=(50, 4), dpi=100)              #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\\nplt.plot(EPOCH, DLOSS, 'b', label='DLOSS')\\nplt.plot(EPOCH, GLOSS, 'r', label='GLOSS ')\\n#plt.plot(EPOCH, ndb_scores, 'g', label='NDB_SCORES ')\\nplt.title('Training and validation accuracy (IMG)')\\nplt.legend()\\nplt.savefig(os.path.join(path_progettoDL+'weights/GANLOSS.pdf')) \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":254}],"source":["\n","DLOSS = gan_history['DLOSS']\n","GLOSS = gan_history['GLOSS']\n","EPOCH = gan_history['EPOCH']\n","\n","plt.figure(figsize=(50, 4), dpi=100)              #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n","plt.plot(EPOCH, DLOSS, 'b', label='DLOSS')\n","plt.plot(EPOCH, GLOSS, 'r', label='GLOSS ')\n","#plt.plot(EPOCH, ndb_scores, 'g', label='NDB_SCORES ')\n","plt.title('Training and validation accuracy (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/GANLOSS.pdf')) \n"]},{"cell_type":"markdown","metadata":{"id":"D5TSgRgRs96F"},"source":["###GAN METRICS DA CALCOLARE OFFLINE"]},{"cell_type":"markdown","metadata":{"id":"u6nY8B7VcfYj"},"source":["####PLOT FID METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rCssMXychf5","executionInfo":{"status":"ok","timestamp":1656876452695,"user_tz":-120,"elapsed":58073,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e05184f2-1112-424f-d1f9-a4306080b40b"},"outputs":[{"output_type":"stream","name":"stdout","text":["(29, 270, 470, 3)\n","(30, 270, 470, 3)\n","(30, 270, 470, 3)\n","135.58481259668577\n","112.13097059054073\n","115.40922215963286\n","115.40922215963286\n","114.31647163660216\n"]}],"source":["'''ALTERNATIVA SCARICANDO LA VERSIONE IN GITHUB DI UN UTENTE'''\n","os.chdir('/content/drive/MyDrive/')\n","#!git clone https://github.com/hukkelas/pytorch-frechet-inception-distance.git fid_metric \n","\n","#!python /content/drive/MyDrive/fid_metric/fid.py --path1 /content/drive/MyDrive/CALCIO_CROP_BASE/patches/class0.0 --path2 /content/drive/MyDrive/CALCIO_CROP_BASE/Data_Aug_GAN --batch-size 8\n","\n","\n","\n","\n","'''CALCOLO METRICA ATTRAVERSO CLASSE IMPLEMENTATA'''\n","images1 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0')     #QUA CAMBIA\n","images1 = images1[1:(int(len(images1)/2)-1)]\n","print(images1.shape)\n","\n","images2 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0')     #QUA CAMBIA\n","images2 = images2[(int(len(images2)/2)+1):len(images2)]\n","print(images2.shape)\n","\n","images4 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0')     #QUA CAMBIA\n","images4 = images4[(int(len(images4)/4)+1):int(len(images4)/4*3)]\n","print(images2.shape)\n","\n","images3 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/Data_Aug_GAN/images_9')  #QUA CAMBIA\n","BatchSize = 8 \n","UseMultiprocessing = False \n","fid_value = calculate_fid(images1, images2, UseMultiprocessing, BatchSize)\n","fid_value2 = calculate_fid(images1, images3, UseMultiprocessing, BatchSize)\n","fid_value3 = calculate_fid(images2, images3, UseMultiprocessing, BatchSize)\n","fid_value4 = calculate_fid(images2, images3, UseMultiprocessing, BatchSize)\n","print(fid_value)\n","print(fid_value2)\n","print(fid_value3)\n","print(fid_value4)\n","print((fid_value2+fid_value3+fid_value4)/3)"]},{"cell_type":"markdown","metadata":{"id":"A_U_C71VdOAO"},"source":["####PLOT IS METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PqQamMndPmv","executionInfo":{"status":"ok","timestamp":1656876457865,"user_tz":-120,"elapsed":5225,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e55d3998-5503-4b12-a19e-ac760ab8ea78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Inception Score...\n","        ID series            filename  class\n","66    1817      4  20200529172017.png    9.0\n","67    1817      4  20200529171958.png    9.0\n","70    1836     10  20200529194441.png    9.0\n","71    1836     10  20200529194427.png    9.0\n","106   1689      8  20200525181453.png    9.0\n","...    ...    ...                 ...    ...\n","1218   637      4  20200506131733.png    9.0\n","1244   473      4  20203 108 2449.png    9.0\n","1245   473      4  20203 108 2438.png    9.0\n","1246   498      4  20203 109 5735.png    9.0\n","1247   498      4  20203 109 5720.png    9.0\n","\n","[62 rows x 4 columns]\n","62\n","       ID  series                               filename  class\n","642  2670      10  Data_Aug_GAN/images_9/2501_1_18_2.png      9\n","643  2671      10    Data_Aug_GAN/images_9/2502_1_11.png      9\n","644  2672      10    Data_Aug_GAN/images_9/2503_1_14.png      9\n","645  2673      10     Data_Aug_GAN/images_9/2503_1_2.png      9\n","646  2674      10  Data_Aug_GAN/images_9/2514_1_27_2.png      9\n","..    ...     ...                                    ...    ...\n","781  2809      10    Data_Aug_GAN/images_9/3998_1_21.png      9\n","782  2810      10    Data_Aug_GAN/images_9/3999_1_15.png      9\n","783  2811      10    Data_Aug_GAN/images_9/3999_1_23.png      9\n","784  2812      10    Data_Aug_GAN/images_9/3999_1_26.png      9\n","785  2813      10    Data_Aug_GAN/images_9/3999_1_29.png      9\n","\n","[144 rows x 4 columns]\n","144\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2501_1_18_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2502_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2503_1_14.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2503_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2514_1_27_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2521_1_30_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2531_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2536_1_26_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2544_1_14_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2575_1_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2594_1_10_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2601_1_15_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2615_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2633_1_13_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2650_1_31_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2733_1_20_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2743_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2766_1_30_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/2900_1.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3048_1_12_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3116_1_27_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3147_40.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3210_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3218_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3230_1_2_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3259_1_24_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3281_50.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3358_1_24_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3416_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3418_30.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3419_1_7_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3419_38.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3424_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3425_41.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3428_40.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3430_31.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3435_1_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3452_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3470_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3471_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3471_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3471_1_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_27.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3472_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3473_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3496_1_12_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3496_1_8_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3497_1_13.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3497_1_13_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3497_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3497_1_22.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3498_1_21_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3498_1_5.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3499_1_25_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3499_1_6.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3499_1_6_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3532_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3561_41.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3563_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3565_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3569_60.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3570_18.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3572_39.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3574_42.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3577_54.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3578_55.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3595_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3595_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3599_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3606_36.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3615_53.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3620_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3623_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3625_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3625_1_19.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3629_46.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3630_55.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3631_34.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3632_47.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3655_1_11.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3657_33.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3667_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3668_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3668_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3670_1_1.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3674_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3676_1_2.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3701_43.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3705_26.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3719_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3720_30.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3721_3.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3731_1_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3734_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3736_9.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3744_52.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3749_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3750_25.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3755_1_12.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3777_1_4.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3777_47.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3806_35.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3810_55.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3812_17.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3845_35.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3912_1_0.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3923_1_8.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3941_1_24.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3967_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3994_1_7.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3995_1_16.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3996_1_20.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3998_1_21.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3999_1_15.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3999_1_23.png eliminato\n","File Non Esiste !!!\n","File : Data_Aug_GAN/images_9/3999_1_26.png eliminato\n","File Eliminati : 120 \n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/Colab Notebooks/algoritmi_custom/custom_IS.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(x).data.cpu().numpy()\n"]},{"output_type":"stream","name":"stdout","text":["Inception Score : 1.78850\n","Inception Score : 1.53012\n"]}],"source":["print (\"Calculating Inception Score...\")\n","\n","#estraggo le immagini vere\n","df_class_ = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 9]    #QUA CAMBIA\n","print(df_class_)\n","print(len(df_class_))\n","\n","#estraggo le immagini dgenerate\n","df_class_GAN_ = dataframe_GAN.loc[dataframe_GAN['class'] == 9]                     #QUA CAMBIA\n","print(df_class_GAN_)\n","print(len(df_class_GAN_))\n","\n","os.chdir(path_images)\n","i = 0; \n","for index, row in df_class_GAN_.iterrows():\n","    filename = row['filename']\n","    if os.path.exists(path_images+filename) == False:\n","      print('File Non Esiste !!!')\n","    \n","    if(os.path.exists(filename) == False):\n","      df_class_GAN_ = df_class_GAN_.drop(df_class_GAN_[(df_class_GAN_['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","      i = i + 1             \n","print('File Eliminati : {} '.format(i))\n","#print(df_class_GAN_0)\n","\n","'''\n","train_c_dataset_2 = CustomDataset(df_class_new, transform_0=_transform_GAN) \n","trainloader_2 = torch.utils.data.DataLoader(dataset = train_c_dataset, batch_size=bs, shuffle=True)\n","\n","print(trainloader_2.dataset.dataframe)\n","'''\n","\n","train_c_dataset_IS = CustomDataset(df_class_, transform_0=_transform_)       #modofica qua\n","trainloader_IS = torch.utils.data.DataLoader(dataset = train_c_dataset_IS, batch_size=32, shuffle=True)#,num_workers=2\n","\n","train_c_dataset_IS_GAN = CustomDataset(df_class_GAN_, transform_0=_transform_)       #modifica qua\n","trainloader_IS_GAN = torch.utils.data.DataLoader(dataset = train_c_dataset_IS_GAN, batch_size=16, shuffle=True)#,num_workers=2\n","\n","inception_score_ = (inception_score(trainloader_IS, cuda=False, batch_size=32, resize=False, splits=1))\n","inception_score_GAN = (inception_score(trainloader_IS_GAN, cuda=False, batch_size=16, resize=False, splits=1))\n","\n","print(f'Inception Score : {inception_score_[0]:.5f}')\n","print(f'Inception Score : {inception_score_GAN[0]:.5f}')"]},{"cell_type":"markdown","metadata":{"id":"-GViGlheXfJt"},"source":["#### PLOT SSIM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o596X4T3QGBR","executionInfo":{"status":"ok","timestamp":1656876458928,"user_tz":-120,"elapsed":1124,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"11a5b98e-48d1-4734-afdb-da7865ad5eea"},"outputs":[{"output_type":"stream","name":"stderr","text":["1it [00:00,  3.78it/s]"]},{"output_type":"stream","name":"stdout","text":["(1, 32, 3, 270, 470)\n"]},{"output_type":"stream","name":"stderr","text":["2it [00:00,  3.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30, 3, 270, 470)\n","(62, 3, 270, 470)\n","torch.Size([62, 3, 270, 470])\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:00,  4.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(24, 3, 270, 470)\n","tensor(0.1745)\n","tensor(0.1230)\n"]}],"source":["'''CALCOLO SSIM'''\n","\n","#gen_combined = generated_fakes(numTestBatches, generator, img_shape[1], img_shape[2], bs, opt[1][6])\n","\n","#---------------------------preparazione tensori------------------------------\n","#--------------------------------------preparazione immagini reali\n","real_concatenate= []\n","real_batches = []\n","real_batches_np = []\n","trainloader_SSIM = torch.utils.data.DataLoader(dataset = train_c_dataset_IS, batch_size=32, shuffle=True)#,num_workers=2\n","\n","trainloader_SSIM_GAN = torch.utils.data.DataLoader(dataset = train_c_dataset_IS_GAN, batch_size=32, shuffle=True)#,num_workers=2\n","for i, (data, targets, targets2) in tqdm(enumerate(trainloader_SSIM)):\n","\n","    #- MEMORIZZO IL SET DI IMMAGINI REALI PER USARLO NELLA METRICA 'NDB' -\n","    real = data.numpy()\n","    if len(trainloader_SSIM)==1:\n","      real_batches.append(real)\n","      real_concatenate = np.array(real_batches)\n","    else:\n","      if i==0:\n","        real_batches.append(real)\n","        real_batches_np = np.array(real_batches)\n","        print(real_batches_np.shape)\n","        real_batches = []\n","      if i==1:\n","        real_batches.append(real)\n","        real_batches2_np = np.array(real_batches)\n","        print(real_batches2_np.shape)\n","        real_batches = []\n","        real_concatenate = np.concatenate((real_batches_np,real_batches2_np), axis=1)\n","      if i>1:\n","        real_batches.append(real)\n","        real_batches2_np = np.array(real_batches)\n","        print(real_batches2_np.shape)\n","        real_batches = []\n","        real_concatenate = np.concatenate((real_concatenate,real_batches2_np), axis=1)\n","\n","real_batches = real_concatenate\n","real_combined = real_batches.reshape(len(df_class_), 3, 270, 470)  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","print(real_combined.shape)\n","\n","real_combined_ssim = torch.from_numpy(real_combined)\n","real_combined_ssim_1 = real_combined_ssim[1:30]\n","real_combined_ssim_2 = real_combined_ssim[31:+60]\n","real_combined_ssim_3 = real_combined_ssim[1:len(df_class_GAN_)+1]\n","print(real_combined_ssim.shape)\n","\n","#--------------------------------------------------preparazione tensore immagini finte\n","real_concatenate_= []\n","real_batches_ = []\n","real_batches_np_ = []\n","for i, (data, targets, targets2) in tqdm(enumerate(trainloader_SSIM_GAN)):\n","\n","    #- MEMORIZZO IL SET DI IMMAGINI REALI PER USARLO NELLA METRICA 'NDB' -\n","    real = data.numpy()\n","    if len(trainloader_SSIM_GAN)==1:\n","      real_batches_.append(real)\n","      real_concatenate_ = np.array(real_batches_)\n","    else:\n","      if i==0:\n","        real_batches_.append(real)\n","        real_batches_np_ = np.array(real_batches_)\n","        print(real_batches_np_.shape)\n","        real_batches_ = []\n","      if i==1:\n","        real_batches_.append(real)\n","        real_batches2_np_ = np.array(real_batches_)\n","        print(real_batches2_np_.shape)\n","        real_batches_ = []\n","        real_concatenate_ = np.concatenate((real_batches_np_,real_batches2_np_), axis=1)\n","      if i>1:\n","        real_batches_.append(real)\n","        real_batches2_np_ = np.array(real_batches_)\n","        print(real_batches2_np_.shape)\n","        real_batches_ = []\n","        real_concatenate_ = np.concatenate((real_concatenate,real_batches2_np), axis=1)\n","\n","real_batches_ = real_concatenate_\n","real_combined_ = real_batches_.reshape(len(df_class_GAN_), 3, 270, 470)  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","print(real_combined_.shape)\n","\n","real_combined_ssim_4 = torch.from_numpy(real_combined_)\n","\n","\n","\n","#real_combined_ssim = real_batches_np.reshape(len(data)+(i*64), 3, 270, 470)  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","#real_combined_ssim = torch.from_numpy(real_combined_ssim)\n","#print(real_combined_ssim.shape)\n","\n","\n","#---SOTTO SONO LE RIGHE ORIGINALI---\n","#real_combined2 = torch.from_numpy(real_combined)\n","#gen_combined2 = torch.from_numpy(gen_combined)\n","\n","'''\n","#qua sempre con un genloader, vedi di creare il tensore come è stato fatto per real_combined\n","'''\n","\n","#calcolo metrica\n","ris_ssim = ssim(real_combined_ssim_1,real_combined_ssim_2)\n","print(ris_ssim)\n","\n","ris_ssim2 = ssim(real_combined_ssim_3,real_combined_ssim_4)\n","print(ris_ssim2)"]},{"cell_type":"markdown","metadata":{"id":"shGoDK-cd70n"},"source":["####PLOT LPIPS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6bWxL3gd_F-","executionInfo":{"status":"ok","timestamp":1656876648741,"user_tz":-120,"elapsed":189819,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f2064045-4f59-4d28-9ee7-48388d63f568"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GIT-LPIP/lpips/weights/v0.1/alex.pth\n","Avg: 0.64460 +/- 0.01201\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GIT-LPIP/lpips/weights/v0.1/alex.pth\n","Avg: 0.71581 +/- 0.01113\n"]}],"source":["'''\n","nel blocco di definizione delle metriche ci dovrebbe essere anche questo caso \n","ris_lpips = lpips(real_combined2, gen_combined2)\n","print(ris_lpips)\n","'''\n","\n","\n","\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GIT-LPIP')\n","!pip install -r requirements.txt --quiet\n","\n","!python lpips_2dirs.py -d0 /content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/ -d1 /content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0 -o /content/drive/MyDrive/CALCIO_CROP_BASE/LPIPS_RESULTS.txt      #QUA\n","\n","!python lpips_2dirs.py -d0 /content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/ -d1 /content/drive/MyDrive/CALCIO_CROP_BASE/Data_Aug_GAN/images_9 -o /content/drive/MyDrive/CALCIO_CROP_BASE/LPIPS_RESULTS.txt       #QUA"]},{"cell_type":"markdown","metadata":{"id":"ohUoctJUcmoa"},"source":["#### PLOT MMD METRIC - da sistemare il calcolo test post-allenamento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6NBlq-l0IT6","executionInfo":{"status":"ok","timestamp":1656876290550,"user_tz":-120,"elapsed":160,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2380c772-7370-4322-a27d-175a077867b4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nMMD_history = gan_history['MMD']\\nEPOCH = gan_history['EPOCH']\\n\\n\\n#plt.plot(EPOCH, ndb_scores, 'b', label='NDB_SCORES ')\\nplt.plot(EPOCH, MMD_history, 'g', label='MMD')\\nplt.title('Maximum Mean Discrepancy')\\nplt.legend()\\nplt.savefig(os.path.join(path_progettoDL+'weights/GANLOSS_2.pdf')) \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":297}],"source":["'''\n","MMD_history = gan_history['MMD']\n","EPOCH = gan_history['EPOCH']\n","\n","\n","#plt.plot(EPOCH, ndb_scores, 'b', label='NDB_SCORES ')\n","plt.plot(EPOCH, MMD_history, 'g', label='MMD')\n","plt.title('Maximum Mean Discrepancy')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/GANLOSS_2.pdf')) \n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MI91GBplnjVQ","executionInfo":{"status":"ok","timestamp":1656876290553,"user_tz":-120,"elapsed":157,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"037b9fe9-3c9f-4ffe-a355-4e2e685a8d50"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#----------------------------------------------------------------------------------Qui non ho avuto tempo di rivedere ....\\n\\n\\n\\n\\n#g1 = generator(z)\\n\\n#g2 =  g1.reshape(g1.shape[0], g1.shape[1]*g1.shape[2]*g1.shape[3])\\n\\n\\n#immagini reali\\nfor i2, (data2, targets3, targets4) in tqdm(enumerate(trainloader)):\\n        data2 = data2.to(device=device)\\n        \\n        real1 = Variable(data2.type(Tensor))\\n\\nreal2 =  real1.reshape(real1.shape[0], real1.shape[1]*real1.shape[2]*real1.shape[3])\\n\\n\\n#immagini generate - serve fare un csv delle sole immagini generate così da caricare solo quelle e darlo al genloader\\nfor i2, (data2, targets3, targets4) in tqdm(enumerate(genloader)):\\n        data2 = data2.to(device=device)\\n        \\n        real1 = Variable(data2.type(Tensor))\\n\\ngenerated2 =  real1.reshape(real1.shape[0], real1.shape[1]*real1.shape[2]*real1.shape[3])\\n\\n\\nris = MMD(real2, generated2, KERNEL_TYPE, device)  #COPIA DI SOPRA, CHE NON VA COMUNQUE!!!\\n\\nprint(\"\")\\nprint(\"MMD - post training\")\\nprint(ris)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":298}],"source":["\n","'''\n","#----------------------------------------------------------------------------------Qui non ho avuto tempo di rivedere ....\n","\n","\n","\n","\n","#g1 = generator(z)\n","\n","#g2 =  g1.reshape(g1.shape[0], g1.shape[1]*g1.shape[2]*g1.shape[3])\n","\n","\n","#immagini reali\n","for i2, (data2, targets3, targets4) in tqdm(enumerate(trainloader)):\n","        data2 = data2.to(device=device)\n","        \n","        real1 = Variable(data2.type(Tensor))\n","\n","real2 =  real1.reshape(real1.shape[0], real1.shape[1]*real1.shape[2]*real1.shape[3])\n","\n","\n","#immagini generate - serve fare un csv delle sole immagini generate così da caricare solo quelle e darlo al genloader\n","for i2, (data2, targets3, targets4) in tqdm(enumerate(genloader)):\n","        data2 = data2.to(device=device)\n","        \n","        real1 = Variable(data2.type(Tensor))\n","\n","generated2 =  real1.reshape(real1.shape[0], real1.shape[1]*real1.shape[2]*real1.shape[3])\n","\n","\n","ris = MMD(real2, generated2, KERNEL_TYPE, device)  #COPIA DI SOPRA, CHE NON VA COMUNQUE!!!\n","\n","print(\"\")\n","print(\"MMD - post training\")\n","print(ris)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"k-82Jf4Ndqdd"},"source":["####PLOT GS METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-0HBUn8ds5Y"},"outputs":[],"source":["'''\n","ris = gs.geom_score(real_combined, gen_combined)\n","\n","rlts = gs.rlts(real_combined, gamma=1.0/128, n=100)\n","mrlt = np.mean(rlts, axis=0)\n","\n","gs.fancy_plot(mrlt, label='MRLT of 1')\n","plt.xlim([0, 30])\n","plt.legend()\n","'''"]},{"cell_type":"markdown","metadata":{"id":"NdslbUPRlioe"},"source":["####PLOT KL-DIV METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2Qp-VhohS3j"},"outputs":[],"source":["m = nn.Softmax(dim=3)\n","output = m(real_combined2)\n","\n","print(output)\n","print(output.shape)\n","\n","print(output[0][0][0][:].sum())\n","ris = torch.sum(output[0][0][0][:])\n","print(ris)\n","\n","output2 = m(gen_combined2)\n","print(output2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ST65CEpfll_U"},"outputs":[],"source":["'''\n","P = torch.Tensor([0.36, 0.48, 0.16])\n","Q = torch.Tensor([0.333, 0.333, 0.333])\n","\n","(P * (P / Q).log()).sum()\n","# tensor(0.0863), 10.2 µs ± 508\n","\n","ris1 = F.kl_div(Q.log(), P, None, None, 'sum')\n","'''\n","'''\n","real_combined3 = torch.Tensor(real_combined2)\n","gen_combined3 = torch.Tensor(gen_combined2)\n","\n","#(real_combined3 * (real_combined3 / gen_combined3.log()).sum()\n","'''\n","\n","ris1 = F.kl_div(output2.log(), output, None, None, 'sum')\n","\n","# tensor(0.0863), 14.1 µs ± 408 ns\n","print(ris1)\n","\n","\n","\n","print('controlla bene la metrica, è stata implementata con diverse alternative sopra')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBFGY72Xq1RP"},"outputs":[],"source":["'''\n","# calculate (P || Q)\n","kl_pq = rel_entr(real_combined3, gen_combined3)\n","print(kl_pq.shape)\n","print('KL(P || Q): %.3f nats' % sum(kl_pq))\n","# calculate (Q || P)\n","kl_qp = rel_entr(gen_combined3, real_combined3)\n","print('KL(Q || P): %.3f nats' % sum(kl_qp))\n","'''\n","\n","\n","from math import log2\n","# calculate the kl divergence\n","def kl_divergence(p, q):\n","\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n","\n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n"," \n","print(kl_divergence(real_combined3,gen_combined3))\n","\n","print(kl_divergence(p,q))"]},{"cell_type":"markdown","metadata":{"id":"UoMC9ZJLkLm7"},"source":["## TRAIN VGG - CLASSIFICATORE \n","\n","Versione di addestramento rete in Pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMXAiMAvyom5"},"outputs":[],"source":["from sklearn.preprocessing import LabelBinarizer\n","import time\n","\n","#per progress bar\n","from tqdm import tqdm\n","\n","#LINK UTILE : https://stackoverflow.com/questions/59584457/pytorch-is-there-a-definitive-training-loop-similar-to-keras-fit\n","'''\n","LINK UTILE PER CREARE UN OGGETO DI CLASSE Trainer, CHE OTTIMIZZA IL PROCESSO DI TRAINING INCORPORANDO ANCHE LE CALLBACKS\n","https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer\n","'''\n","\n","#per ignorare i vari warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss() #sarebbere la nostra loss function \n","optimizer = optimizer_ #opt definito all'interno del blocco Hyper-Parameters \n","\n","\n","history = {} # Collects per-epoch loss and acc like Keras' fit().\n","history['loss'] = []\n","history['val_loss'] = []\n","history['acc'] = []\n","history['val_acc'] = []\n","history['bal_acc'] = []\n","history['val_bal_acc'] = []\n","\n","start_time_sec = time.time()\n","losses = [] \n","\n","early_stopping = _EarlyStopping(patience=10, verbose=True, path = path_drive+'ProgettoDL/pytorch_model')\n","\n","print('train() called: model={}, opt={}, epochs={}, device={}\\n'.\n","      format(type(model).__name__,type(optimizer).__name__, epochs, device))\n","\n","for epoch in range(0, epochs):\n","    print('-----------------------------------------------------------------')\n","    print('Inizio Epoch : {}'.format(epoch+1))\n","    #alleno il modello \n","    model.train()\n","    train_loss         = 0.0\n","    num_train_correct  = 0\n","    num_train_examples = 0\n","    \n","    for batch_idx, (data, targets, targets2) in tqdm(enumerate(trainloader), desc = 'Epoch : {} train batch'.format(epoch+1)):\n","      data = data.to(device=device)\n","      targets = targets.to(device = device) #classes\n","      targets2 = targets2.to(device = device) #series\n","\n","      # Clear the gradients\n","      optimizer.zero_grad()\n","      # Forward Pass\n","      scores = model(data)\n","      # Find the Loss\n","      loss = criterion(scores,targets)\n","      # Calculate gradients \n","      loss.backward()\n","      # Update Weights\n","      optimizer.step()\n","      # Calculate Loss\n","      train_loss += loss.item()  * data.size(0)\n","      num_train_correct  += (torch.max(scores, 1)[1] == targets).sum().item()\n","      num_train_examples += data.shape[0]   \n","   \n","\n","    #print('num_train_correct {}'.format(num_train_correct))\n","    #print('num_train_examples {}'.format(num_train_examples))\n","\n","    train_acc   = num_train_correct / num_train_examples\n","    train_loss  = train_loss / len(trainloader.dataset.dataframe)\n","    #train_loss  = train_loss / sampler_.number_of_samples\n","\n","    bal_acc = _bal_acc_(targets,scores)\n","\n","    model.eval()\n","    val_loss       = 0.0\n","    num_val_correct  = 0\n","    num_val_examples = 0\n","\n","    for batch_idx, (data, targets, targets2) in tqdm(enumerate(valloader), desc = 'Epoch : {} val batch'.format(epoch+1)):\n","      data = data.to(device=device)\n","      targets = targets.to(device = device) #classes\n","      targets2 = targets2.to(device = device) #series\n","\n","      scores = model(data)\n","      loss = criterion(scores,targets)\n","\n","      val_loss         += loss.data.item()  * data.size(0)\n","      num_val_correct  += (torch.max(scores, 1)[1] == targets).sum().item()\n","      num_val_examples += scores.shape[0]\n","    \n","    #print('num_val_correct {}'.format(num_val_correct))\n","    #print('num_val_examples {}'.format(num_val_examples))\n","    val_acc  = num_val_correct / num_val_examples\n","    val_loss = val_loss / len(valloader.dataset.dataframe)\n","    val_bal_acc = _bal_acc_(targets,scores)\n","    \n","\n","    history['loss'].append(train_loss)\n","    history['val_loss'].append(val_loss)\n","    history['acc'].append(train_acc)\n","    history['val_acc'].append(val_acc)\n","    history['bal_acc'].append(bal_acc)\n","    history['val_bal_acc'].append(val_bal_acc)\n","\n","\n","    print('Epoch : {} di {}, train loss : {} , train acc : {}, val loss : {}, val acc : {}, bal acc : {} , val bal acc : {} ' \n","          . format(epoch+1,epochs, train_loss, train_acc, val_loss, val_acc, bal_acc, val_bal_acc))\n","   \n","    \n","    #early_stopping(val_loss, model)\n","    early_stopping(val_bal_acc, model)\n","        \n","    if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n","\n","    if epoch%10 == 0 and epoch>5:\n","      #salvataggio modello pesi finali\n","      path = path_progettoDL+'pytorch_model_GAN_3_{}'.format(epoch)\n","      torch.save(model.state_dict(), os.path.join(path))\n","      print(\"Saved model to disk\")\n","\n","\n","end_time_sec = time.time()\n","total_time_sec = end_time_sec - start_time_sec\n","time_per_epoch_sec = total_time_sec / epochs\n","print('Time total:     %5.2f sec' % (total_time_sec))\n","print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n"]},{"cell_type":"markdown","metadata":{"id":"xBGTOuc1VSqq"},"source":["## PLOT "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQzh_8drJ56m"},"outputs":[],"source":["'''PLOT CURVES'''\n","import datetime\n","path = path_progettoDL\n","\n","\n","data_ora = datetime.datetime.now()\n","\n","acc = history['acc']\n","val_acc = history['val_acc']\n","loss = history['loss']\n","val_loss = history['val_loss']\n","bal_acc = history['bal_acc']\n","val_bal_acc = history['val_bal_acc']\n","lista = [acc,val_acc,loss,val_loss, bal_acc, val_bal_acc]\n","\n","import csv\n","os.chdir(path_progettoDL+'weights/')\n","with open(\"VGG16.csv\", \"w\", newline=\"\") as f:\n","    writer = csv.writer(f)\n","    writer.writerows(lista)\n","     \n","len_epochs = range(len(acc))\n","\n","plt.plot(len_epochs, acc, 'b', label='Training acc')\n","plt.plot(len_epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/PlotAcc_{}_{}.pdf'.format(type_img,data_ora))) \n","\n","plt.figure()\n"," \n","plt.plot(len_epochs, loss, 'b', label='Training loss')\n","plt.plot(len_epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/PlotLoss_{}_{}.pdf'.format(type_img,data_ora)))\n","\n","#plt.figure(figsize=(20, 6), dpi=80)\n","plt.figure()\n","\n","plt.plot(len_epochs, bal_acc, 'b', label='Training Balance Accuracy')\n","plt.plot(len_epochs, val_bal_acc, 'r', label='Validation Balance Accuracy')\n","plt.title('Training and validation balance accuracy (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/PlotBalAcc_{}_{}.pdf'.format(type_img,data_ora)))"]},{"cell_type":"markdown","metadata":{"id":"QjI6Y_nefFTj"},"source":["## SAVE MODEL \n","\n","https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApFaEaa6oPu0"},"outputs":[],"source":["#salvataggio modello pesi finali\n","path = path_progettoDL+'pytorch_model_GAN_IMB'\n","torch.save(model.state_dict(), os.path.join(path))\n","print(\"Saved model to disk\")"]},{"cell_type":"markdown","metadata":{"id":"XefnmV_tsbOw"},"source":["## LOAD MODEL "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VDiBcm-XEOb"},"outputs":[],"source":["path = path_progettoDL+'pytorch_model_GAN_IMB'\n","model.load_state_dict(torch.load(path))\n","model.eval()\n","print('Model IMG Loaded')"]},{"cell_type":"markdown","metadata":{"id":"HQMtb_JBD5Vp"},"source":["## PREDICTION "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4H0bpu6kjiui"},"outputs":[],"source":["num_test_correct  = 0\n","num_test_examples = 0\n","#da rimuovere dopo \n","#criterion = nn.CrossEntropyLoss()\n","\n","scores_, targets_ = list(), list()\n","\n","for (data, targets, targets2) in (testloader):\n","  data = data.to(device=device)\n","  targets = targets.to(device = device) #classes\n","  targets2 = targets2.to(device = device) #series\n","\n","  scores = model(data)\n","  \n","  num_test_correct  += (torch.max(scores, 1)[1] == targets).sum().item()\n","  num_test_examples += scores.shape[0]\n","\n","  #scores_.append(torch.max(scores, 1)[1])\n","  scores_ = np.append(scores_, torch.max(scores, 1)[1].detach().numpy())\n","  targets_ = np.append(targets_, targets.detach().numpy())\n","\n","#print('num_val_correct {}'.format(num_val_correct))\n","#print('num_val_examples {}'.format(num_val_examples))\n","\n","test_acc  = num_test_correct / num_test_examples\n","test_bal_acc = balanced_accuracy_score(targets_, scores_)\n","\n","print('Accuracy : {:.4f}, balance accuracy : {:.4f}'. format(test_acc, test_bal_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srwp3LLlMqXW"},"outputs":[],"source":["# qui ho cercato di capire quanti scores_ e targets_ sono uguali tra loro\n","K=0\n","K += (scores_ == targets_).sum().item()\n","print(f'predizione corrette : {K}')"]},{"cell_type":"markdown","metadata":{"id":"VkaCQp4Ndzbe"},"source":["##SEARCH UNIVOQUE SERIES TO BALANCE SETS (DA CONTROLLARE SERVE PER LE CM) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkviN4oh2M6l"},"outputs":[],"source":["#SEARCHING UNIVOQUE SERIES\n","test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4, test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9, test_array_s10, test_array_s11, test_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n","pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4, pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9, pred_array_s10, pred_array_s11, pred_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n","i=0\n","for index, row in test_balance_df.iterrows():\n","    \n","    series_ = int(row['series'])\n","    if series_ == 0:\n","      test_array_s0.append(targets_[i])\n","      pred_array_s0.append(scores_[i])\n","    if series_ == 1:\n","      test_array_s1.append(targets_[i])\n","      pred_array_s1.append(scores_[i])\n","    if series_ == 2:\n","      test_array_s2.append(targets_[i])\n","      pred_array_s2.append(scores_[i])\n","    if series_ == 3:\n","      test_array_s3.append(targets_[i])\n","      pred_array_s3.append(scores_[i])\n","    if series_ == 4:\n","      test_array_s4.append(targets_[i])\n","      pred_array_s4.append(scores_[i])\n","    if series_ == 5:\n","      test_array_s5.append(targets_[i])\n","      pred_array_s5.append(scores_[i])\n","    if series_ == 6:\n","      test_array_s6.append(targets_[i])\n","      pred_array_s6.append(scores_[i])\n","    if series_ == 7:\n","      test_array_s7.append(targets_[i])\n","      pred_array_s7.append(scores_[i])\n","    if series_ == 8:\n","      test_array_s8.append(targets_[i])\n","      pred_array_s8.append(scores_[i])\n","    if series_ == 9:\n","      test_array_s9.append(targets_[i])\n","      pred_array_s9.append(scores_[i])\n","    if series_ == 10:\n","      test_array_s10.append(targets_[i])\n","      pred_array_s10.append(scores_[i])\n","    if series_ == 11:\n","      test_array_s11.append(targets_[i])\n","      pred_array_s11.append(scores_[i])\n","    if series_ == 12:\n","      test_array_s12.append(targets_[i])\n","      pred_array_s12.append(scores_[i])\n","\n","    i=i+1\n","\n","print(test_array_s0)\n","print(pred_array_s0)\n","\n","from functools import reduce\n","reduced = reduce(np.union1d, (pred_array_s0, test_array_s0))\n","print(reduced)"]},{"cell_type":"markdown","metadata":{"id":"D8WKlbpqKgAP"},"source":["## METRICHE MASK & IMG "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHSjdjJcgc1Y"},"outputs":[],"source":["'''METRICHE'''\n","print('--------------Metrice IMG----------------')\n","#print(y_test)\n","#print(y_pred)\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html                                  \n","print(\"test accuracy  : {:.4f}\".format(accuracy_score(targets_, scores_) ))\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html?highlight=precision_score#sklearn.metrics.precision_score\n","print(\"precision  : {:.4f}\".format(precision_score(targets_, scores_, average=\"macro\")))\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall_score#sklearn.metrics.recall_score         \n","print(\"recall : {:.4f}\".format(recall_score(targets_, scores_ , average=\"macro\")))\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score             \n","print(\"f1_score : {:.4f}\".format(f1_score(targets_, scores_, average=\"macro\")))        \n","print('classification report')\n","print(classification_report(targets_, scores_))  \n"]},{"cell_type":"markdown","metadata":{"id":"tJjt39sGAfQF"},"source":["Per quanto riguarda la funzione np_quadratic_weighted_kappa abbiamo avuto alcune difficoltà implementative e quindi abbiamo cercato un codice online che ci calcolasse la stessa metrica \n","\n","[Link Utilizzato](https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps)"]},{"cell_type":"markdown","metadata":{"id":"MHdYyyAffspk"},"source":["## METRICHE SECONDARIE QWK, MS, MAE \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in2Vpij3AHgB"},"outputs":[],"source":["\n","# The following 3 functions have been taken from Ben Hamner's github repository\n","# https://github.com/benhamner/Metrics\n","def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n","    \"\"\"\n","    Returns the confusion matrix between rater's ratings\n","    \"\"\"\n","    assert(len(rater_a) == len(rater_b))\n","    if min_rating is None:\n","        min_rating = min(rater_a + rater_b)\n","    if max_rating is None:\n","        max_rating = max(rater_a + rater_b)\n","    num_ratings = int(max_rating - min_rating + 1)\n","    conf_mat = [[0 for i in range(num_ratings)]\n","                for j in range(num_ratings)]\n","    for a, b in zip(rater_a, rater_b):\n","        conf_mat[a - min_rating][b - min_rating] += 1\n","    return conf_mat\n","\n","\n","def histogram(ratings, min_rating=None, max_rating=None):\n","    \"\"\"\n","    Returns the counts of each type of rating that a rater made\n","    \"\"\"\n","    if min_rating is None:\n","        min_rating = min(ratings)\n","    if max_rating is None:\n","        max_rating = max(ratings)\n","    num_ratings = int(max_rating - min_rating + 1)\n","    hist_ratings = [0 for x in range(num_ratings)]\n","    for r in ratings:\n","        hist_ratings[r - min_rating] += 1\n","    return hist_ratings\n","\n","def quadratic_weighted_kappa(y, y_pred):\n","    \"\"\"\n","    Calculates the quadratic weighted kappa\n","    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n","    value, which is a measure of inter-rater agreement between two raters\n","    that provide discrete numeric ratings.  Potential values range from -1\n","    (representing complete disagreement) to 1 (representing complete\n","    agreement).  A kappa value of 0 is expected if all agreement is due to\n","    chance.\n","    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n","    each correspond to a list of integer ratings.  These lists must have the\n","    same length.\n","    The ratings should be integers, and it is assumed that they contain\n","    the complete range of possible ratings.\n","    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n","    is the minimum possible rating, and max_rating is the maximum possible\n","    rating\n","    \"\"\"\n","    rater_a = y\n","    rater_b = y_pred\n","    min_rating=1 # era None abbiamo messo 0\n","    max_rating=9 # era None abbiamo messo 9\n","    rater_a = np.array(rater_a, dtype=int)\n","    rater_b = np.array(rater_b, dtype=int)\n","    assert(len(rater_a) == len(rater_b))\n","    if min_rating is None:\n","        min_rating = min(min(rater_a), min(rater_b))\n","    if max_rating is None:\n","        max_rating = max(max(rater_a), max(rater_b))\n","    conf_mat = Cmatrix(rater_a, rater_b,\n","                                min_rating, max_rating)\n","    num_ratings = len(conf_mat)\n","    num_scored_items = float(len(rater_a))\n","\n","    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n","    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n","\n","    numerator = 0.0\n","    denominator = 0.0\n","\n","    for i in range(num_ratings):\n","        for j in range(num_ratings):\n","            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n","                              / num_scored_items)\n","            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n","            numerator += d * conf_mat[i][j] / num_scored_items\n","            denominator += d * expected_count / num_scored_items\n","\n","    return (1.0 - numerator / denominator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jydD-lsObD4e"},"outputs":[],"source":["path_drive = '/content/drive/My Drive/'\n","path = path_drive+'ProgettoDL/'\n","\n","os.chdir(path)\n","\n","from metrics import np_quadratic_weighted_kappa, minimum_sensitivity\n","from sklearn.metrics import mean_absolute_error\n","\n","#Alternativa al MS di metrics \n","from imblearn.metrics import sensitivity_score\n","\n","def compute_metrics(y_true, y_pred, num_classes):\n","  # Calculate metric\n","  qwk = quadratic_weighted_kappa(y_true, y_pred)\n","  mae = mean_absolute_error(y_true, y_pred)\n","  #ms = minimum_sensitivity(y_true, y_pred) #---> DA RIVEDERE PERCHE' NON C'é PIU' y_pred_no_argmax\n","  \n","  ms = sensitivity_score(y_true, y_pred, average='macro')\n","\n","\n","  metrics = {\n","\t\t'QWK': qwk,\n","\t\t'MS': ms,\n","\t\t'MAE': mae\n","  }\n","  \n","  return metrics\n","\n","def print_metrics(metrics):\n","\tprint('QWK: {:.4f}'.format(metrics['QWK']))\n","\tprint('MS: {:.4f}'.format(metrics['MS']))\n","\tprint('MAE: {:.4f}'.format(metrics['MAE']))    \n","\n","\n","#-----codice------\n","\n","num_classi = 10\n","\n","print('Metrics')\n","metrics = compute_metrics(targets_, scores_,num_classi)\n","print_metrics(metrics)\n","\n","\n","with open(\"metrics.txt\", \"w\") as text_file:\n","    print(print_metrics, file=text_file)\n"]},{"cell_type":"markdown","metadata":{"id":"Tnlbf22bBpIt"},"source":["***Metrice Ottenute***\n","\n","**K Cohen**   https://it.vvikipedla.com/wiki/Cohen%27s_kappa\n","Il Kappa di Cohen è un coefficiente statistico che rappresenta il grado di accuratezza e affidabilità in una classificazione statistica; è un indice di concordanza che tiene conto della probabilità di concordanza casuale; l'indice calcolato in base al rapporto tra l'accordo in eccesso rispetto alla probabilità di concordanza casuale e l'eccesso massimo ottenibile. Attraverso la matrice di confusione è possibile valutare questo parametro. In particolare ... Esistono diversi \"gradi di concordanza\", in base ai quali possiamo definire se Kappa di Cohen è scarso o ottimo:\n","\n","- se k assume valori inferiori a 0, allora non c'è concordanza;\n","- se k assume valori compresi tra 0-0,4, allora la concordanza è scarsa;\n","- se k assume valori compresi tra 0,4-0,6, allora la concordanza è discreta;\n","- se k assume valori compresi tra 0,6-0,8, la concordanza è buona;\n","- se k assume valori compresi tra 0,8-1, la concordanza è ottima.\n","\n","**QWK**: 0.7849\n","\n","BLA BLA BLA \n","\n","**MS**: 1.0000\n","\n","\n","In statistics, **mean absolute error (MAE)** is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. \n","\n","**MAE**: 0.0000"]},{"cell_type":"markdown","metadata":{"id":"8VdVD6ChMIOs"},"source":["## PLOT CONFUSION MATRIX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2OZL1txL-5V"},"outputs":[],"source":["import sklearn.metrics as metrics\n","data_ora = datetime.datetime.now()\n","\n","\n","fig, axs = plt.subplots(1)\n","fig0 = ConfusionMatrixDisplay.from_predictions(y_true=targets_, y_pred=scores_, cmap='Blues', ax = axs)\n","plt.suptitle('Confusion Matrix IMG', y=1.0, fontsize=12)\n","plt.title('Accuracy {:.4f} , Prediction {:.4f}'.format(accuracy_score(targets_, scores_),precision_score(targets_, scores_, average=\"macro\") ), fontsize=10)\n","plt.show()\n","fig.savefig(os.path.join(path+'weights/CM_{}_{}.pdf'.format(type_img,data_ora))) \n"]},{"cell_type":"markdown","metadata":{"id":"yy4ylbat-r51"},"source":["##PLOT CONFUSION MATRIX PER CIASCUNA SERIE DEL CALCIO "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6QFmQV0IBLp"},"outputs":[],"source":["path = path_progettoDL\n","data_ora = datetime.datetime.now()\n","\n","test_array_series_complete = [\n","        test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4,\n","        test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9,\n","        test_array_s10, test_array_s11, test_array_s12             \n","]\n","\n","pred_array_series_complete = [\n","        pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4,\n","        pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9,\n","        pred_array_s10, pred_array_s11, pred_array_s12             \n","]\n","\n","for series in range(13):\n","  fig, axs = plt.subplots(1)\n","  fig0 = ConfusionMatrixDisplay.from_predictions(y_true=test_array_series_complete[series], y_pred=pred_array_series_complete[series], cmap='Blues', ax = axs)\n","  plt.suptitle('Confusion Matrix Series {}'.format(series), y=1.0, fontsize=12)\n","  plt.title('Accuracy {:.4f} , Prediction {:.4f}'.format(accuracy_score(test_array_series_complete[series], pred_array_series_complete[series]),precision_score(test_array_series_complete[series], pred_array_series_complete[series], average=\"macro\") ), fontsize=10)\n","  plt.show()\n","  fig.savefig(os.path.join(path+'weights/CM_serie{}_{}_{}.pdf'.format(series, type_img,data_ora))) "]},{"cell_type":"markdown","metadata":{"id":"TUnAYcXVclfD"},"source":["## CRAMER V CORRELATION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqvgi3VKck3o"},"outputs":[],"source":["\n","#PRIMA VERSIONE\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as ss\n","import seaborn as sns\n","\n","def cramers_v(confusion_matrix):\n","    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n","        uses correction from Bergsma and Wicher,\n","        Journal of the Korean Statistical Society 42 (2013): 323-328\n","    \"\"\"\n","    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n","    n = confusion_matrix.sum()\n","    phi2 = chi2 / n\n","    r, k = confusion_matrix.shape\n","    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n","    rcorr = r - ((r-1)**2)/(n-1)\n","    kcorr = k - ((k-1)**2)/(n-1)\n","    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n","\n","confusion_matrix = pd.crosstab(scores_, targets_)\n","print(\"cramer correlation tra predizioni delle classi, e le classi effettive\")\n","cramer1 = cramers_v(confusion_matrix.values)\n","print('CRAMER : {:.5f} '.format(cramer1))\n","\n","\n","'''\n","#------ prove denis ----------\n","test_array_series = np.array(test_balance_df['series']) \n","y_test_series = test_array_series #custom_to_categorical(np.unique(test_array_series, return_inverse=True)[1], num_classes=13)  \n","#print(y_test_series)\n","#------ fine prove denis ----------\n","\n","confusion_matrix2 = pd.crosstab(y_test_series, scores_)\n","print(\"cramer correlation tra predizioni delle classi e le ground thruth di shotgun series\")\n","cramer2 = cramers_v(confusion_matrix2.values)\n","print(cramer2)\n","'''\n","print('-------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sZfKVKTmEdX"},"outputs":[],"source":["print('Seconda versione Cramer')\n","'''\n","#SECONDA VERSIONE.        https://www.youtube.com/watch?v=eTnLTJer_Oo\n","contTable = pd.crosstab(y_test_series, scores_)\n","print(contTable)\n","\n","!pip install researchpy\n","\n","import researchpy\n","\n","crosstab, res = researchpy.crosstab(pd.Series(y_test_series), pd.Series(scores_), test='chi-square')\n","print(\"\\n{}\".format(res))\n","\n","df = min(contTable.shape[0], contTable.shape[1]) - 1\n","print(\"\\ndf = {}\".format(df))\n","\n","V = res.iloc[2,1]\n","print(\"V = {}\".format(V))\n","\n","if df == 1:\n","    if V < 0.10:\n","        qual = 'negligible'\n","    elif V < 0.30:\n","        qual = 'small'\n","    elif V < 0.50:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 2:\n","    if V < 0.07:\n","        qual = 'negligible'\n","    elif V < 0.21:\n","        qual = 'small'\n","    elif V < 0.35:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 3:\n","    if V < 0.06:\n","        qual = 'negligible'\n","    elif V < 0.17:\n","        qual = 'small'\n","    elif V < 0.29:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 4:\n","    if V < 0.05:\n","        qual = 'negligible'\n","    elif V < 0.15:\n","        qual = 'small'\n","    elif V < 0.25:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","else:\n","    if V < 0.05:\n","        qual = 'negligible'\n","    elif V < 0.13:\n","        qual = 'small'\n","    elif V < 0.22:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","\n","print(\"\\nquality classification of the correlation is:   {}\".format(qual))\n","'''\n","print('------')"]},{"cell_type":"markdown","metadata":{"id":"UHab1JLzqkKx"},"source":["To indicate the strength of the association between two nominal variables, Cramér's V (Cramér, 1946) is often used.\n","\n","As for the interpretation for Cramér's V various rules of thumb exist but one of them is from Cohen (1988, pp. 222, 224, 225) who let's the interpretation depend on the degrees of freedom, shown in the table below.\n","\n","|df*|negligible|small|medium|large|\n","|-------|---|---|---|---|\n","|1|0 < .10|.10 < .30|.30 < .50|.50 or more|\n","|2|0 < .07|.07 < .21|.21 < .35|.35 or more|\n","|3|0 < .06|.06 < .17|.17 < .29|.29 or more|\n","|4|0 < .05|.05 < .15|.15 < .25|.25 or more|\n","|5|0 < .05|.05 < .13|.13 < .22|.22 or more|\n","\n","The degrees of freedom (df*) is for Cramér's V the minimum of the number of rows, or number of columns, then minus one.\n","\n","Lets see how to obtain Cramér's V with Python, using an example.\n","\n","\n","\n","\n","**A SECONDA DEI RISULTATI E CONFRONTANDOLI CON LA TABELLA RIUSCIAMO A CAPIRE L'INTENSITA' DEL BIAS TRA DIVERSE VARIABILI**"]},{"cell_type":"markdown","metadata":{"id":"NoegV_027yUO"},"source":["## **T-SNE  & PCA**\n"]},{"cell_type":"markdown","metadata":{"id":"X7-u3Qf0YsFe"},"source":["### Spiegazioni, Link Utili e Implementazione "]},{"cell_type":"markdown","metadata":{"id":"c964uCqMUpS4"},"source":["***(t-SNE)*** t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, you may have to plot fewer exploratory data analysis plots next time you work with high dimensional data.\n","\n","[Link utile ](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n","\n","***(PCA) Principal Component Analysis***\n","Lʹanalisi delle componenti principali (detta pure PCA oppure CPA) è una tecnica utilizzata nell’ambito della statistica multivariata per la semplificazione dei dati d’origine.\n","Lo scopo primario di questa tecnica è la riduzione di un numero più o meno elevato di variabili (rappresentanti altrettante caratteristiche del fenomeno analizzato) in alcune variabili latenti. Ciò avviene tramite una trasformazione lineare delle variabili che proietta quelle originarie in un nuovo sistema cartesiano nel quale le variabili vengono ordinate in ordine decrescente di varianza: pertanto, la variabile con maggiore varianza viene proiettata sul primo asse, la seconda sul secondo asse e così via. La riduzione della complessità avviene limitandosi ad analizzare le principali (per varianza) tra le nuove variabili.\n","Diversamente da altre trasformazioni (lineari) di variabili praticate nellʹambito della statistica, in questa tecnica sono gli stessi dati che determinano i vettori di trasformazione.\n","[Step By Step](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n","\n","[Link Utile](https://www.analyticsvidhya.com/blog/2020/12/an-end-to-end-comprehensive-guide-for-pca/) "]},{"cell_type":"markdown","metadata":{"id":"1o8fXQZePP25"},"source":["***Parametri del TSNE***\n","1. **n_components** int, default=2 - Dimension of the embedded space.\n","\n","2. **perplexityfloat, default=30.0** - The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results.\n","\n","3. **early_exaggeration float, default=12.0**\n","Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.\n","\n","4. **learning_ratefloat, default=200.0** The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.\n","\n","5. **n_iterint, default=1000**\n","Maximum number of iterations for the optimization. Should be at least 250.\n","\n","6. **n_iter_without_progressint, default=300**\n","Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.\n","\n","7. **metricstr or callable, default=’euclidean’**\n","The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is “euclidean” which is interpreted as squared euclidean distance.\n","\n","8. **init{‘random’, ‘pca’} or ndarray of shape(n_samples, n_components), default=’random’**\n","Initialization of embedding. Possible options are ‘random’, ‘pca’, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.\n","\n","9. **verboseint, default=0** Verbosity level.\n","\n","10. **random_stateint, RandomState instance or None, default=None** Determines the random number generator. Pass an int for reproducible results across multiple function calls. Note that different initializations might result in different local minima of the cost function. See :term: Glossary <random_state>.\n","\n","11. **methodstr, default=’barnes_hut’**\n","By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.\n","\n","12. **n_jobsint, default=None**\n","The number of parallel jobs to run for neighbors search. This parameter has no impact when metric=\"precomputed\" or (metric=\"euclidean\" and method=\"exact\"). None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n","\n","\n","[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n","\n","[misread-tsne](https://distill.pub/2016/misread-tsne/)\n","\n","[altro modo spiegato anche meglio](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n"]},{"cell_type":"markdown","metadata":{"id":"umabolL61nFw"},"source":["#### Dataset "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yp2BSeGZ0vij"},"outputs":[],"source":["'''\n","import numpy as np\n","from keras.models import Sequential\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import os\n","import pandas as pd\n","\n","os.chdir('/content/drive/MyDrive/ProgettoDL')\n","path = os.getcwd()\n","\n","col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n","dataframe_sx_complessivo = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n","\n","\n","col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n","dataframe_dx_complessivo = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n","\n","\n","dataframe_sx_complessivo.columns = ['ID','series', 'filename', 'class']\n","dataframe_dx_complessivo.columns = ['ID','series', 'filename', 'class']\n","\n","#print(dataframe_sx.columns)                 #stampo i due elementi con stesso ID (lato dx e sx di stesso CALCIO)\n","frames = [dataframe_sx_complessivo, dataframe_dx_complessivo]\n","result_complessivo = pd.concat(frames)\n","#print(result_complessivo)\n","#print(result_complessivo.loc[[1]])\n","#print(type(result_complessivo.loc[[1]]))\n","\n","result_complessivo[\"class\"] = result_complessivo[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n","result_complessivo[\"series\"] = result_complessivo[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)})\n","\n","#IDENTIFICAZIONE VALORI NULL \n","print(\"Null VALUE di class : \"+format(result_complessivo['class'].isnull().sum()))\n","print(result_complessivo.loc[result_complessivo['class'] == '0'])\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","result_complessivo['class'] = pd.to_numeric(result_complessivo['class'], errors='coerce')\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","result_complessivo = result_complessivo.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","\n","print(\"Null VALUE di class : \"+format(result_complessivo['class'].isnull().sum()))\n","\n","#IMMG EXIST ?  (cerco se qualche path non esiste e lo elimino dal dataframe) e se esiste ne faccio la MASCHERA\n","import os.path\n","from os import path\n","os.chdir('/content/drive/MyDrive/CALCIO_NOPRE')\n","for index, row in result_complessivo.iterrows():\n","    filename = row['filename']\n","    if(os.path.exists(filename) == False):\n","      result_complessivo = result_complessivo.drop(result_complessivo[(result_complessivo['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","\n","print('------------------- DATASET BASE ---------------')\n","print(type(result_complessivo))  \n","print(len(result_complessivo))\n","print(result_complessivo)\n","\n","result_complessivo_totale = pd.DataFrame()\n","\n","for index, row in result_complessivo.iterrows():\n","  filename_mask = 'mask_{}'.format(row['filename'])\n","  #filename_gray = 'gray_{}'.format(row['filename'])\n","  class_ = row['class']\n","  series_ = row['series']\n","  #print('{}_{}_{}_{}'.format(filename_gray,filename_mask, class_, series_)) \"ID\": row['ID']\n","  row_df_1 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename_mask, \"class\" : class_},index=[0])\n","  #row_df_2 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename_gray, \"class\" : class_},index=[0])\n","  #row_df_3 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename, \"class\" : class_},index=[0])\n","  #print(row_df_1)\n","  #print(row_df_2)\n","  result_complessivo_totale = result_complessivo_totale.append(row_df_1)\n","  #result_complessivo_totale = result_complessivo_totale.append(row_df_2)\n","  #result_complessivo_totale = result_complessivo_totale.append(row_df_3)\n","\n","\n","print('------------------- DATASET COMPLESSIVO ---------------') \n","print(type(result_complessivo_totale))  \n","print(len(result_complessivo_totale))\n","#print(result_complessivo_totale)\n","\n","from sklearn.utils import shuffle\n","result_complessivo_totale = shuffle(result_complessivo_totale)\n","print(type(result_complessivo_totale))  \n","print(len(result_complessivo_totale))\n","print(result_complessivo_totale)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"_U2UEFjs1iXp"},"source":["#### import utili per il TSNE e PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvdYrmbf1aYd"},"outputs":[],"source":["'''\n","%matplotlib inline\n","from __future__ import print_function\n","import time\n","import numpy as np\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import seaborn as sns\n","from sklearn.manifold import TSNE\n","import pandas as pd    \n","from sklearn.preprocessing import StandardScaler\n","'''"]},{"cell_type":"markdown","metadata":{"id":"z56gEZfX14mX"},"source":["#### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini"]},{"cell_type":"markdown","metadata":{"id":"7qiUkpP0_LXM"},"source":["##### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini - QUALITY CLASS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7t1a9DA0wnE"},"outputs":[],"source":["'''\n","# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n","from tqdm import tqdm\n","immg_rows = 270 \n","immg_cols = 470\n","X = [] \n","imgs_array_tot = []\n","\n","data_X = result_complessivo_totale['filename'][:1000] #---versione originale \n","result_complessivo_totale_min = result_complessivo_totale[:1000] #--deve essere uguale a y_dim ---versione originale \n","y = result_complessivo_totale['class'][:1000] #--- deve essere uguale ... ---versione originale \n","\n","for index, row in tqdm(result_complessivo_totale_min.iterrows()):\n","    filename = row['filename']\n","    if(filename[0] == 'm'):\n","      image = load_img('/content/drive/My Drive/MASK_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    elif(filename[0] == 'g'): \n","      image = load_img('/content/drive/My Drive/GRAY_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    else:\n","      image = load_img('/content/drive/My Drive/CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    \n","    #print('Originale : {} x {} x {}'.format(image.size[0], image.size[1], len(image.size)-1))\n","    #plt.imshow(image)\n","    scale_percent = 90 # percent of original size\n","    width, height = image.size\n","    #print('channel : {}'.format(len(image.size)))\n","    width = int(width * scale_percent / 100)\n","    height = int(height * scale_percent / 100)\n","    dim = (width, height)\n","    # resize image\n","    x = img_to_array(image)\n","    resized = cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n","    #print('Ridimensionata : {}'.format((resized.shape)))\n","    #print('Resized Dimensions : ',resized.shape)\n","    imgs_array_tot.append(resized)\n","    X = np.asarray(imgs_array_tot)\n","print(X.shape)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"7IsDgmRB_QxY"},"source":["##### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini - SHOTGUN SERIES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WGaPWme_Ca8"},"outputs":[],"source":["'''\n","# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n","from tqdm import tqdm\n","\n","immg_rows = 270 \n","immg_cols = 470\n","X = [] \n","imgs_array_tot = []\n","data_X = result_complessivo_totale['filename'][:1000]\n","\n","result_complessivo_totale_min = result_complessivo_totale[:1000] #--deve essere uguale a y_dim\n","\n","y = result_complessivo_totale['class'][:1000] #--- deve essere uguale ... \n","\n","y_series = result_complessivo_totale['series'][:1000] #--- deve essere uguale ...\n","\n","for index, row in tqdm(result_complessivo_totale_min.iterrows()):\n","    filename = row['filename']\n","    if(filename[0] == 'm'):\n","      image = load_img('/content/drive/My Drive/MASK_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    elif(filename[0] == 'g'): \n","      image = load_img('/content/drive/My Drive/GRAY_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    else:\n","      image = load_img('/content/drive/My Drive/CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","\n","    scale_percent = 90 # percent of original size\n","    width, height = image.size\n","    width = int(width * scale_percent / 100)\n","    height = int(height * scale_percent / 100)\n","    dim = (width, height)\n","    # resize image\n","    x = img_to_array(image)\n","    resized = cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n","    #print('Ridimensionata : {}'.format((resized.shape)))\n","    #print('Resized Dimensions : ',resized.shape)\n","    imgs_array_tot.append(resized)\n","    X2 = np.asarray(imgs_array_tot)\n","print(X2.shape) \n","'''"]},{"cell_type":"markdown","metadata":{"id":"ihY4u-CL2JKI"},"source":["#### Check & Create Dataframe for PCA (Principal Analysis Component) & T-SNE (t-distributed stochastic neighbor embedding)"]},{"cell_type":"markdown","metadata":{"id":"2OaloopJ5gCK"},"source":["##### classi di qualità "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niWaWydy2H9P"},"outputs":[],"source":["'''\n","print('X SHAPE : {}'.format(X.shape))\n","\n","nsamples = X.shape[0]\n","rows = X.shape[1]\n","cols = X.shape[2]\n","channel = 1\n","\n","print('n_samples : {} , rows : {} , cols : {} , channel : {} '.format(nsamples, rows, cols, channel))\n","print(type(X))\n","X_1 = np.reshape(X, (X.shape[0],rows*cols*channel)) #-- serve per modificare la dimensione, per il fit_transform          FORSE QUI BISOGNA SOLO USARE I PRIMI 2 VALORI E IL 3 DEI CANALI NO!\n","\n","print('X MODIFICATO : {}'.format(X_1.shape)) #--- controllo se ho fatto tutto correttamente \n","\n","feat_cols = [ 'pixel'+str(i) for i in range(X_1.shape[1]) ]\n","print('Feat Cols : {} '.format(len(feat_cols)))\n","#print(feat_cols)\n","df = pd.DataFrame(X_1,columns=feat_cols)\n","#df = pd.DataFrame(X_1)\n","df['y'] = pd.DataFrame({ 'y': np.array(y) })\n","df['label'] = df['y'].apply(lambda i: str(i))\n","#X, y = None, None\n","print('Size of the dataframe: {}'.format(df.shape))\n","\n","# For reproducability of the results\n","np.random.seed(42)\n","rndperm = np.random.permutation(df.shape[0])\n","'''"]},{"cell_type":"markdown","metadata":{"id":"qrtXkNtk5luP"},"source":["##### shotgun series "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nRob1Q0F5buA"},"outputs":[],"source":["'''\n","print('X2 SHAPE : {}'.format(X2.shape))\n","\n","nsamples = X2.shape[0]\n","rows = X2.shape[1]\n","cols = X2.shape[2]\n","channel = 1\n","\n","print('n_samples : {} , rows : {} , cols : {} , channel : {} '.format(nsamples, rows, cols, channel))\n","print(type(X2))\n","X_11 = np.reshape(X2, (X2.shape[0],rows*cols*channel)) #-- serve per modificare la dimensione, per il fit_transform          FORSE QUI BISOGNA SOLO USARE I PRIMI 2 VALORI E IL 3 DEI CANALI NO!\n","\n","print('X MODIFICATO : {}'.format(X_11.shape)) #--- controllo se ho fatto tutto correttamente \n","#print(X_1)\n","\n","feat_cols = [ 'pixel'+str(i) for i in range(X_11.shape[1]) ]\n","print('Feat Cols : {} '.format(len(feat_cols)))\n","#print(feat_cols)\n","df_2 = pd.DataFrame(X_11,columns=feat_cols)\n","#df = pd.DataFrame(X_1)\n","df_2['y'] = pd.DataFrame({ 'y': np.array(y_series) })\n","df_2['label'] = df_2['y'].apply(lambda i: str(i))\n","#X, y = None, None\n","print('Size of the dataframe: {}'.format(df_2.shape))\n","\n","\n","\n","# For reproducability of the results\n","np.random.seed(42)\n","rndperm = np.random.permutation(df_2.shape[0])\n","'''"]},{"cell_type":"markdown","metadata":{"id":"hbq6tUyY2bxr"},"source":["#### Calcolo TSNE & PLOT TSNE"]},{"cell_type":"markdown","metadata":{"id":"PUPCoiGQ6j-D"},"source":["##### TSNE QUALITY CLASS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bg-1zy_g2bDc"},"outputs":[],"source":["'''\n","time_start = time.time()\n","N = 1000 \n","df_subset = df.loc[rndperm[:N],:].copy()\n","data_subset = df_subset[feat_cols].values\n","#data_subset = df_subset\n","#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... originale \n","tsne = TSNE(n_components=2, verbose=1, perplexity=200, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n","tsne_results = tsne.fit_transform(data_subset)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n","'''"]},{"cell_type":"markdown","metadata":{"id":"J9nnpLbE6sfN"},"source":["##### TSNE SHOTGUN SERIES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5Ht1Qoc6rtg"},"outputs":[],"source":["'''\n","time_start = time.time()\n","N = 1000\n","df_subset_series = df_2.loc[rndperm[:N],:].copy()\n","#data_subset_series = df_subset_series\n","data_subset_series = df_subset_series[feat_cols].values\n","#tsne_series = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... \n","tsne_series = TSNE(n_components=2, verbose=1, perplexity=5, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n","tsne_results_series = tsne_series.fit_transform(data_subset_series)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n","'''"]},{"cell_type":"markdown","metadata":{"id":"RdicUGi28Sr_"},"source":["##### PLOT TSNE QUALITY CLASSES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXkTZdoA3Thu"},"outputs":[],"source":["'''\n","df_subset['tsne-2d-one'] = tsne_results[:,0]\n","df_subset['tsne-2d-two'] = tsne_results[:,1]\n","plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n","    hue=\"y\",\n","    palette=sns.color_palette('Paired', as_cmap = True),\n","    data=df_subset,\n","    legend=\"full\",\n","    alpha=0.3\n",")\n","'''"]},{"cell_type":"markdown","metadata":{"id":"1WOsA8la8b6O"},"source":["##### TSNE PLOT SHOTGUN SERIES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOdq6FA08f4b"},"outputs":[],"source":["'''\n","df_subset_series['tsne-2d-one'] = tsne_results_series[:,0]\n","df_subset_series['tsne-2d-two'] = tsne_results_series[:,1]\n","plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n","    hue=\"y\",\n","    palette=sns.color_palette('Paired', as_cmap = True),\n","    data=df_subset_series,\n","    legend=\"full\",\n","    alpha=0.3\n",")\n","'''"]},{"cell_type":"markdown","metadata":{"id":"5aSi_lI_xuvO"},"source":["## **Metriche Nuove** + **B.A. across series**"]},{"cell_type":"markdown","metadata":{"id":"u7H7AQn_u3pW"},"source":["#### Alcune Definizioni \n"]},{"cell_type":"markdown","metadata":{"id":"dxwu_-ZMOLMs"},"source":["*  **True Positives** (TP): Items where the true label is positive and whose class is correctly predicted to be positive.\n","*  **False Positives** (FP): Items where the true label is negative and whose class is incorrectly predicted to be positive\n","*  **True Negatives** (N): Items where the true label is negative and whose class is correctly predicted to be negative.\n","*  **False Negatives** (FN): Items where the true label is positive and whose class is incorrectly predicted to be negative.\n","\n","* **False Positive Rate**, or *Type I Error*: Number of items wrongly identified as positive out of the total actual negatives — FP/(FP+TN) - This error means that an image not containing a particular parasite egg is incorrectly labeled as having it\n","* **False Negative Rate**, or *Type II Error*: Number of items wrongly identified as negative out of the total actual positives — FN/(FN+TP). This metric is especially important to us, as it tells us the frequency with which a particular parasite egg is not classified correctly\n","\n","-------------\n","\n","* **Statistical Parity Difference**\n","This measure is based on the following formula :\n","𝑃𝑟(𝑌=1|𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑)−𝑃𝑟(𝑌=1|𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑) Here the bias or statistical imparity is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1. So it has to be close to 0 so it will be fair.\n","\n","*  **Equal Opportunity Difference** This metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula - 𝑇𝑃𝑅𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑−𝑇𝑃𝑅𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑 Same as the previous metric we need it to be close to 0.\n","\n","* **demographic parity** A fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n","\n","* **equality of opportunity** A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership."]},{"cell_type":"markdown","metadata":{"id":"XRI4vgyNd-1T"},"source":["#### Implementazione Metriche Nuove e B.A. across series"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7dJJO7ixuJ7"},"outputs":[],"source":["!pip install fairlearn \n","from fairlearn.metrics import selection_rate\n","from fairlearn.metrics import true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate\n","from fairlearn.metrics import equalized_odds_difference\n","\n","import sklearn as sk\n","\n","\n","#---- metriche lisa ----#\n","#_true = test_balance_df['class'].to_numpy()\n","#VERIFICA SE SERVE RIFARE STA RIGA SOPRA, MA BASTA PRENDERE:\n","y_true = targets_\n","y_pred = scores_\n","\n","\n","SR = selection_rate(y_true, y_pred, pos_label=1, sample_weight=None)\n","print('selection_rate : {}' . format(SR))\n","\n","\n","#Per quanto riguarda AO come metrica, potremo utilizzare i risultati della confusion matrix ?\n","#LINK : https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n","#LINK : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n","#print('Unique Element Y_test : {}'.format(np.unique(y_test)))\n","#print('Unique Element Y_pred : {}'.format(np.unique(y_pred)))\n","#print('True_Positive_Rate : {}'.format(true_positive_rate(y_true, y_pred)))\n","\n","from sklearn.metrics import confusion_matrix \n","cm = confusion_matrix (y_true, y_pred)\n","FP = cm.sum(axis=0) - np.diag(cm)  \n","FN = cm.sum(axis=1) - np.diag(cm)\n","TP = np.diag(cm)\n","TN = cm.sum() - (FP + FN + TP)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","print('TPR : {}'.format(TPR))\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","print('TNR : {}'.format(TNR))\n","# Precision or positive predictive value\n","PPV = TP/(TP+FP)\n","print('PPV : {}'.format(PPV))\n","# Negative predictive value\n","NPV = TN/(TN+FN)\n","print('NPV : {}'.format(NPV))\n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","print('FPR : {}'.format(FPR))\n","# False negative rate\n","FNR = FN/(TP+FN)\n","print('FNR : {}'.format(FNR))\n","# False discovery rate\n","FDR = FP/(TP+FP)\n","print('FDR : {}'.format(FDR))\n","\n","# Overall accuracy\n","ACC = (TP+TN)/(TP+FP+FN+TN)\n","print('Accuracy : {}'.format(ACC))\n","\n","\n","AO = 0.5*(\n","    (TPR[0] + FPR[0]) - \n","    (TPR[1] + FPR[1]) + \n","    (TPR[2] + FPR[2]) - \n","    (TPR[3] + FPR[3]) +\n","    (TPR[4] + FPR[4]) -\n","    (TPR[5] + FPR[5]) +\n","    (TPR[6] + FPR[6]) -\n","    (TPR[7] + FPR[7]) +\n","    (TPR[8] + FPR[8]) -\n","    (TPR[9] + FPR[9]))\n","\n","print('AO : {}'.format(AO))\n","#y_true= y_true.reshape(1,-1)\n","#y_pred= y_pred.reshape(-1,1)\n","#print(y_true.shape)\n","#print(y_pred.shape)\n","\n","\n","'''FORSE QUA RIUSCIAMO A TROVARE UN ESEMPIO DI APPLICAZIONE DEL METODO'''\n","'''https://deepnote.com/@Machine-Learning-2/Miniproject-z523fGqWSSu7QV34n_u7OA'''\n","'''https://fairlearn.org/main/user_guide/assessment.html'''\n","\n","\n","EO =(TPR[0] - TPR[1] + TPR[2] - TPR[3] + TPR[4] - TPR[5] + TPR[6] - TPR[7] + TPR[8] - FPR[9]) \n","print('EO : {}' . format(EO))\n","\n","\n","#Demographic parity\n","'''\n","Demographic parity is one of the most popular fairness indicators in the literature. \n","Demographic parity is achieved if the absolute number of positive predictions \n","in the subgroups are close to each other. This measure does not take true class into\n","consideration and only depends on the model predictions. In some literature, \n","demographic parity is also referred to as statsictal parity or independence.\n","'''\n","DP = (TP + FP)\n","print('Demographic parity : {}' . format(DP))\n","\n","#Equalized odds\n","'''\n","Equalized odds, also known as separation, are achieved if the sensitivities in the \n","subgroups are close to each other. The group-specific sensitivities \n","indicate the number of the true positives divided by the total \n","number of positives in that group.\n","'''\n","Equalized_Odds = TP / (TP + FN)\n","print('Equalized Odds : {}' . format(Equalized_Odds))\n","\n","\n","##---- Link Riccardo ----##\n","#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n","\n","print('----------------')\n","Balanced_Accuracy = sk.metrics.balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Generale : {:.5f}' . format(Balanced_Accuracy))\n","\n","\n","#####----------- PER CIASCUNA SERIE BALANCED ACCURACY -----------####\n","test_array_series_complete = [\n","        test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4,\n","        test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9,\n","        test_array_s10, test_array_s11, test_array_s12             \n","]\n","\n","pred_array_series_complete = [\n","        pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4,\n","        pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9,\n","        pred_array_s10, pred_array_s11, pred_array_s12             \n","]\n","sum_BA = 0\n","print('----------------')\n","for series in range(13):\n","  BA = sk.metrics.balanced_accuracy_score(test_array_series_complete[series],pred_array_series_complete[series], sample_weight=None, adjusted=False)\n","  print('Balanced Accuracy Series {} : {:.5f}' . format(series,BA ))\n","  print('----------------')\n","  sum_BA = sum_BA + BA\n","\n","\n","#----------- MEDIA DELLE BALANCED ACCURACY ---------------\n","Average = sum_BA/13\n","print('Average Balanced Accuracy : {:.5f}' . format(Average))\n"," \n","\n","##---- Wodsworth et Al ----# \n","#HIGH_RISK_GAP = SP #modulo o cardinalità \n","\n","#FN_GAP = false_negative (s1) - false negative (s2) \n","#FN_GAP = (false_negative_rate(y_true, y_pred) - false_negative_rate(y_true, y_pred))  #modulo o cardinalità\n","  \n","#FN_GAP = false_negative (s1) - false negative (s2) \n","#FP_GAP = (false_positive_rate(y_true, y_pred) - false_positive_rate(y_true, y_pred))  #modulo o cardinalità\n","\n","\n","\n","### LINK UTILE ####\n","#https://www.kaggle.com/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["ox6IFmrBfpP-","50n4oVKemd9n","VgAYZM1NDIRj","zUvw3mwvfCMA","xuS19wSXbdE8","28i0TbSpzt_Z","Ti_N5HuUFggs","o3iWx_QMIVQp","VX2SB5uYzzyB","26Za2K_yFljo","DBHMPMTPX5V5","lGEjbyRmMQNm","jJ_HZL4CX0sf","oow7OM0obIF4","pMvexlvYe5xD","MG_zlz7PfzZC","zghXE8FCfueE","hUVliMM4zT4F","JCHj6IT9fnoi","XnUgaUmoFykZ","ig3uCmF7L-Zt","u6nY8B7VcfYj","A_U_C71VdOAO","-GViGlheXfJt","shGoDK-cd70n","ohUoctJUcmoa","RRGftpaXbdNE","k-82Jf4Ndqdd","NdslbUPRlioe","-zMdWtZif_ui","Hosx74qRSs31","xildnrRfoxil","tbPUx79ZozKx","eEMosgpdo2Hj","d45MkW5no5oD","lnkHiR9qSc9t","VkaCQp4Ndzbe","z25yiN0zKZ5Y","yy4ylbat-r51","NoegV_027yUO","palNIXpFq6SV"],"machine_shape":"hm","provenance":[{"file_id":"1e4Y9JykKQU4LTHcLHy5sAzpwI-SPmTl6","timestamp":1623766889995},{"file_id":"1d0RkIbwRSJuanJjnXiLFbQSY2wZpHWXW","timestamp":1623496078537},{"file_id":"1eCj3WI6GCKlwC6VldMXH9RiHM1W8W5kj","timestamp":1623321103095},{"file_id":"1tcILVHrFk_-CCLnWgai88ENO5EJx1w_0","timestamp":1623062215958},{"file_id":"1GtF5i2sJMGQwC1uEVhcJJwKD7Zm_MjqW","timestamp":1622106936931},{"file_id":"1QblOtm62z9o1JIL5BVVghxIfnaxJ-Xtw","timestamp":1622099766973},{"file_id":"1gcmGZCqZ_dyjzG5wG6b5xgRBEGH0BKdK","timestamp":1621966104442},{"file_id":"10dyVnpAJogKpKJo-l5KYpqNYLxR8v0v8","timestamp":1621951584071},{"file_id":"1i_LnVOfxnPfx3KpC89VtMAf6M6eKIip1","timestamp":1621802010244},{"file_id":"1E4TbDqseXZ6wxtPDbKKuKUpB7F076PP4","timestamp":1621784336493}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"38b8c5002b544143b663b2df73691646":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d343692141b458cac84bdf864b816a7","IPY_MODEL_c0b3209ce4834e7a8429844c4eafd6ee","IPY_MODEL_b3dcbcbfd74b4b3f9124a51656494dbc"],"layout":"IPY_MODEL_f2708be5de65451490d7b5d4ad60ca49"}},"8d343692141b458cac84bdf864b816a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cb573b5fc45423b8b6d2fdf6ac552eb","placeholder":"​","style":"IPY_MODEL_6067d8bbaaed40dcaea7c94d2859954d","value":"100%"}},"c0b3209ce4834e7a8429844c4eafd6ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20ef333fa2554c68a91d514af5f593e3","max":553433881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_625336ef685c42fbb86f4703590d5d13","value":553433881}},"b3dcbcbfd74b4b3f9124a51656494dbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c68a3f24d8714f18b10191eb8424cce4","placeholder":"​","style":"IPY_MODEL_ac304c2ccac64bdb811b36e6d9e9debf","value":" 528M/528M [00:02&lt;00:00, 253MB/s]"}},"f2708be5de65451490d7b5d4ad60ca49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cb573b5fc45423b8b6d2fdf6ac552eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6067d8bbaaed40dcaea7c94d2859954d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20ef333fa2554c68a91d514af5f593e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"625336ef685c42fbb86f4703590d5d13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c68a3f24d8714f18b10191eb8424cce4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac304c2ccac64bdb811b36e6d9e9debf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}