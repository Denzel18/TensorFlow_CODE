{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denzel18/Tensorflow_Architecture_CODE/blob/main/TensorFlow_architettura_base_(D_A_Offline).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1Fs0GL_mVgn"
      },
      "source": [
        "# ***Tensorflow Architettura Base (D.A. Offline)  - A. Giacomini & D. Bernovschi***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Parametri Test (CROP e NO CROP) "
      ],
      "metadata": {
        "id": "GrO_C4jtoOYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_IMAGES = '/content/drive/MyDrive/CALCIO_CROP_BASE/'\n",
        "#path_IMAGES = '/content/drive/MyDrive/CALCIO_NOPRE/'\n",
        "\n",
        "'''some useful parameters and variables'''\n",
        "parte = 'CALCIO'\n",
        "tipo = 'CROP' #CROP, CROP_gray_ridge\n",
        "\n",
        "#Ricordati di cambiare anche gli Hyper Parameters !!!!"
      ],
      "metadata": {
        "id": "2oIejDBmoOqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox6IFmrBfpP-"
      },
      "source": [
        "## IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwFSXnnkfot7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "if 'google.colab' in sys.modules:\n",
        "  subprocess.call(\"pip install -U progress\".split())\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import os.path\n",
        "from os import path\n",
        "import scipy.ndimage\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import optimizers, layers\n",
        "from tensorflow.keras.layers import Activation, Input, Conv2D, ZeroPadding2D, MaxPooling2D, UpSampling2D, concatenate, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from PIL import Image, ImageOps\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score,classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from random import randrange\n",
        "print('Tensor Flow {}'.format(tf.__version__))\n",
        "print('Keras {}'.format(tf.keras.__version__))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''vecchio costrutto'''\n",
        "#random.seed( 40 )\n",
        "\n",
        "'''nuovo costrutto'''\n",
        "def fix_seeds(seed: int) -> None:\n",
        "  \"\"\" Fix random seeds for numpy, tensorflow, random, etc.\n",
        "\n",
        "  Parameters\n",
        "  -----------\n",
        "  seed : int.\n",
        "  Random seed.\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(seed) # numpy seed\n",
        "  tf.random.set_seed(seed) # tensorflow seed\n",
        "  random.seed(seed) # random seed\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
        "  os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "fix_seeds(40)"
      ],
      "metadata": {
        "id": "X6vZwklnndeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUvw3mwvfCMA"
      },
      "source": [
        "## DRIVE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkWUwK_IfBtx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path_drive = '/content/drive/My Drive/'\n",
        "path = path_drive+'ProgettoDL/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEzVkjhHbISE"
      },
      "source": [
        "## Parametri Immagini "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSyxD4LWbG3J"
      },
      "outputs": [],
      "source": [
        "'''DEFINE VARIABLES AND PARAMETERS TO COLLECT THE INFORMATIONS FROM GOOGLE DRIVE'''\n",
        "'''define a path for the collection of informations (CSV file) for the creation of the dataframe'''\n",
        "os.chdir('/content/drive/MyDrive/ProgettoDL/') \n",
        "\n",
        "'''to have always the same sequence of randomized values (numbers)'''\n",
        "random_state = 3  \n",
        "\n",
        "'''some useful parameters and variables'''\n",
        "augment = True\n",
        "metaclassi = False\n",
        "cnn = \"vgg16\" \n",
        "\n",
        "'''series of production & quality classes of the wood rifle butt'''\n",
        "classi = ['1','2-','2','2+','3-','3','3+','4-','4','4+']          \n",
        "serie = [2,4,8,10,6,9,3,11,12,13,14,15,7] \n",
        "cod_componente = [ 2,  4,  8, 10,  6,  9,  3, 11, 12, 13, 14, 15,  7]\n",
        "\n",
        "'''size of the images & their paths (location) '''\n",
        "immg_rows = 270 \n",
        "immg_cols = 470\n",
        "immgs = '{}_{}'.format(parte,tipo)\n",
        "path_imgs = os.path.join(path_drive+'{}'.format(immgs))\n",
        "\n",
        "'''CSV loading (reading annotations/attributes/informations)'''\n",
        "csv = pd.read_csv(('/content/drive/MyDrive/ProgettoDL/20201102_ExportDB.txt'), sep=\";\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuS19wSXbdE8"
      },
      "source": [
        "## SPLIT DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0JSykxbbeCU"
      },
      "outputs": [],
      "source": [
        "'''CUSTOM SPLIT DATA INTO TRAIN/TEST/VALIDATION SETS'''\n",
        "\"\"\"\n",
        "NOTE\n",
        "- VERSIONE CON NUMERI PRESI DIRETTAMENTE DAL BILANCIAMENTO CALCOLATO RISPETTO IL TOTALE DI 2120 (che ci sono in questo progetto), PER RENDERLO DINAMICO CALCOLARE PESI IN MODO AUTOMATICO (STUDIA ALTERNATIVA)\n",
        "- UNICO PROBLEMA È CHE A VOLTE IMMAGINI CON STESSO ID HANNO IN REALTÀ DIVERSA CLASSE DI QUALITÀ, QUINDI I DATASET NON SONO PERFETTAMENTE BILANCIATI MA VARIANO LEGGERMENTE,\n",
        "(perchè lo stesso ID deve stare in stesso set anche se i lati del calcio del fucile possono avere qualità differente)\n",
        "\"\"\"\n",
        "\n",
        "'''split method'''\n",
        "def split_data(dataframe_result, val_size, test_size, random_state):\n",
        "  #n_ immagini per ciascuna classe di qualità\n",
        "  classes_count = dataframe_result.groupby(['class']).size() \n",
        "\n",
        "  unique_result, counts = np.unique(dataframe_result['ID'], return_counts=True)   #conto quanti ID univoci esistono nel dataset e li raccolgo tutti in vettore\n",
        "\n",
        "  '''randomizing the order of the IDs, (to change the sequence change the random_state)'''\n",
        "  #id_perm = unique_result.iloc[np.random.permutation(unique_result.index)].reset_index(drop=True)\n",
        "  id_perm = np.random.RandomState(random_state).permutation(unique_result)\n",
        "  #print('ID Perm : {}'.format(id_perm))\n",
        "               \n",
        "  '''define finals sub-sets of data'''\n",
        "  column_names = ['ID','series','filename','class']\n",
        "  x_train = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  x_test = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  x_val = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  '''define variables to count elements inside the sub-sets'''\n",
        "  conta, conta0, conta1, conta2, conta3, conta4, conta5, conta6, conta7, conta8, conta9 = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
        "  class_ = 0\n",
        "\n",
        "\n",
        "  '''performing cycles to divide the images into the 3 sub-sets'''\n",
        "  for i in id_perm:\n",
        "     result_ID = dataframe_result.loc[(dataframe_result['ID'] == i)]\n",
        "      #print('Size : {} '.format(result_ID[result_ID.columns[0]].count()))\n",
        "     if result_ID[result_ID.columns[0]].count() == 2:\n",
        "       row_1=result_ID.iloc[0]\n",
        "       class_ = int(row_1['class'])\n",
        "       row_2=result_ID.iloc[1]\n",
        "       class2_ = int(row_2['class'])\n",
        "       conta = 2\n",
        "       #print(\"ID doppio\")\n",
        "     else:\n",
        "       row_1=result_ID.iloc[0]\n",
        "       class_ = int(row_1['class'])\n",
        "       conta = 1\n",
        "       #print(\"ID singolo\")\n",
        "\n",
        "     if class_ == 0 and conta0 < int((classes_count[0]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta0 = conta0 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta0 = conta0 + 1\n",
        "     elif class_ == 1 and conta1 < int((classes_count[1]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta1 = conta1 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta1 = conta1 + 1\n",
        "     elif class_ == 2 and conta2 < int((classes_count[2]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta2 = conta2 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta2 = conta2 + 1\n",
        "     elif class_ == 3 and conta3 < int((classes_count[3]/100)*60) :\n",
        "        if conta == 2 :\n",
        "          x_train=x_train.append(row_1, ignore_index=True) \n",
        "          x_train=x_train.append(row_2, ignore_index=True)\n",
        "          conta3 = conta3 + 2\n",
        "        else :\n",
        "          x_train=x_train.append(row_1, ignore_index=True) \n",
        "          conta3 = conta3 + 1\n",
        "     elif class_ == 4 and conta4 < int((classes_count[4]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta4 = conta4 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True)\n",
        "         conta4 = conta4 + 1 \n",
        "     elif class_ == 5 and conta5 < int((classes_count[5]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta5 = conta5 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta5 = conta5 + 1\n",
        "     elif class_ == 6 and conta6 < int((classes_count[6]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta6 = conta6 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta6 = conta6 + 1\n",
        "     elif class_ == 7 and conta7 < int((classes_count[7]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta7 = conta7 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta7 = conta7 + 1\n",
        "     elif class_ == 8 and conta8 < int((classes_count[8]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta8 = conta8 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta8 = conta8 + 1\n",
        "     elif class_ == 9 and conta9 < int((classes_count[9]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta9 = conta9 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta9 = conta9 + 1\n",
        "     elif class_ == 0 and conta0 >= int((classes_count[0]/100)*60) and conta0 < int((classes_count[0]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta0 = conta0 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta0 = conta0 + 1\n",
        "     elif class_ == 1 and conta1 >= int((classes_count[1]/100)*60) and conta1 < int((classes_count[1]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta1 = conta1 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta1 = conta1 + 1\n",
        "     elif class_ == 2 and conta2 >= int((classes_count[2]/100)*60) and conta2 < int((classes_count[2]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta2 = conta2 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta2 = conta2 + 1\n",
        "     elif class_ == 3 and conta3 >= int((classes_count[3]/100)*60) and conta3 < int((classes_count[3]/100)*80) :\n",
        "        if conta == 2 :\n",
        "          x_val=x_val.append(row_1, ignore_index=True) \n",
        "          x_val=x_val.append(row_2, ignore_index=True)\n",
        "          conta3 = conta3 + 2\n",
        "        else :\n",
        "          x_val=x_val.append(row_1, ignore_index=True) \n",
        "          conta3 = conta3 + 1\n",
        "     elif class_ == 4 and conta4 >= int((classes_count[4]/100)*60) and conta4 < int((classes_count[4]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta4 = conta4 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True)\n",
        "         conta4 = conta4 + 1 \n",
        "     elif class_ == 5 and conta5 >= int((classes_count[5]/100)*60) and conta5 < int((classes_count[5]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta5 = conta5 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta5 = conta5 + 1\n",
        "     elif class_ == 6 and conta6 >= int((classes_count[6]/100)*60) and conta6 < int((classes_count[6]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta6 = conta6 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta6 = conta6 + 1\n",
        "     elif class_ == 7 and conta7 >= int((classes_count[7]/100)*60) and conta7 < int((classes_count[7]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta7 = conta7 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta7 = conta7 + 1\n",
        "     elif class_ == 8 and conta8 >= int((classes_count[8]/100)*60) and conta8 < int((classes_count[8]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta8 = conta8 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta8 = conta8 + 1\n",
        "     elif class_ == 9 and conta9 >= int((classes_count[9]/100)*60) and conta9 < int((classes_count[9]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta9 = conta9 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta9 = conta9 + 1\n",
        "     elif class_ == 0 and conta0 >= int((classes_count[0]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta0 = conta0 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta0 = conta0 + 1\n",
        "     elif class_ == 1 and conta1 >= int((classes_count[1]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta1 = conta1 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta1 = conta1 + 1\n",
        "     elif class_ == 2 and conta2 >= int((classes_count[2]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta2 = conta2 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta2 = conta2 + 1\n",
        "     elif class_ == 3 and conta3 >= int((classes_count[3]/100)*80) :\n",
        "        if conta == 2 :\n",
        "          x_test=x_test.append(row_1, ignore_index=True) \n",
        "          x_test=x_test.append(row_2, ignore_index=True)\n",
        "          conta3 = conta3 + 2\n",
        "        else :\n",
        "          x_test=x_test.append(row_1, ignore_index=True) \n",
        "          conta3 = conta3 + 1\n",
        "     elif class_ == 4 and conta4 >= int((classes_count[4]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta4 = conta4 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True)\n",
        "         conta4 = conta4 + 1 \n",
        "     elif class_ == 5 and conta5 >= int((classes_count[5]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta5 = conta5 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta5 = conta5 + 1\n",
        "     elif class_ == 6 and conta6 >= int((classes_count[6]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta6 = conta6 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta6 = conta6 + 1\n",
        "     elif class_ == 7 and conta7 >= int((classes_count[7]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta7 = conta7 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta7 = conta7 + 1\n",
        "     elif class_ == 8 and conta8 >= int((classes_count[8]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta8 = conta8 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta8 = conta8 + 1\n",
        "     elif class_ == 9 and conta9 >= int((classes_count[9]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta9 = conta9 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta9 = conta9 + 1\n",
        "\n",
        "    \n",
        "  return x_train, x_test, x_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4OSrOanfe7I"
      },
      "source": [
        "## DATA GENERATION "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScS3q5MmWMJg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "    def __init__(self, df, X_col, y_col,\n",
        "                 batch_size,\n",
        "                 input_size = (270, 470),\n",
        "                 shuffle = True):\n",
        "      \n",
        "        \n",
        "        self.df = df.copy()\n",
        "        self.X_col = X_col\n",
        "        self.y_col = y_col\n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        \n",
        "        self.n = len(self.df)\n",
        "        self.n_CLASSE_CALCIO = df[y_col['CLASSE']].nunique()\n",
        "        \n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    def __get_input(self, path, target_size):\n",
        "      try:\n",
        "        image = tf.keras.preprocessing.image.load_img(path_IMAGES+path, color_mode=\"rgb\" , target_size=(target_size[0],target_size[1]))\n",
        "      except Exception:\n",
        "        print('\\n{}_not found'.format(path))\n",
        "      \n",
        "      image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
        "      image_arr = tf.keras.applications.vgg16.preprocess_input(image_arr, data_format=\"channels_first\")               \n",
        "\n",
        "\n",
        "      return image_arr\n",
        "    \n",
        "    def __get_output(self, label, num_classes):\n",
        "        return tf.keras.utils.to_categorical(label, num_classes=num_classes)\n",
        "\n",
        "    def __get_output2(self, label, num_series):\n",
        "        return tf.keras.utils.to_categorical(label, num_classes=num_series)\n",
        "    \n",
        "    def __get_data(self, batches):\n",
        "        # Generates data containing batch_size samples\n",
        "        path_batch = batches[self.X_col['PATH_IMG']]  \n",
        "        CLASSE_batch = batches[self.y_col['CLASSE']]\n",
        "        #SERIE_batch = batches[self.y_col['GEOMETRIA']]\n",
        "\n",
        "        X_batch = np.asarray([self.__get_input(x, self.input_size) for x in path_batch])\n",
        "        y_batch = np.asarray([self.__get_output(y, self.n_CLASSE_CALCIO) for y in CLASSE_batch])\n",
        "        return X_batch, y_batch\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X, y = self.__get_data(batches)\n",
        "        return X, y                         \n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.n) // self.batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28i0TbSpzt_Z"
      },
      "source": [
        "## NETWORK "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Dc7ahEzpWW"
      },
      "outputs": [],
      "source": [
        "'''NETWORK'''\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "vgg16_conv = VGG16(include_top=False, weights='imagenet', input_shape=(immg_rows, immg_cols, 3))      #pre allenata con immagini di imagenet, e quindi pesi già esistenti\n",
        "for layer in vgg16_conv.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "##NOTA : in alcune alternative ho visto che applicano il Flatten prima della ramificazione.\n",
        "x = Flatten(name='flatten')(vgg16_conv.output)\n",
        "#classifier ramification     \n",
        "#x = Flatten(name='flatten')(vgg16_conv.output)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4096, activation='relu', name='fc1')(x)\n",
        "x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(len(classi), activation='softmax', name='class_output')(x)\n",
        "\n",
        "# stitch together\n",
        "dot_img_file = '/tmp/model_2.png'\n",
        "\n",
        "#secondo modello dove faccio la ramificazione\n",
        "model = keras.Model(vgg16_conv.input, x, name=\"quality_recognizer\")\n",
        "\n",
        "# inspect\n",
        "model.summary()\n",
        "\n",
        "#PLOT DEL MODELLO COMPLETO RAMIFICATO\n",
        "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XV_nhqtTWxr"
      },
      "source": [
        "## Metrica Balance Accuracy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-rg-uNKANyC"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "'''\n",
        "Funzione per Balance Accuracy \n",
        "-- Link Utile : https://medium.com/@mostafa.m.ayoub/customize-your-keras-metrics-44ac2e2980bd --\n",
        "-- https://medium.com/analytics-vidhya/custom-metrics-for-keras-tensorflow-ae7036654e05 --- \n",
        "-- https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score --- \n",
        "'''\n",
        "\n",
        "\t\t\n",
        "'''\n",
        "Funzione per Balance Accuracy \n",
        "'''\n",
        "def monitor_balance_accuracy ():\n",
        "\tdef bal_acc(y_true, y_pred):\n",
        "\t\tprint(y_pred)\n",
        "\t\tprint(len(y_pred))\n",
        "\t\tprint(type(y_pred))\n",
        "\t\tprint(y_true)\n",
        "\t\tprint(len(y_true))\n",
        "\t\tprint(type(y_true))\n",
        "\t\t#y_true = y_true.numpy().argmax(axis=1) #Returns the indices of the maximum values along an axis.\n",
        "\t\t#y_pred = y_pred.numpy().argmax(axis=1) #Returns the indices of the maximum values along an axis.\n",
        "\t\tBalanced_Accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "\t\tBalanced_Accuracy = tf.constant(Balanced_Accuracy)\n",
        "\t\treturn K.min(Balanced_Accuracy)\n",
        "\treturn bal_acc\n",
        "\n",
        "\n",
        "#https://medium.com/@mostafa.m.ayoub/customize-your-keras-metrics-44ac2e2980bd\n",
        "#https://www.statology.org/balanced-accuracy-python-sklearn/\n",
        "def specificity(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    return tn / (tn + fp + K.epsilon())\n",
        "\n",
        "def sensitivity(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def balanced_accuracy_new (y_true, y_pred):\n",
        "\t\tsensitivity_ = sensitivity(y_true, y_pred)\n",
        "\t\tspecificity_ = specificity(y_true, y_pred)\n",
        "\t\tBA = (sensitivity_ + specificity_) / 2\n",
        "\t\treturn BA \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX2SB5uYzzyB"
      },
      "source": [
        "## Callbacks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvAnOHC2W134"
      },
      "outputs": [],
      "source": [
        "'''CALLBACKS'''\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "#TEST BALANCE ACCURACY \n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "#NB: create sul drive una cartella weights dove salvare i pesi durante l'allenamento\n",
        "path_drive = '/content/drive/My Drive/'\n",
        "path = path_drive+'ProgettoDL/'\n",
        "model_checkpoint_val_bal_acc = ModelCheckpoint( filepath=os.path.join('/content/drive/My Drive/ProgettoDL/weights/model_{}_{}/best_weights.h5'.format(immgs,cnn)), monitor='val_balanced_accuracy_new', verbose=1, save_best_only=True)\n",
        "model_checkpoint_val_loss = ModelCheckpoint( filepath=os.path.join('/content/drive/My Drive/ProgettoDL/weights/model_{}_{}/best_weights.h5'.format(immgs,cnn)), monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "### MODIFICATO QUA - Implementazione Early Stopping###\n",
        "early_stopping_val_bal_acc = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_balanced_accuracy_new', #Quantity to be monitored \n",
        "    min_delta=0, #Minimum change in the monitored quantity to qualify as an improvement\n",
        "    patience=10, #Number of epochs with no improvement after which training will be stopped\n",
        "    #verbosity mode, setting verbose 0, 1 or 2 you just say \n",
        "    #how do you want to 'see' the training progress for each epoch.\n",
        "    #verbose=0 will show you nothing (silent)\n",
        "    #verbose=1 will show you an animated progress bar like this: progres_bar\n",
        "    verbose=0, \n",
        "    #Mode = One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity \n",
        "    #monitored has stopped decreasing; in \"max\" mode \n",
        "    #it will stop when the quantity monitored has stopped increasing; \n",
        "    #in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n",
        "    mode=\"max\",\n",
        "    #Training will stop if the model doesn't show improvement over the baseline.\n",
        "    baseline=None,\n",
        "    #Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
        "    restore_best_weights=False\n",
        ")\n",
        "early_stopping_val_loss = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', #Quantity to be monitored \n",
        "    min_delta=0, #Minimum change in the monitored quantity to qualify as an improvement\n",
        "    patience=10, #Number of epochs with no improvement after which training will be stopped\n",
        "    #verbosity mode, setting verbose 0, 1 or 2 you just say \n",
        "    #how do you want to 'see' the training progress for each epoch.\n",
        "    #verbose=0 will show you nothing (silent)\n",
        "    #verbose=1 will show you an animated progress bar like this: progres_bar\n",
        "    verbose=0, \n",
        "    #Mode = One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity \n",
        "    #monitored has stopped decreasing; in \"max\" mode \n",
        "    #it will stop when the quantity monitored has stopped increasing; \n",
        "    #in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n",
        "    mode=\"auto\",\n",
        "    #Training will stop if the model doesn't show improvement over the baseline.\n",
        "    baseline=None,\n",
        "    #Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
        "    restore_best_weights=False\n",
        ")\n",
        "callbacks=[model_checkpoint_val_bal_acc, model_checkpoint_val_loss , early_stopping_val_bal_acc, early_stopping_val_loss ]\n",
        "#TEST DENIS\n",
        "#callbacks=[model_checkpoint_val_loss, early_stopping_val_loss ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQsYqmoQVJIu"
      },
      "source": [
        "## PREPROCESSING & DATA FRAME "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''verify that same IDs are in the same sub-sets'''\n",
        "\n",
        "'''method for univoque sets'''\n",
        "def check_for_leakage(df1, df2, patient_col):\n",
        "    \"\"\"\n",
        "    Return True if there any patients are in both df1 and df2.\n",
        "\n",
        "    Args:\n",
        "        df1 (dataframe): dataframe describing first dataset\n",
        "        df2 (dataframe): dataframe describing second dataset\n",
        "        patient_col (str): string name of column with patient IDs\n",
        "    \n",
        "    Returns:\n",
        "        leakage (bool): True if there is leakage, otherwise False\n",
        "    \"\"\"\n",
        "    \n",
        "    df1_patients_unique = set(df1[patient_col])\n",
        "    df2_patients_unique = set(df2[patient_col])\n",
        "    \n",
        "    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)\n",
        "\n",
        "    # leakage contains true if there is patient overlap, otherwise false.\n",
        "    leakage = len(patients_in_both_groups) >= 1 # boolean (true if there is at least 1 patient in both groups)\n",
        "        \n",
        "    return leakage"
      ],
      "metadata": {
        "id": "KgDSg0Ylufeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCvpWRyju3cE"
      },
      "outputs": [],
      "source": [
        "'''PREPROCESSING PHASE OF THE DATAFRAME (CREATIONS OF THE SUBSETS TRAIN/VALIDATION/TEST, CALCULATE WEIGHTS OF ELEMENTS OF THE SUBSETS, VERIFY THAT SAME IDs ARE IN THE SAME SUBSET)'''\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from pandas.compat._optional import import_optional_dependency\n",
        "\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/ProgettoDL')\n",
        "path = os.getcwd()\n",
        "\n",
        "\n",
        "'''reading inforamtions from the CSV'''\n",
        "col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n",
        "dataframe_sx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n",
        "\n",
        "\n",
        "col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n",
        "dataframe_dx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n",
        "\n",
        "\n",
        "'''rename the dataframe columns'''\n",
        "dataframe_sx.columns = ['ID','series', 'filename', 'class']\n",
        "dataframe_dx.columns = ['ID','series', 'filename', 'class']\n",
        "\n",
        "frames = [dataframe_sx, dataframe_dx] \n",
        "result = pd.concat(frames) #concatenate the two dataframes\n",
        "\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"DATAFRAME COMPLETO INIZIALE\")\n",
        "print(\"result\")\n",
        "print(result)\n",
        "\n",
        "\n",
        "'''mapping the values used for the classification into integer values'''\n",
        "#version with 10 classes\n",
        "result[\"class\"] = result[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n",
        "result[\"series\"] = result[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)}) \n",
        "\n",
        "\n",
        "'''identification of NULL values that would bring the execution on failing and eliminate those values'''\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"Number of Null values in column 'quality_classes' : \"+format(result['class'].isnull().sum()))\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "#print(result.loc[result['class'] == '0'])\n",
        "print(\"mostro quegli elementi che hanno valore nullo\")\n",
        "print(result[result['class'].isnull()])\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "\n",
        "'''Remove Null elements to avoid failures during executions (data in not useful!)'''\n",
        "print(\"Rimuovo gli elementi nulli e verifico stampando nuovamente i valori nulli:\")\n",
        "result['class'] = pd.to_numeric(result['class'], errors='coerce')\n",
        "result = result.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n",
        "\n",
        "print(\"elementi nulli rimasti: \"+format(result['class'].isnull().sum()))     #stampo per verifica se ci sono elementi nulli\n",
        "\n",
        "\n",
        "'''verify if images exist in the Google Drive folder, when not present it is eliminated from the dataset aswell'''\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"elimino i file che non sono presenti in Google Drive anche se ci sono nel CSV\")\n",
        "print('CHECK FILE NON PRESENTI NELLA CARTELLA')\n",
        "\n",
        "#--------TORNA QUI ---------\n",
        "os.chdir(path_IMAGES)\n",
        "\n",
        "i = 0; \n",
        "for index, row in result.iterrows():\n",
        "    filename = row['filename']\n",
        "    if os.path.exists(path_IMAGES+filename) == False:\n",
        "      \n",
        "      print('File Non Esiste !!!')\n",
        "    if(os.path.exists(filename) == False):\n",
        "      result = result.drop(result[(result['filename'] == filename)].index)\n",
        "      print('File : {} eliminato'.format(filename))\n",
        "      i = i + 1             \n",
        "print('File Eliminati : {} '.format(i))\n",
        "\n",
        "print('CHECK FILE CON NaN')\n",
        "print(result[result['class'].isnull()])\n",
        "print(result[result['series'].isnull()])\n",
        "print(result[result['filename'].isnull()])\n",
        "print(result[result['ID'].isnull()])\n",
        "result = result[result['class'].notna()]\n",
        "result = result[result['series'].notna()]\n",
        "result = result[result['filename'].notna()]\n",
        "result = result[result['ID'].notna()]\n",
        "\n",
        "'''performing the splitting of the dataframe into sub-sets'''\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"SPLIT DATA\")\n",
        "train_balance_df, test_balance_df, val_balance_df  = split_data(result, 0.2, 0.2, 3)  #CUSTOM SPLIT CON ID IN STESSO SET DI DATI\n",
        "#train_mask, test_mask, validation_mask  = split_data(result2, 0.2, 0.2, 3)           #split per test con immagini con maschere\n",
        "\n",
        "print(\"train_balance_df\")\n",
        "print(train_balance_df)\n",
        "print(\"test_balance_df\")\n",
        "print(test_balance_df)\n",
        "print(\"val_balance_df\")\n",
        "print(val_balance_df)\n",
        "\n",
        "'''verify distibution of classes in the sub-sets and calculate weights of the classes in each sub-set'''\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "vals, counts = np.unique(train_balance_df['class'], return_counts=True)\n",
        "print(\"conta del numero di immagini per speicfica classe in set Train\")\n",
        "print(len(train_balance_df))\n",
        "for i in range(0,len(classi)):\n",
        "    print('{}:{}'.format(classi[i], counts[i]))\n",
        "\n",
        "vals2, counts2 = np.unique(val_balance_df['class'], return_counts=True)\n",
        "print(\"conta del numero di immagini per speicfica classe in set Validation\")\n",
        "print(len(val_balance_df))\n",
        "for i in range(0,len(classi)):\n",
        "    print('{}:{}'.format(classi[i], counts2[i]))\n",
        "\n",
        "vals3, counts3 = np.unique(test_balance_df['class'], return_counts=True)\n",
        "print(\"conta del numero di immagini per speicfica classe in set Test\")\n",
        "print(len(test_balance_df))\n",
        "for i in range(0,len(classi)):\n",
        "    print('{}:{}'.format(classi[i], counts3[i]))    \n",
        "\n",
        "\n",
        "class_weights_train = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(train_balance_df['class']),y = train_balance_df['class'])\n",
        "weight_train = {i : round(class_weights_train[i], 2) for i in range(len(classi))} \n",
        "print('Weight train_balance_df')\n",
        "print(weight_train)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(val_balance_df['class']),y = val_balance_df['class'])\n",
        "weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n",
        "print('Weight val_balance_df')\n",
        "print(weight)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(test_balance_df['class']),y = test_balance_df['class'])\n",
        "weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n",
        "print('Weight test_balance_df')\n",
        "print(weight)\n",
        "\n",
        "#--------verifico che stessi ID siano in stesso set--------\n",
        "print(\"test case 1 - train VS validation\")\n",
        "print(f\"Stessi ID in set usati?: {check_for_leakage(train_balance_df, val_balance_df, 'ID')}\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"test case 2 - train VS test\")\n",
        "print(f\"Stessi ID in set usati ?: {check_for_leakage(train_balance_df, test_balance_df, 'ID')}\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"test case 3 - validation VS test\")\n",
        "print(f\"Stessi ID in set usati?: {check_for_leakage(val_balance_df, test_balance_df, 'ID')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPihVVu9fXJb"
      },
      "outputs": [],
      "source": [
        "'''Verifica Classi Qualità per ogni Serie'''\n",
        "print(\"Verifica Classi Qualità per ogni Serie\")\n",
        "result_x_ = result.groupby(['series','class']).size()\n",
        "print(result_x_)\n",
        "result_class = result.groupby(['class']).size()\n",
        "print(result_class)\n",
        "result_series = result.groupby(['series']).size()\n",
        "print(result_series)\n",
        "\n",
        "print('SOMMA IMG : {}'.format(result_class[0]+result_class[1]+result_class[2]+result_class[3]+result_class[4]+result_class[5]+result_class[6]+result_class[7]+result_class[8]+result_class[9]))\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D.A. OFFLINE\n"
      ],
      "metadata": {
        "id": "Q7UtR8q43GP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import save_img\n",
        "from skimage import io, exposure\n",
        "\n",
        "print(train_balance_df)\n",
        "train_balance_df_copy = train_balance_df\n",
        "n_img_for_class = np.zeros(10)\n",
        "for i in range(10):\n",
        "  n_img_for_class[i] = len(train_balance_df[train_balance_df['class']==i])\n",
        "print(f'n° immagini per ciascuna classe ', n_img_for_class) #-- il n° di img per ciascuna classe \n",
        "\n",
        "classe_maggioritaria = np.max(n_img_for_class)\n",
        "classe_minoritaria = np.min(n_img_for_class)\n",
        "print(f'La classe maggioritaria contiene n: ',classe_maggioritaria,'immagini')\n",
        "print(f'La classe minoritaria contiene n: ',classe_minoritaria,'immagini')\n",
        "index_min = np.argmin(n_img_for_class)\n",
        "index_max = np.argmax(n_img_for_class)\n",
        "print(f'classe minima : ',index_min)\n",
        "print(f'classe massima : ',index_max)\n",
        "\n",
        "\n",
        "# calcolo n° immagini da creare per ciascuna classe di qualità - non lo usiamo direttamente\n",
        "n_img_for_class_to_create = np.zeros(10)\n",
        "for i in range(10):\n",
        "  n_img_for_class_to_create[i] = classe_maggioritaria - n_img_for_class[i] \n",
        "print(f'n° immagini da creare per ciascuna classe ', n_img_for_class_to_create) #-- il n° di img per ciascuna classe \n",
        "\n",
        "\n",
        "#Data Augmentation Offline - Parto dalla prima classe di qualità e genero immagini volta per volta fino ad arrivare al numero della maggioritaria\n",
        "for k in range(10): \n",
        "  #print(k)\n",
        "  actual_df = train_balance_df[train_balance_df['class']==k]\n",
        "  #print(actual_df)\n",
        "  if n_img_for_class_to_create[k] > 0:\n",
        "    print(f'devo creare immagini della classe : ',k)\n",
        "    n_img_in_actual_df = len(actual_df)\n",
        "    print(f'len actual df', n_img_in_actual_df)\n",
        "    count = n_img_in_actual_df\n",
        "    #for row in range(int(n_img_in_actual_df)):\n",
        "    for i in range(int(abs(classe_maggioritaria/classe_minoritaria))): \n",
        "      for row in range(int(n_img_in_actual_df)):\n",
        "        if (count < classe_maggioritaria):\n",
        "          count = count + 1\n",
        "          row_selected = actual_df.iloc[row]\n",
        "\n",
        "          filename = row_selected['filename']\n",
        "          \n",
        "          img_path = os.path.join(path_IMAGES+filename)\n",
        "          \n",
        "          if i==0: \n",
        "            if os.path.exists(path_IMAGES+'T1_'+filename) == False:\n",
        "              image = io.imread(img_path)\n",
        "              image_bright = exposure.adjust_gamma(image, gamma=0.5, gain=1)\n",
        "              io.imsave(path_IMAGES+'T1_'+filename,image)\n",
        "            row_selected['filename'] = 'T1_'+filename\n",
        "            train_balance_df_copy = train_balance_df_copy.append(row_selected, ignore_index = True)   \n",
        "          \n",
        "          if i==1: \n",
        "            if os.path.exists(path_IMAGES+'T2_'+filename) == False:\n",
        "              image = io.imread(img_path)\n",
        "              #horizontal flip\n",
        "              image = image[:, ::-1]\n",
        "              io.imsave(path_IMAGES+'T2_'+filename, image)\n",
        "            row_selected['filename'] = 'T2_'+filename\n",
        "            train_balance_df_copy = train_balance_df_copy.append(row_selected, ignore_index = True)  \n",
        "           \n",
        "          if i==2: \n",
        "            if os.path.exists(path_IMAGES+'T3_'+filename) == False:\n",
        "              image = io.imread(img_path)\n",
        "              #vertical flip\n",
        "              image = image[::-1, :]\n",
        "              io.imsave(path_IMAGES+'T3_'+filename, image)\n",
        "            row_selected['filename'] = 'T3_'+filename\n",
        "            train_balance_df_copy = train_balance_df_copy.append(row_selected, ignore_index = True)           \n",
        "\n",
        "        else: \n",
        "          break\n",
        "  else:\n",
        "    print(f'non devo creare immagini della classe : ',k)\n",
        "# check di verifica \n",
        "#print(f'class_weight train_balance_df',compute_sample_weight(class_weight='balanced', y=train_balance_df['class']))\n",
        "print(f'class_weight train_balance_df_copy',compute_sample_weight(class_weight='balanced', y=train_balance_df_copy['class']))\n",
        "n_img_for_class_after = np.zeros(10)\n",
        "for i in range(10):\n",
        "  n_img_for_class_after[i] = len(train_balance_df_copy[train_balance_df_copy['class']==i])\n",
        "print(f'n° immagini per ciascuna classe ', n_img_for_class_after) #-- il n° di img per ciascuna classe \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2mo38oZl3JDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_balance_df_copy"
      ],
      "metadata": {
        "id": "Br_gKgyzWWQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Za2K_yFljo"
      },
      "source": [
        "## Weighted Categorical Crossentropy Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ05ClCuFnkj"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "class weighted_categorical_crossentropy(object):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "    \n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "    \n",
        "    Usage:\n",
        "        loss = weighted_categorical_crossentropy(weights).loss\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,weights):\n",
        "        self.weights = K.variable(weights)\n",
        "        \n",
        "    def loss(self,y_true, y_pred): \n",
        "        # scale preds so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred)\n",
        "        # clip\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "        # calc\n",
        "        \n",
        "        loss = y_true*K.log(y_pred)*self.weights\n",
        "        loss =-K.sum(loss,-1)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks3Fly2oUf3g"
      },
      "source": [
        "## HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbjwj4QwW6yx"
      },
      "outputs": [],
      "source": [
        "#Definizione hyperparameters                                                                                                                         #prova SGD\n",
        "\n",
        "#opt = Adam(learning_rate=\t0.01)\n",
        "opt = SGD(learning_rate = 0.0001, decay = 1e-4, momentum= 0.9)\n",
        "\n",
        "num_epochs = 100 \n",
        "bs = 32\n",
        "\n",
        "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"accuracy\",balanced_accuracy_new])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gwtHXxECiTK"
      },
      "source": [
        "## Creazione TrainGen, ValGen, TestGen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St_p67bjCnby"
      },
      "outputs": [],
      "source": [
        "#traingen = CustomDataGen(train_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class'}, batch_size=bs, input_size=(270,470))\n",
        "traingen = CustomDataGen(train_balance_df_copy, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class'}, batch_size=bs, input_size=(270,470))\n",
        "testgen = CustomDataGen(test_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class'}, batch_size=bs, input_size=(270,470))   \n",
        "valgen = CustomDataGen(val_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class'}, batch_size=bs, input_size=(270,470))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfJ3LnbAZhQx"
      },
      "source": [
        "## Testing Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMXAiMAvyom5"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "history = model.fit(x=traingen,validation_data=valgen, epochs=num_epochs, callbacks = [callbacks] , verbose=1)\n",
        "print(history.history.keys()) #---serve per stampare le metriche che ho nel modello \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBGTOuc1VSqq"
      },
      "source": [
        "## PLOT "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQzh_8drJ56m"
      },
      "outputs": [],
      "source": [
        "'''PLOT CURVES'''\n",
        "\n",
        "path = path_drive+'ProgettoDL/'\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "bal_acc = history.history['balanced_accuracy_new']\n",
        "val_bal_acc = history.history['val_balanced_accuracy_new']\n",
        "lista = [acc,val_acc,loss,val_loss,bal_acc, val_bal_acc]\n",
        "\n",
        "\n",
        "#print(corr(history.history['accuracy'], history_mask.history_mask['accuracy']))\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(\"VGG16.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(lista)\n",
        "     \n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy (IMG)')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(path+'weights/PlotAcc_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "plt.figure()\n",
        " \n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss (IMG)')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(path+'weights/PlotLoss_{}_{}.pdf'.format(immgs,cnn)))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, bal_acc, 'b', label='Training Balance Accuracy')\n",
        "plt.plot(epochs, val_bal_acc, 'r', label='Validation Balance Accuracy')\n",
        "plt.title('Training and validation balance accuracy (IMG)')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(path+'weights/PlotBalAcc_{}_{}.pdf'.format(immgs,cnn)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjI6Y_nefFTj"
      },
      "source": [
        "## SAVE MODEL "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApFaEaa6oPu0"
      },
      "outputs": [],
      "source": [
        "#salvataggio modello pesi finali\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "path = path_drive+'ProgettoDL/'\n",
        "model.save(os.path.join(path+'weights/model_{}_{}/Final'.format(immgs,cnn)))\n",
        "print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XefnmV_tsbOw"
      },
      "source": [
        "## LOAD MODEL "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VDiBcm-XEOb"
      },
      "outputs": [],
      "source": [
        "'''TEST'''\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "path_drive = '/content/drive/My Drive/'\n",
        "path = path_drive+'ProgettoDL/'\n",
        "\n",
        "path_model = os.path.join(path+'weights/model_{}_{}/Final'.format(immgs,cnn))\n",
        "\n",
        "\n",
        "'''\n",
        "Per evitare di fare compile=False, forse sarebbe da aggiungere metric bal_acc nel nostro caso --- da fare comunque perché non è corretto così \n",
        "model = load_model(model_path, custom_objects={'conditional_average_metric': conditional_average_metric, 'specificity': specificity, 'sensitivity': sensitivity})\n",
        "'''\n",
        "#model = load_model(path_model, compile=False)\n",
        "model = load_model(path_model, custom_objects={'balanced_accuracy_new':balanced_accuracy_new})\n",
        "\n",
        "print('Model IMG Loaded')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQMtb_JBD5Vp"
      },
      "source": [
        "## PREDICTION "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDb9juVJfFBc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from tensorflow.keras import Model\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "test_array = []\n",
        "test_array_series = []\n",
        "\n",
        "for index, row in test_balance_df.iterrows():\n",
        "    class_ = int(row['class'])\n",
        "    series_ = int(row['series'])          #---da qui e nei prossimi, calcolo la ground thruth del shotgun series, ovvero i semplici COD_COMPONENTE (serie) che appartengono al sub-set di test\n",
        "    test_array.append(class_)\n",
        "    test_array_series.append(series_)     #---\n",
        "\n",
        "test_array = np.array(test_array)\n",
        "test_array_series = np.array(test_array_series)   #---\n",
        "\n",
        "y_test = to_categorical(np.unique(test_array, return_inverse=True)[1])\n",
        "y_test_series = to_categorical(np.unique(test_array_series, return_inverse=True)[1])      #---\n",
        "\n",
        "imgs_array = [] \n",
        "\n",
        "\n",
        "for index, row in testgen.df.iterrows():\n",
        "    filename = row['filename'] \n",
        "    image = load_img(os.path.join(path_IMAGES,filename), target_size = (immg_rows, immg_cols))\n",
        "    x = img_to_array(image)\n",
        "    x = preprocess_input(x, data_format=\"channels_first\")  #non dovrebbe servire\n",
        "    imgs_array.append(x)\n",
        "    X_test = np.asarray(imgs_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FpZ8K4yZXZ7"
      },
      "outputs": [],
      "source": [
        "y_test_no_argmax = y_test\n",
        "y_test = y_test.argmax(axis=1)\n",
        "\n",
        "y_test_series = y_test_series.argmax(axis=1)      #---\n",
        "y_pred = model.predict(X_test)\n",
        "#---------------\n",
        "#metriche nuove senza usare argmax.\n",
        "#---------------\n",
        "y_pred_no_argmax = y_pred\n",
        "y_pred = np.argmax(y_pred,axis=1)\n",
        "\n",
        "print(y_pred.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(y_pred_no_argmax.shape)\n",
        "print(y_test_no_argmax)\n",
        "print(y_test_no_argmax.shape)\n",
        "\n",
        "#y_pred_conf = model.predict(X_test)\n",
        "#index = np.where(np.equal(y_pred, y_test) == False)[0]\n",
        "#print(np.around(y_pred_conf[index], decimals = 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzrVVF8NCQ5Q"
      },
      "outputs": [],
      "source": [
        "print(y_test)\n",
        "print(y_test.shape)\n",
        "print(y_test_series)\n",
        "print(y_test_series.shape)\n",
        "print(y_pred)\n",
        "print(y_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkaCQp4Ndzbe"
      },
      "source": [
        "##SEARCH UNIVOQUE SERIES TO BALANCE SETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkviN4oh2M6l"
      },
      "outputs": [],
      "source": [
        "#SEARCHING UNIVOQUE SERIES\n",
        "test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4, test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9, test_array_s10, test_array_s11, test_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4, pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9, pred_array_s10, pred_array_s11, pred_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "i=0\n",
        "for index, row in test_balance_df.iterrows():\n",
        "    \n",
        "    series_ = int(row['series'])\n",
        "    if series_ == 0:\n",
        "      test_array_s0.append(y_test[i])\n",
        "      pred_array_s0.append(y_pred[i])\n",
        "    if series_ == 1:\n",
        "      test_array_s1.append(y_test[i])\n",
        "      pred_array_s1.append(y_pred[i])\n",
        "    if series_ == 2:\n",
        "      test_array_s2.append(y_test[i])\n",
        "      pred_array_s2.append(y_pred[i])\n",
        "    if series_ == 3:\n",
        "      test_array_s3.append(y_test[i])\n",
        "      pred_array_s3.append(y_pred[i])\n",
        "    if series_ == 4:\n",
        "      test_array_s4.append(y_test[i])\n",
        "      pred_array_s4.append(y_pred[i])\n",
        "    if series_ == 5:\n",
        "      test_array_s5.append(y_test[i])\n",
        "      pred_array_s5.append(y_pred[i])\n",
        "    if series_ == 6:\n",
        "      test_array_s6.append(y_test[i])\n",
        "      pred_array_s6.append(y_pred[i])\n",
        "    if series_ == 7:\n",
        "      test_array_s7.append(y_test[i])\n",
        "      pred_array_s7.append(y_pred[i])\n",
        "    if series_ == 8:\n",
        "      test_array_s8.append(y_test[i])\n",
        "      pred_array_s8.append(y_pred[i])\n",
        "    if series_ == 9:\n",
        "      test_array_s9.append(y_test[i])\n",
        "      pred_array_s9.append(y_pred[i])\n",
        "    if series_ == 10:\n",
        "      test_array_s10.append(y_test[i])\n",
        "      pred_array_s10.append(y_pred[i])\n",
        "    if series_ == 11:\n",
        "      test_array_s11.append(y_test[i])\n",
        "      pred_array_s11.append(y_pred[i])\n",
        "    if series_ == 12:\n",
        "      test_array_s12.append(y_test[i])\n",
        "      pred_array_s12.append(y_pred[i])\n",
        "\n",
        "    i=i+1\n",
        "\n",
        "print(test_array_s0)\n",
        "print(pred_array_s0)\n",
        "\n",
        "from functools import reduce\n",
        "reduced = reduce(np.union1d, (pred_array_s0, test_array_s0))\n",
        "print(reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8WKlbpqKgAP"
      },
      "source": [
        "## METRICHE MASK & IMG "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHSjdjJcgc1Y"
      },
      "outputs": [],
      "source": [
        "'''METRICHE'''\n",
        "print('--------------Metrice IMG----------------')\n",
        "\n",
        "a = accuracy_score(y_test, y_pred)                                              # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
        "print(\"test accuracy: {0:0.4f}\".format(a))\n",
        "print(\"precision: {0:0.4f}\".format(precision_score(y_test, y_pred , average=\"macro\")))         # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html?highlight=precision_score#sklearn.metrics.precision_score\n",
        "print(\"recall: {0:0.4f}\".format(recall_score(y_test, y_pred , average=\"macro\")))                # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall_score#sklearn.metrics.recall_score\n",
        "print(\"f1_score: {0:0.4f}\".format(f1_score(y_test, y_pred , average=\"macro\")))                # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score\n",
        "\n",
        "print('classification report:')\n",
        "print(classification_report(y_test, y_pred))  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJjt39sGAfQF"
      },
      "source": [
        "Per quanto riguarda la funzione np_quadratic_weighted_kappa abbiamo avuto alcune difficoltà implementative e quindi abbiamo cercato un codice online che ci calcolasse la stessa metrica \n",
        "\n",
        "[Link Utilizzato](https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in2Vpij3AHgB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# The following 3 functions have been taken from Ben Hamner's github repository\n",
        "# https://github.com/benhamner/Metrics\n",
        "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the confusion matrix between rater's ratings\n",
        "    \"\"\"\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(rater_a + rater_b)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(rater_a + rater_b)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    conf_mat = [[0 for i in range(num_ratings)]\n",
        "                for j in range(num_ratings)]\n",
        "    for a, b in zip(rater_a, rater_b):\n",
        "        conf_mat[a - min_rating][b - min_rating] += 1\n",
        "    return conf_mat\n",
        "\n",
        "\n",
        "def histogram(ratings, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the counts of each type of rating that a rater made\n",
        "    \"\"\"\n",
        "    if min_rating is None:\n",
        "        min_rating = min(ratings)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(ratings)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    hist_ratings = [0 for x in range(num_ratings)]\n",
        "    for r in ratings:\n",
        "        hist_ratings[r - min_rating] += 1\n",
        "    return hist_ratings\n",
        "\n",
        "def quadratic_weighted_kappa(y, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the quadratic weighted kappa\n",
        "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    rater_a = y\n",
        "    rater_b = y_pred\n",
        "    min_rating=1 # era None abbiamo messo 0\n",
        "    max_rating=9 # era None abbiamo messo 9\n",
        "    rater_a = np.array(rater_a, dtype=int)\n",
        "    rater_b = np.array(rater_b, dtype=int)\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(min(rater_a), min(rater_b))\n",
        "    if max_rating is None:\n",
        "        max_rating = max(max(rater_a), max(rater_b))\n",
        "    conf_mat = Cmatrix(rater_a, rater_b,\n",
        "                                min_rating, max_rating)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return (1.0 - numerator / denominator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jydD-lsObD4e"
      },
      "outputs": [],
      "source": [
        "path_drive = '/content/drive/My Drive/'\n",
        "path = path_drive+'ProgettoDL/'\n",
        "\n",
        "os.chdir(path)\n",
        "\n",
        "from metrics import np_quadratic_weighted_kappa, minimum_sensitivity\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "#errore no graph before run \n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(y_true, y_pred, num_classes):\n",
        "\n",
        "  #run function minimum_sensitivity\n",
        "\n",
        "\n",
        "  # Calculate metric\n",
        "  sess = keras.backend.get_session()\n",
        "\n",
        "  #qwk = np_quadratic_weighted_kappa(np.argmax(y_true, axis=0), np.argmax(y_pred, axis=0), 0,\n",
        "\t#\t\t\t\t\t\t\t\t\tnum_classes - 1)\n",
        "  \n",
        "  qwk = quadratic_weighted_kappa(y_true, y_pred)\n",
        "  ms = minimum_sensitivity(y_test_no_argmax, y_pred_no_argmax)\n",
        "  mae = sess.run(K.mean(keras.losses.mean_absolute_error(y_test_no_argmax, y_pred_no_argmax)))\n",
        "  \n",
        "  metrics = {\n",
        "\t\t'QWK': qwk,\n",
        "\t\t'MS': ms,\n",
        "\t\t'MAE': mae}\n",
        "  \n",
        "  return metrics\n",
        "\n",
        "def print_metrics(metrics):\n",
        "\tprint('QWK: {:.4f}'.format(metrics['QWK']))\n",
        "\tprint('MS: {:.4f}'.format(metrics['MS']))\n",
        "\tprint('MAE: {:.4f}'.format(metrics['MAE']))    \n",
        "\n",
        "\n",
        "#-----codice------\n",
        "\n",
        "num_classi = 10\n",
        "metrics = compute_metrics(y_test, y_pred,num_classi)\n",
        "print_metrics(metrics)\n",
        "\n",
        "with open(\"metrics.txt\", \"w\") as text_file:\n",
        "    print(print_metrics, file=text_file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnlbf22bBpIt"
      },
      "source": [
        "***Metrice Ottenute***\n",
        "\n",
        "**K Cohen**   https://it.vvikipedla.com/wiki/Cohen%27s_kappa\n",
        "Il Kappa di Cohen è un coefficiente statistico che rappresenta il grado di accuratezza e affidabilità in una classificazione statistica; è un indice di concordanza che tiene conto della probabilità di concordanza casuale; l'indice calcolato in base al rapporto tra l'accordo in eccesso rispetto alla probabilità di concordanza casuale e l'eccesso massimo ottenibile. Attraverso la matrice di confusione è possibile valutare questo parametro. In particolare ... Esistono diversi \"gradi di concordanza\", in base ai quali possiamo definire se Kappa di Cohen è scarso o ottimo:\n",
        "\n",
        "- se k assume valori inferiori a 0, allora non c'è concordanza;\n",
        "- se k assume valori compresi tra 0-0,4, allora la concordanza è scarsa;\n",
        "- se k assume valori compresi tra 0,4-0,6, allora la concordanza è discreta;\n",
        "- se k assume valori compresi tra 0,6-0,8, la concordanza è buona;\n",
        "- se k assume valori compresi tra 0,8-1, la concordanza è ottima.\n",
        "\n",
        "**QWK**: 0.7849\n",
        "\n",
        "BLA BLA BLA \n",
        "\n",
        "**MS**: 1.0000\n",
        "\n",
        "\n",
        "In statistics, **mean absolute error (MAE)** is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. \n",
        "\n",
        "**MAE**: 0.0000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z25yiN0zKZ5Y"
      },
      "source": [
        "## PLOT CONFUSION MATRIX FUNCTION "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsYoP-3hgfY5"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix - CROP\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i in range (cm.shape[0]):\n",
        "      for j in range (cm.shape[1]):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    axes = plt.gca()\n",
        "    bottom, top = axes.get_ylim()\n",
        "    axes.set_ylim(bottom + 0.5, top - 0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0JFOHg2KUnT"
      },
      "source": [
        "## PLOT CONFUSION MATRIX "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpj28QpDKTkz"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "cm = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "fig = plot_confusion_matrix(cm,\n",
        "                      target_names = classi,\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix IMG \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_{}_{}.pdf'.format(immgs,cnn))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy4ylbat-r51"
      },
      "source": [
        "##PLOT CONFUSION MATRIX PER CIASCUNA SERIE DEL CALCIO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEruoZcb-q_S"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "from functools import reduce\n",
        "#serie 0\n",
        "cm0 = metrics.confusion_matrix(y_true=test_array_s0, y_pred=pred_array_s0)\n",
        "fig = plot_confusion_matrix(cm0,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s0, test_array_s0)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 0 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie0_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 1\n",
        "cm1 = metrics.confusion_matrix(y_true=test_array_s1, y_pred=pred_array_s1)\n",
        "fig = plot_confusion_matrix(cm1,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s1, test_array_s1)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 1 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie1_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 2\n",
        "cm2 = metrics.confusion_matrix(y_true=test_array_s2, y_pred=pred_array_s2)\n",
        "fig = plot_confusion_matrix(cm2,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s2, test_array_s2)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 2 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie2_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 3\n",
        "cm3 = metrics.confusion_matrix(y_true=test_array_s3, y_pred=pred_array_s3)\n",
        "fig = plot_confusion_matrix(cm3,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s3, test_array_s3)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 3 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie3_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 4\n",
        "cm4 = metrics.confusion_matrix(y_true=test_array_s4, y_pred=pred_array_s4)\n",
        "fig = plot_confusion_matrix(cm4,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s4, test_array_s4)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 4 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie4_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 5\n",
        "cm5 = metrics.confusion_matrix(y_true=test_array_s5, y_pred=pred_array_s5)\n",
        "fig = plot_confusion_matrix(cm5,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s5, test_array_s5)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 5 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie5_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 6\n",
        "cm6 = metrics.confusion_matrix(y_true=test_array_s6, y_pred=pred_array_s6)\n",
        "fig = plot_confusion_matrix(cm6,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s6, test_array_s6)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 6 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie6_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 7\n",
        "cm7 = metrics.confusion_matrix(y_true=test_array_s7, y_pred=pred_array_s7)\n",
        "fig = plot_confusion_matrix(cm7,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s7, test_array_s7)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 7 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie7_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 8\n",
        "cm8 = metrics.confusion_matrix(y_true=test_array_s8, y_pred=pred_array_s8)\n",
        "fig = plot_confusion_matrix(cm8,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s8, test_array_s8)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 8 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie8_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 9 \n",
        "cm9 = metrics.confusion_matrix(y_true=test_array_s9, y_pred=pred_array_s9)\n",
        "fig = plot_confusion_matrix(cm9,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s9, test_array_s9)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 9 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie9_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 10\n",
        "cm10 = metrics.confusion_matrix(y_true=test_array_s10, y_pred=pred_array_s10)\n",
        "fig = plot_confusion_matrix(cm10,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s10, test_array_s10)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 10 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie10_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 11\n",
        "cm11 = metrics.confusion_matrix(y_true=test_array_s11, y_pred=pred_array_s11)\n",
        "fig = plot_confusion_matrix(cm11,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s11, test_array_s11)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 11 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie11_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "\n",
        "cm12 = metrics.confusion_matrix(y_true=test_array_s12, y_pred=pred_array_s12)\n",
        "fig = plot_confusion_matrix(cm12,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s12, test_array_s12)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 12 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie12_{}_{}.pdf'.format(immgs,cnn))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUnAYcXVclfD"
      },
      "source": [
        "## CRAMER V CORRELATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqvgi3VKck3o"
      },
      "outputs": [],
      "source": [
        "#PRIMA VERSIONE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as ss\n",
        "import seaborn as sns\n",
        "\n",
        "def cramers_v(confusion_matrix):\n",
        "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
        "        uses correction from Bergsma and Wicher,\n",
        "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
        "    \"\"\"\n",
        "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    rcorr = r - ((r-1)**2)/(n-1)\n",
        "    kcorr = k - ((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
        "\n",
        "confusion_matrix = pd.crosstab(y_test, y_pred)\n",
        "print(\"cramer correlation tra predizioni delle classi, e le classi effettive\")\n",
        "cramer1 = cramers_v(confusion_matrix.values)\n",
        "print(cramer1)\n",
        "\n",
        "confusion_matrix2 = pd.crosstab(y_test_series, y_pred)\n",
        "print(\"cramer correlation tra predizioni delle classi e le ground thruth di shotgun series\")\n",
        "cramer2 = cramers_v(confusion_matrix2.values)\n",
        "print(cramer2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sZfKVKTmEdX"
      },
      "outputs": [],
      "source": [
        "#SECONDA VERSIONE.        https://www.youtube.com/watch?v=eTnLTJer_Oo\n",
        "contTable = pd.crosstab(y_test_series, y_pred)\n",
        "print(contTable)\n",
        "\n",
        "!pip install researchpy\n",
        "\n",
        "import researchpy\n",
        "\n",
        "crosstab, res = researchpy.crosstab(pd.Series(y_test_series), pd.Series(y_pred), test='chi-square')\n",
        "print(\"\\n{}\".format(res))\n",
        "\n",
        "df = min(contTable.shape[0], contTable.shape[1]) - 1\n",
        "print(\"\\ndf = {}\".format(df))\n",
        "\n",
        "V = res.iloc[2,1]\n",
        "print(\"V = {}\".format(V))\n",
        "\n",
        "if df == 1:\n",
        "    if V < 0.10:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.30:\n",
        "        qual = 'small'\n",
        "    elif V < 0.50:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "elif df == 2:\n",
        "    if V < 0.07:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.21:\n",
        "        qual = 'small'\n",
        "    elif V < 0.35:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "elif df == 3:\n",
        "    if V < 0.06:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.17:\n",
        "        qual = 'small'\n",
        "    elif V < 0.29:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "elif df == 4:\n",
        "    if V < 0.05:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.15:\n",
        "        qual = 'small'\n",
        "    elif V < 0.25:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "else:\n",
        "    if V < 0.05:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.13:\n",
        "        qual = 'small'\n",
        "    elif V < 0.22:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "\n",
        "print(\"\\nquality classification of the correlation is:   {}\".format(qual))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHab1JLzqkKx"
      },
      "source": [
        "To indicate the strength of the association between two nominal variables, Cramér's V (Cramér, 1946) is often used.\n",
        "\n",
        "As for the interpretation for Cramér's V various rules of thumb exist but one of them is from Cohen (1988, pp. 222, 224, 225) who let's the interpretation depend on the degrees of freedom, shown in the table below.\n",
        "\n",
        "|df*|negligible|small|medium|large|\n",
        "|-------|---|---|---|---|\n",
        "|1|0 < .10|.10 < .30|.30 < .50|.50 or more|\n",
        "|2|0 < .07|.07 < .21|.21 < .35|.35 or more|\n",
        "|3|0 < .06|.06 < .17|.17 < .29|.29 or more|\n",
        "|4|0 < .05|.05 < .15|.15 < .25|.25 or more|\n",
        "|5|0 < .05|.05 < .13|.13 < .22|.22 or more|\n",
        "\n",
        "The degrees of freedom (df*) is for Cramér's V the minimum of the number of rows, or number of columns, then minus one.\n",
        "\n",
        "Lets see how to obtain Cramér's V with Python, using an example.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**A SECONDA DEI RISULTATI E CONFRONTANDOLI CON LA TABELLA RIUSCIAMO A CAPIRE L'INTENSITA' DEL BIAS TRA DIVERSE VARIABILI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoegV_027yUO"
      },
      "source": [
        "## **T-SNE  & PCA**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7-u3Qf0YsFe"
      },
      "source": [
        "### Spiegazioni, Link Utili e Implementazione "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c964uCqMUpS4"
      },
      "source": [
        "***(t-SNE)*** t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, you may have to plot fewer exploratory data analysis plots next time you work with high dimensional data.\n",
        "\n",
        "[Link utile ](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n",
        "\n",
        "***(PCA) Principal Component Analysis***\n",
        "Lʹanalisi delle componenti principali (detta pure PCA oppure CPA) è una tecnica utilizzata nell’ambito della statistica multivariata per la semplificazione dei dati d’origine.\n",
        "Lo scopo primario di questa tecnica è la riduzione di un numero più o meno elevato di variabili (rappresentanti altrettante caratteristiche del fenomeno analizzato) in alcune variabili latenti. Ciò avviene tramite una trasformazione lineare delle variabili che proietta quelle originarie in un nuovo sistema cartesiano nel quale le variabili vengono ordinate in ordine decrescente di varianza: pertanto, la variabile con maggiore varianza viene proiettata sul primo asse, la seconda sul secondo asse e così via. La riduzione della complessità avviene limitandosi ad analizzare le principali (per varianza) tra le nuove variabili.\n",
        "Diversamente da altre trasformazioni (lineari) di variabili praticate nellʹambito della statistica, in questa tecnica sono gli stessi dati che determinano i vettori di trasformazione.\n",
        "[Step By Step](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n",
        "\n",
        "[Link Utile](https://www.analyticsvidhya.com/blog/2020/12/an-end-to-end-comprehensive-guide-for-pca/) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o8fXQZePP25"
      },
      "source": [
        "***Parametri del TSNE***\n",
        "1. **n_components** int, default=2 - Dimension of the embedded space.\n",
        "\n",
        "2. **perplexityfloat, default=30.0** - The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results.\n",
        "\n",
        "3. **early_exaggeration float, default=12.0**\n",
        "Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.\n",
        "\n",
        "4. **learning_ratefloat, default=200.0** The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.\n",
        "\n",
        "5. **n_iterint, default=1000**\n",
        "Maximum number of iterations for the optimization. Should be at least 250.\n",
        "\n",
        "6. **n_iter_without_progressint, default=300**\n",
        "Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.\n",
        "\n",
        "7. **metricstr or callable, default=’euclidean’**\n",
        "The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is “euclidean” which is interpreted as squared euclidean distance.\n",
        "\n",
        "8. **init{‘random’, ‘pca’} or ndarray of shape(n_samples, n_components), default=’random’**\n",
        "Initialization of embedding. Possible options are ‘random’, ‘pca’, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.\n",
        "\n",
        "9. **verboseint, default=0** Verbosity level.\n",
        "\n",
        "10. **random_stateint, RandomState instance or None, default=None** Determines the random number generator. Pass an int for reproducible results across multiple function calls. Note that different initializations might result in different local minima of the cost function. See :term: Glossary <random_state>.\n",
        "\n",
        "11. **methodstr, default=’barnes_hut’**\n",
        "By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.\n",
        "\n",
        "12. **n_jobsint, default=None**\n",
        "The number of parallel jobs to run for neighbors search. This parameter has no impact when metric=\"precomputed\" or (metric=\"euclidean\" and method=\"exact\"). None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
        "\n",
        "\n",
        "[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
        "\n",
        "[misread-tsne](https://distill.pub/2016/misread-tsne/)\n",
        "\n",
        "[altro modo spiegato anche meglio](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umabolL61nFw"
      },
      "source": [
        "#### Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp2BSeGZ0vij"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/ProgettoDL')\n",
        "path = os.getcwd()\n",
        "\n",
        "col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n",
        "dataframe_sx_complessivo = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n",
        "\n",
        "\n",
        "col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n",
        "dataframe_dx_complessivo = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n",
        "\n",
        "\n",
        "dataframe_sx_complessivo.columns = ['ID','series', 'filename', 'class']\n",
        "dataframe_dx_complessivo.columns = ['ID','series', 'filename', 'class']\n",
        "\n",
        "#print(dataframe_sx.columns)                 #stampo i due elementi con stesso ID (lato dx e sx di stesso CALCIO)\n",
        "frames = [dataframe_sx_complessivo, dataframe_dx_complessivo]\n",
        "result_complessivo = pd.concat(frames)\n",
        "#print(result_complessivo)\n",
        "#print(result_complessivo.loc[[1]])\n",
        "#print(type(result_complessivo.loc[[1]]))\n",
        "\n",
        "result_complessivo[\"class\"] = result_complessivo[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n",
        "result_complessivo[\"series\"] = result_complessivo[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)})\n",
        "\n",
        "#IDENTIFICAZIONE VALORI NULL \n",
        "print(\"Null VALUE di class : \"+format(result_complessivo['class'].isnull().sum()))\n",
        "print(result_complessivo.loc[result_complessivo['class'] == '0'])\n",
        "print(result_complessivo[result_complessivo['class'].isnull()])\n",
        "result_complessivo['class'] = pd.to_numeric(result_complessivo['class'], errors='coerce')\n",
        "print(result_complessivo[result_complessivo['class'].isnull()])\n",
        "result_complessivo = result_complessivo.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n",
        "print(result_complessivo[result_complessivo['class'].isnull()])\n",
        "\n",
        "print(\"Null VALUE di class : \"+format(result_complessivo['class'].isnull().sum()))\n",
        "\n",
        "#IMMG EXIST ?  (cerco se qualche path non esiste e lo elimino dal dataframe) e se esiste ne faccio la MASCHERA\n",
        "import os.path\n",
        "from os import path\n",
        "os.chdir('/content/drive/MyDrive/CALCIO_NOPRE')\n",
        "for index, row in result_complessivo.iterrows():\n",
        "    filename = row['filename']\n",
        "    if(os.path.exists(filename) == False):\n",
        "      result_complessivo = result_complessivo.drop(result_complessivo[(result_complessivo['filename'] == filename)].index)\n",
        "      print('File : {} eliminato'.format(filename))\n",
        "\n",
        "print('------------------- DATASET BASE ---------------')\n",
        "print(type(result_complessivo))  \n",
        "print(len(result_complessivo))\n",
        "print(result_complessivo)\n",
        "\n",
        "result_complessivo_totale = pd.DataFrame()\n",
        "\n",
        "for index, row in result_complessivo.iterrows():\n",
        "  #filename_mask = 'mask_{}'.format(row['filename'])\n",
        "  filename = '{}'.format(row['filename'])\n",
        "  #filename_gray = 'gray_{}'.format(row['filename'])\n",
        "  class_ = row['class']\n",
        "  series_ = row['series']\n",
        "  #print('{}_{}_{}_{}'.format(filename_gray,filename_mask, class_, series_)) \"ID\": row['ID']\n",
        "  row_df_1 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename, \"class\" : class_},index=[0])\n",
        "  #row_df_2 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename_gray, \"class\" : class_},index=[0])\n",
        "  #row_df_3 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename, \"class\" : class_},index=[0])\n",
        "  #print(row_df_1)\n",
        "  #print(row_df_2)\n",
        "  result_complessivo_totale = result_complessivo_totale.append(row_df_1)\n",
        "  #result_complessivo_totale = result_complessivo_totale.append(row_df_2)\n",
        "  #result_complessivo_totale = result_complessivo_totale.append(row_df_3)\n",
        "\n",
        "\n",
        "print('------------------- DATASET COMPLESSIVO ---------------') \n",
        "print(type(result_complessivo_totale))  \n",
        "print(len(result_complessivo_totale))\n",
        "#print(result_complessivo_totale)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "result_complessivo_totale = shuffle(result_complessivo_totale)\n",
        "print(type(result_complessivo_totale))  \n",
        "print(len(result_complessivo_totale))\n",
        "print(result_complessivo_totale)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U2UEFjs1iXp"
      },
      "source": [
        "#### import utili per il TSNE e PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvdYrmbf1aYd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd    \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z56gEZfX14mX"
      },
      "source": [
        "#### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qiUkpP0_LXM"
      },
      "source": [
        "##### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini - QUALITY CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7t1a9DA0wnE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
        "from tqdm import tqdm\n",
        "immg_rows = 270 \n",
        "immg_cols = 470\n",
        "X = [] \n",
        "imgs_array_tot = []\n",
        "\n",
        "data_X = result_complessivo_totale['filename'][:1000] #---versione originale \n",
        "result_complessivo_totale_min = result_complessivo_totale[:1000] #--deve essere uguale a y_dim ---versione originale \n",
        "y = result_complessivo_totale['class'][:1000] #--- deve essere uguale ... ---versione originale \n",
        "\n",
        "for index, row in tqdm(result_complessivo_totale_min.iterrows()):\n",
        "    filename = row['filename']\n",
        "    image = load_img('/content/drive/My Drive/CALCIO_CROP_BASE/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n",
        "    \n",
        "    #print('Originale : {} x {} x {}'.format(image.size[0], image.size[1], len(image.size)-1))\n",
        "    #plt.imshow(image)\n",
        "    scale_percent = 90 # percent of original size\n",
        "    width, height = image.size\n",
        "    #print('channel : {}'.format(len(image.size)))\n",
        "    width = int(width * scale_percent / 100)\n",
        "    height = int(height * scale_percent / 100)\n",
        "    dim = (width, height)\n",
        "    # resize image\n",
        "    x = img_to_array(image)\n",
        "    resized = cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n",
        "    #print('Ridimensionata : {}'.format((resized.shape)))\n",
        "    #print('Resized Dimensions : ',resized.shape)\n",
        "    imgs_array_tot.append(resized)\n",
        "    X = np.asarray(imgs_array_tot)\n",
        "print(X.shape)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsDgmRB_QxY"
      },
      "source": [
        "##### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini - SHOTGUN SERIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WGaPWme_Ca8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
        "from tqdm import tqdm\n",
        "\n",
        "immg_rows = 270 \n",
        "immg_cols = 470\n",
        "X = [] \n",
        "imgs_array_tot = []\n",
        "data_X = result_complessivo_totale['filename'][:1000]\n",
        "\n",
        "result_complessivo_totale_min = result_complessivo_totale[:1000] #--deve essere uguale a y_dim\n",
        "\n",
        "y = result_complessivo_totale['class'][:1000] #--- deve essere uguale ... \n",
        "\n",
        "y_series = result_complessivo_totale['series'][:1000] #--- deve essere uguale ...\n",
        "\n",
        "for index, row in tqdm(result_complessivo_totale_min.iterrows()):\n",
        "    filename = row['filename']\n",
        "    if(filename[0] == 'm'):\n",
        "      image = load_img('/content/drive/My Drive/MASK_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n",
        "    elif(filename[0] == 'g'): \n",
        "      image = load_img('/content/drive/My Drive/GRAY_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n",
        "    else:\n",
        "      image = load_img('/content/drive/My Drive/CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n",
        "\n",
        "    scale_percent = 90 # percent of original size\n",
        "    width, height = image.size\n",
        "    width = int(width * scale_percent / 100)\n",
        "    height = int(height * scale_percent / 100)\n",
        "    dim = (width, height)\n",
        "    # resize image\n",
        "    x = img_to_array(image)\n",
        "    resized = cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n",
        "    #print('Ridimensionata : {}'.format((resized.shape)))\n",
        "    #print('Resized Dimensions : ',resized.shape)\n",
        "    imgs_array_tot.append(resized)\n",
        "    X2 = np.asarray(imgs_array_tot)\n",
        "print(X2.shape) \n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihY4u-CL2JKI"
      },
      "source": [
        "#### Check & Create Dataframe for PCA (Principal Analysis Component) & T-SNE (t-distributed stochastic neighbor embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OaloopJ5gCK"
      },
      "source": [
        "##### classi di qualità "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niWaWydy2H9P"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "print('X SHAPE : {}'.format(X.shape))\n",
        "\n",
        "nsamples = X.shape[0]\n",
        "rows = X.shape[1]\n",
        "cols = X.shape[2]\n",
        "channel = 1\n",
        "\n",
        "print('n_samples : {} , rows : {} , cols : {} , channel : {} '.format(nsamples, rows, cols, channel))\n",
        "print(type(X))\n",
        "X_1 = np.reshape(X, (X.shape[0],rows*cols*channel)) #-- serve per modificare la dimensione, per il fit_transform          FORSE QUI BISOGNA SOLO USARE I PRIMI 2 VALORI E IL 3 DEI CANALI NO!\n",
        "\n",
        "print('X MODIFICATO : {}'.format(X_1.shape)) #--- controllo se ho fatto tutto correttamente \n",
        "\n",
        "feat_cols = [ 'pixel'+str(i) for i in range(X_1.shape[1]) ]\n",
        "print('Feat Cols : {} '.format(len(feat_cols)))\n",
        "#print(feat_cols)\n",
        "df = pd.DataFrame(X_1,columns=feat_cols)\n",
        "#df = pd.DataFrame(X_1)\n",
        "df['y'] = pd.DataFrame({ 'y': np.array(y) })\n",
        "df['label'] = df['y'].apply(lambda i: str(i))\n",
        "#X, y = None, None\n",
        "print('Size of the dataframe: {}'.format(df.shape))\n",
        "\n",
        "# For reproducability of the results\n",
        "np.random.seed(42)\n",
        "rndperm = np.random.permutation(df.shape[0])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrtXkNtk5luP"
      },
      "source": [
        "##### shotgun series "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRob1Q0F5buA"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "print('X2 SHAPE : {}'.format(X2.shape))\n",
        "\n",
        "nsamples = X2.shape[0]\n",
        "rows = X2.shape[1]\n",
        "cols = X2.shape[2]\n",
        "channel = 1\n",
        "\n",
        "print('n_samples : {} , rows : {} , cols : {} , channel : {} '.format(nsamples, rows, cols, channel))\n",
        "print(type(X2))\n",
        "X_11 = np.reshape(X2, (X2.shape[0],rows*cols*channel)) #-- serve per modificare la dimensione, per il fit_transform          FORSE QUI BISOGNA SOLO USARE I PRIMI 2 VALORI E IL 3 DEI CANALI NO!\n",
        "\n",
        "print('X MODIFICATO : {}'.format(X_11.shape)) #--- controllo se ho fatto tutto correttamente \n",
        "#print(X_1)\n",
        "\n",
        "feat_cols = [ 'pixel'+str(i) for i in range(X_11.shape[1]) ]\n",
        "print('Feat Cols : {} '.format(len(feat_cols)))\n",
        "#print(feat_cols)\n",
        "df_2 = pd.DataFrame(X_11,columns=feat_cols)\n",
        "#df = pd.DataFrame(X_1)\n",
        "df_2['y'] = pd.DataFrame({ 'y': np.array(y_series) })\n",
        "df_2['label'] = df_2['y'].apply(lambda i: str(i))\n",
        "#X, y = None, None\n",
        "print('Size of the dataframe: {}'.format(df_2.shape))\n",
        "\n",
        "\n",
        "\n",
        "# For reproducability of the results\n",
        "np.random.seed(42)\n",
        "rndperm = np.random.permutation(df_2.shape[0])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbq6tUyY2bxr"
      },
      "source": [
        "#### Calcolo TSNE & PLOT TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUPCoiGQ6j-D"
      },
      "source": [
        "##### TSNE QUALITY CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg-1zy_g2bDc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "time_start = time.time()\n",
        "N = 1000 \n",
        "df_subset = df.loc[rndperm[:N],:].copy()\n",
        "data_subset = df_subset[feat_cols].values\n",
        "#data_subset = df_subset\n",
        "#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... originale \n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=200, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n",
        "tsne_results = tsne.fit_transform(data_subset)\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9nnpLbE6sfN"
      },
      "source": [
        "##### TSNE SHOTGUN SERIES "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5Ht1Qoc6rtg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "time_start = time.time()\n",
        "N = 1000\n",
        "df_subset_series = df_2.loc[rndperm[:N],:].copy()\n",
        "#data_subset_series = df_subset_series\n",
        "data_subset_series = df_subset_series[feat_cols].values\n",
        "#tsne_series = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... \n",
        "tsne_series = TSNE(n_components=2, verbose=1, perplexity=5, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n",
        "tsne_results_series = tsne_series.fit_transform(data_subset_series)\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdicUGi28Sr_"
      },
      "source": [
        "##### PLOT TSNE QUALITY CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXkTZdoA3Thu"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
        "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.scatterplot(\n",
        "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "    hue=\"y\",\n",
        "    palette=sns.color_palette('Paired', as_cmap = True),\n",
        "    data=df_subset,\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WOsA8la8b6O"
      },
      "source": [
        "##### TSNE PLOT SHOTGUN SERIES "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOdq6FA08f4b"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "df_subset_series['tsne-2d-one'] = tsne_results_series[:,0]\n",
        "df_subset_series['tsne-2d-two'] = tsne_results_series[:,1]\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.scatterplot(\n",
        "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "    hue=\"y\",\n",
        "    palette=sns.color_palette('Paired', as_cmap = True),\n",
        "    data=df_subset_series,\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSi_lI_xuvO"
      },
      "source": [
        "## **Metriche Nuove**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7H7AQn_u3pW"
      },
      "source": [
        "## Alcune Definizioni \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **True Positives** (TP): Items where the true label is positive and whose class is correctly predicted to be positive.\n",
        "*  **False Positives** (FP): Items where the true label is negative and whose class is incorrectly predicted to be positive\n",
        "*  **True Negatives** (N): Items where the true label is negative and whose class is correctly predicted to be negative.\n",
        "*  **False Negatives** (FN): Items where the true label is positive and whose class is incorrectly predicted to be negative.\n",
        "\n",
        "* **False Positive Rate**, or *Type I Error*: Number of items wrongly identified as positive out of the total actual negatives — FP/(FP+TN) - This error means that an image not containing a particular parasite egg is incorrectly labeled as having it\n",
        "* **False Negative Rate**, or *Type II Error*: Number of items wrongly identified as negative out of the total actual positives — FN/(FN+TP). This metric is especially important to us, as it tells us the frequency with which a particular parasite egg is not classified correctly\n",
        "\n",
        "-------------\n",
        "\n",
        "* **Statistical Parity Difference**\n",
        "This measure is based on the following formula :\n",
        "𝑃𝑟(𝑌=1|𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑)−𝑃𝑟(𝑌=1|𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑) Here the bias or statistical imparity is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1. So it has to be close to 0 so it will be fair.\n",
        "\n",
        "*  **Equal Opportunity Difference** This metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula - 𝑇𝑃𝑅𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑−𝑇𝑃𝑅𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑 Same as the previous metric we need it to be close to 0.\n",
        "\n",
        "* **demographic parity** A fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n",
        "\n",
        "* **equality of opportunity** A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership."
      ],
      "metadata": {
        "id": "Yv_e2AQQPoXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CODICE METRICHE NUOVE"
      ],
      "metadata": {
        "id": "-pMObaKYPq9B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7dJJO7ixuJ7"
      },
      "outputs": [],
      "source": [
        "!pip install fairlearn \n",
        "from fairlearn.metrics import selection_rate\n",
        "from fairlearn.metrics import true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate\n",
        "from fairlearn.metrics import equalized_odds_difference\n",
        "\n",
        "import sklearn as sk\n",
        "\n",
        "\n",
        "#---- metriche lisa ----#\n",
        "y_true = testgen.df['class'].to_numpy()\n",
        "SR = selection_rate(y_true, y_pred, pos_label=1, sample_weight=None)\n",
        "print('selection_rate : {}' . format(SR))\n",
        "\n",
        "\n",
        "#Per quanto riguarda AO come metrica, potremo utilizzare i risultati della confusion matrix ?\n",
        "#LINK : https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "#LINK : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n",
        "#print('Unique Element Y_test : {}'.format(np.unique(y_test)))\n",
        "#print('Unique Element Y_pred : {}'.format(np.unique(y_pred)))\n",
        "#print('True_Positive_Rate : {}'.format(true_positive_rate(y_true, y_pred)))\n",
        "\n",
        "FP = cm.sum(axis=0) - np.diag(cm)  \n",
        "FN = cm.sum(axis=1) - np.diag(cm)\n",
        "TP = np.diag(cm)\n",
        "TN = cm.sum() - (FP + FN + TP)\n",
        "\n",
        "# Sensitivity, hit rate, recall, or true positive rate\n",
        "TPR = TP/(TP+FN)\n",
        "print('TPR : {}'.format(TPR))\n",
        "# Specificity or true negative rate\n",
        "TNR = TN/(TN+FP) \n",
        "print('TNR : {}'.format(TNR))\n",
        "# Precision or positive predictive value\n",
        "PPV = TP/(TP+FP)\n",
        "print('PPV : {}'.format(PPV))\n",
        "# Negative predictive value\n",
        "NPV = TN/(TN+FN)\n",
        "print('NPV : {}'.format(NPV))\n",
        "# Fall out or false positive rate\n",
        "FPR = FP/(FP+TN)\n",
        "print('FPR : {}'.format(FPR))\n",
        "# False negative rate\n",
        "FNR = FN/(TP+FN)\n",
        "print('FNR : {}'.format(FNR))\n",
        "# False discovery rate\n",
        "FDR = FP/(TP+FP)\n",
        "print('FDR : {}'.format(FDR))\n",
        "\n",
        "# Overall accuracy\n",
        "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "print('Accuracy : {}'.format(ACC))\n",
        "\n",
        "\n",
        "AO = 0.5*(\n",
        "    (TPR[0] + FPR[0]) - \n",
        "    (TPR[1] + FPR[1]) + \n",
        "    (TPR[2] + FPR[2]) - \n",
        "    (TPR[3] + FPR[3]) +\n",
        "    (TPR[4] + FPR[4]) -\n",
        "    (TPR[5] + FPR[5]) +\n",
        "    (TPR[6] + FPR[6]) -\n",
        "    (TPR[7] + FPR[7]) +\n",
        "    (TPR[8] + FPR[8]) -\n",
        "    (TPR[9] + FPR[9]))\n",
        "\n",
        "print('AO : {}'.format(AO))\n",
        "#y_true= y_true.reshape(1,-1)\n",
        "#y_pred= y_pred.reshape(-1,1)\n",
        "#print(y_true.shape)\n",
        "#print(y_pred.shape)\n",
        "\n",
        "\n",
        "'''FORSE QUA RIUSCIAMO A TROVARE UN ESEMPIO DI APPLICAZIONE DEL METODO'''\n",
        "'''https://deepnote.com/@Machine-Learning-2/Miniproject-z523fGqWSSu7QV34n_u7OA'''\n",
        "'''https://fairlearn.org/main/user_guide/assessment.html'''\n",
        "\n",
        "\n",
        "EO =(TPR[0] - TPR[1] + TPR[2] - TPR[3] + TPR[4] - TPR[5] + TPR[6] - TPR[7] + TPR[8] - FPR[9]) \n",
        "print('EO : {}' . format(EO))\n",
        "\n",
        "\n",
        "#Demographic parity\n",
        "'''\n",
        "Demographic parity is one of the most popular fairness indicators in the literature. \n",
        "Demographic parity is achieved if the absolute number of positive predictions \n",
        "in the subgroups are close to each other. This measure does not take true class into\n",
        "consideration and only depends on the model predictions. In some literature, \n",
        "demographic parity is also referred to as statsictal parity or independence.\n",
        "'''\n",
        "DP = (TP + FP)\n",
        "print('Demographic parity : {}' . format(DP))\n",
        "\n",
        "#Equalized odds\n",
        "'''\n",
        "Equalized odds, also known as separation, are achieved if the sensitivities in the \n",
        "subgroups are close to each other. The group-specific sensitivities \n",
        "indicate the number of the true positives divided by the total \n",
        "number of positives in that group.\n",
        "'''\n",
        "Equalized_Odds = TP / (TP + FN)\n",
        "print('Equalized Odds : {}' . format(Equalized_Odds))\n",
        "\n",
        "\n",
        "##---- Link Riccardo ----##\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n",
        "\n",
        "\n",
        "Balanced_Accuracy = sk.metrics.balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n",
        "print('Balanced Accuracy Generale : {0:0.4f}' . format(Balanced_Accuracy))\n",
        "\n",
        "\n",
        "#####----------- PER CIASCUNA SERIE BALANCED ACCURACY -----------####\n",
        "test_array_series = [\n",
        "                     test_array_s0, \n",
        "                     test_array_s1,\n",
        "                     test_array_s2,\n",
        "                     test_array_s3,\n",
        "                     test_array_s4,\n",
        "                     test_array_s5,\n",
        "                     test_array_s6,\n",
        "                     test_array_s7,\n",
        "                     test_array_s8,\n",
        "                     test_array_s9,\n",
        "                     test_array_s10,\n",
        "                     test_array_s11,\n",
        "                     test_array_s12\n",
        "                     ]\n",
        "\n",
        "pred_array_series = [\n",
        "                     pred_array_s0, \n",
        "                     pred_array_s1,\n",
        "                     pred_array_s2,\n",
        "                     pred_array_s3,\n",
        "                     pred_array_s4,\n",
        "                     pred_array_s5,\n",
        "                     pred_array_s6,\n",
        "                     pred_array_s7,\n",
        "                     pred_array_s8,\n",
        "                     pred_array_s9,\n",
        "                     pred_array_s10,\n",
        "                     pred_array_s11,\n",
        "                     pred_array_s12\n",
        "                     ]\n",
        "\n",
        "SUM_BA = 0\n",
        "for ba_i in range (13):\n",
        "  BA = sk.metrics.balanced_accuracy_score(test_array_series[ba_i], pred_array_series[ba_i], sample_weight=None, adjusted=False)\n",
        "  SUM_BA += BA\n",
        "  print('Balanced Accuracy Series {0} : {1:0.4f}' . format(ba_i, BA))\n",
        "\n",
        "\n",
        "#----------- MEDIA DELLE BALANCED ACCURACY ---------------\n",
        "Average = SUM_BA/12\n",
        "print('Average Balanced Accuracy : {0:0.4f}' . format(Average))\n",
        " \n",
        "\n",
        "##---- Wodsworth et Al ----# \n",
        "#HIGH_RISK_GAP = SP #modulo o cardinalità \n",
        "\n",
        "#FN_GAP = false_negative (s1) - false negative (s2) \n",
        "#FN_GAP = (false_negative_rate(y_true, y_pred) - false_negative_rate(y_true, y_pred))  #modulo o cardinalità\n",
        "  \n",
        "#FN_GAP = false_negative (s1) - false negative (s2) \n",
        "#FP_GAP = (false_positive_rate(y_true, y_pred) - false_positive_rate(y_true, y_pred))  #modulo o cardinalità\n",
        "\n",
        "\n",
        "\n",
        "### LINK UTILE ####\n",
        "#https://www.kaggle.com/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zUvw3mwvfCMA",
        "YEzVkjhHbISE",
        "xuS19wSXbdE8",
        "28i0TbSpzt_Z",
        "2XV_nhqtTWxr",
        "VX2SB5uYzzyB",
        "VQsYqmoQVJIu",
        "Q7UtR8q43GP-",
        "26Za2K_yFljo",
        "Ks3Fly2oUf3g",
        "-gwtHXxECiTK",
        "bfJ3LnbAZhQx",
        "xBGTOuc1VSqq",
        "QjI6Y_nefFTj",
        "XefnmV_tsbOw",
        "VkaCQp4Ndzbe",
        "z25yiN0zKZ5Y",
        "yy4ylbat-r51",
        "TUnAYcXVclfD",
        "NoegV_027yUO",
        "umabolL61nFw",
        "5aSi_lI_xuvO",
        "u7H7AQn_u3pW",
        "-pMObaKYPq9B"
      ],
      "machine_shape": "hm",
      "name": "TensorFlow - architettura base (D.A. Offline).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}