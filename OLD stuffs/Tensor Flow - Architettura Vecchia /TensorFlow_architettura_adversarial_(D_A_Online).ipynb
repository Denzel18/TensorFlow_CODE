{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TensorFlow - architettura adversarial (D.A. Online).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denzel18/Tensorflow_Architecture_CODE/blob/main/TensorFlow_architettura_adversarial_(D_A_Online).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1Fs0GL_mVgn"
      },
      "source": [
        "# ***Tensorflow Architettura Adversarial (D.A. Online)  - A. Giacomini & D. Bernovschi***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Parametri Test (CROP e NO CROP) "
      ],
      "metadata": {
        "id": "hELNaRu1q_dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_IMAGES = '/content/drive/MyDrive/CALCIO_CROP_BASE/'\n",
        "#path_IMAGES = '/content/drive/MyDrive/CALCIO_NOPRE/'\n",
        "\n",
        "'''some useful parameters and variables'''\n",
        "parte = 'CALCIO'\n",
        "tipo = 'CROP' #CROP, CROP_gray_ridge\n",
        "\n",
        "#Ricordati di cambiare anche gli Hyper Parameters !!!!"
      ],
      "metadata": {
        "id": "JP-_6Trkq_0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox6IFmrBfpP-"
      },
      "source": [
        "## IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwFSXnnkfot7"
      },
      "source": [
        "'''IMPORTING LIBRARIES'''\n",
        "'''Import packages &libraries for all the rest of the code'''\n",
        "import sys\n",
        "import subprocess\n",
        "if 'google.colab' in sys.modules:\n",
        "  subprocess.call(\"pip install -U progress\".split()) \n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import scipy.ndimage\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import sklearn as sk\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import optimizers, layers\n",
        "from tensorflow.keras.layers import Activation, Input, Conv2D, ZeroPadding2D, MaxPooling2D, UpSampling2D, concatenate, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from PIL import Image, ImageOps\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from random import randrange\n",
        "print('Tensor Flow {}'.format(tf.__version__))\n",
        "print('Keras {}'.format(tf.keras.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''vecchio costrutto'''\n",
        "#random.seed( 40 )\n",
        "\n",
        "'''nuovo costrutto'''\n",
        "def fix_seeds(seed: int) -> None:\n",
        "  \"\"\" Fix random seeds for numpy, tensorflow, random, etc.\n",
        "\n",
        "  Parameters\n",
        "  -----------\n",
        "  seed : int.\n",
        "  Random seed.\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(seed) # numpy seed\n",
        "  tf.random.set_seed(seed) # tensorflow seed\n",
        "  random.seed(seed) # random seed\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
        "  os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "fix_seeds(40)"
      ],
      "metadata": {
        "id": "I2ds7Z-yrLXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUvw3mwvfCMA"
      },
      "source": [
        "## DRIVE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkWUwK_IfBtx"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path_drive = '/content/drive/My Drive/'\n",
        "path = path_drive+'ProgettoDL/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEzVkjhHbISE"
      },
      "source": [
        "## Parametri Immagini "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSyxD4LWbG3J"
      },
      "source": [
        "'''DEFINE VARIABLES AND PARAMETERS TO COLLECT THE INFORMATIONS FROM GOOGLE DRIVE'''\n",
        "'''define a path for the collection of informations (CSV file) for the creation of the dataframe'''\n",
        "os.chdir('/content/drive/MyDrive/ProgettoDL/') \n",
        "\n",
        "'''to have always the same sequence of randomized values (numbers)'''\n",
        "random_state = 3  \n",
        "\n",
        "'''some useful parameters and variables'''\n",
        "augment = True\n",
        "metaclassi = False\n",
        "cnn = \"vgg16\" \n",
        "\n",
        "'''series of production & quality classes of the wood rifle butt'''\n",
        "classi = ['1','2-','2','2+','3-','3','3+','4-','4','4+']          \n",
        "serie = [2,4,8,10,6,9,3,11,12,13,14,15,7] \n",
        "cod_componente = [ 2,  4,  8, 10,  6,  9,  3, 11, 12, 13, 14, 15,  7]\n",
        "\n",
        "'''size of the images & their paths (location) '''\n",
        "immg_rows = 270 \n",
        "immg_cols = 470\n",
        "immgs = '{}_{}'.format(parte,tipo)\n",
        "path_imgs = os.path.join(path_drive+'{}'.format(immgs))\n",
        "\n",
        "'''CSV loading (reading annotations/attributes/informations)'''\n",
        "csv = pd.read_csv(('/content/drive/MyDrive/ProgettoDL/20201102_ExportDB.txt'), sep=\";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuS19wSXbdE8"
      },
      "source": [
        "## SPLIT DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0JSykxbbeCU"
      },
      "source": [
        "'''CUSTOM SPLIT DATA INTO TRAIN/TEST/VALIDATION SETS'''\n",
        "\"\"\"\n",
        "NOTE\n",
        "- VERSIONE CON NUMERI PRESI DIRETTAMENTE DAL BILANCIAMENTO CALCOLATO RISPETTO IL TOTALE DI 2120 (che ci sono in questo progetto), PER RENDERLO DINAMICO CALCOLARE PESI IN MODO AUTOMATICO (STUDIA ALTERNATIVA)\n",
        "- UNICO PROBLEMA È CHE A VOLTE IMMAGINI CON STESSO ID HANNO IN REALTÀ DIVERSA CLASSE DI QUALITÀ, QUINDI I DATASET NON SONO PERFETTAMENTE BILANCIATI MA VARIANO LEGGERMENTE,\n",
        "(perchè lo stesso ID deve stare in stesso set anche se i lati del calcio del fucile possono avere qualità differente)\n",
        "\"\"\"\n",
        "\n",
        "'''split method'''\n",
        "def split_data(dataframe_result, val_size, test_size, random_state):\n",
        "  #n_ immagini per ciascuna classe di qualità\n",
        "  classes_count = dataframe_result.groupby(['class']).size() \n",
        "\n",
        "  unique_result, counts = np.unique(dataframe_result['ID'], return_counts=True)   #conto quanti ID univoci esistono nel dataset e li raccolgo tutti in vettore\n",
        "\n",
        "  '''randomizing the order of the IDs, (to change the sequence change the random_state)'''\n",
        "  #id_perm = unique_result.iloc[np.random.permutation(unique_result.index)].reset_index(drop=True)\n",
        "  id_perm = np.random.RandomState(random_state).permutation(unique_result)\n",
        "  #print('ID Perm : {}'.format(id_perm))\n",
        "               \n",
        "  '''define finals sub-sets of data'''\n",
        "  column_names = ['ID','series','filename','class']\n",
        "  x_train = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  x_test = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  x_val = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  '''define variables to count elements inside the sub-sets'''\n",
        "  conta, conta0, conta1, conta2, conta3, conta4, conta5, conta6, conta7, conta8, conta9 = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
        "  class_ = 0\n",
        "\n",
        "\n",
        "  '''performing cycles to divide the images into the 3 sub-sets'''\n",
        "  for i in id_perm:\n",
        "     result_ID = dataframe_result.loc[(dataframe_result['ID'] == i)]\n",
        "      #print('Size : {} '.format(result_ID[result_ID.columns[0]].count()))\n",
        "     if result_ID[result_ID.columns[0]].count() == 2:\n",
        "       row_1=result_ID.iloc[0]\n",
        "       class_ = int(row_1['class'])\n",
        "       row_2=result_ID.iloc[1]\n",
        "       class2_ = int(row_2['class'])\n",
        "       conta = 2\n",
        "       #print(\"ID doppio\")\n",
        "     else:\n",
        "       row_1=result_ID.iloc[0]\n",
        "       class_ = int(row_1['class'])\n",
        "       conta = 1\n",
        "       #print(\"ID singolo\")\n",
        "\n",
        "     if class_ == 0 and conta0 < int((classes_count[0]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta0 = conta0 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta0 = conta0 + 1\n",
        "     elif class_ == 1 and conta1 < int((classes_count[1]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta1 = conta1 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta1 = conta1 + 1\n",
        "     elif class_ == 2 and conta2 < int((classes_count[2]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta2 = conta2 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta2 = conta2 + 1\n",
        "     elif class_ == 3 and conta3 < int((classes_count[3]/100)*60) :\n",
        "        if conta == 2 :\n",
        "          x_train=x_train.append(row_1, ignore_index=True) \n",
        "          x_train=x_train.append(row_2, ignore_index=True)\n",
        "          conta3 = conta3 + 2\n",
        "        else :\n",
        "          x_train=x_train.append(row_1, ignore_index=True) \n",
        "          conta3 = conta3 + 1\n",
        "     elif class_ == 4 and conta4 < int((classes_count[4]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta4 = conta4 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True)\n",
        "         conta4 = conta4 + 1 \n",
        "     elif class_ == 5 and conta5 < int((classes_count[5]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta5 = conta5 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta5 = conta5 + 1\n",
        "     elif class_ == 6 and conta6 < int((classes_count[6]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta6 = conta6 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta6 = conta6 + 1\n",
        "     elif class_ == 7 and conta7 < int((classes_count[7]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta7 = conta7 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta7 = conta7 + 1\n",
        "     elif class_ == 8 and conta8 < int((classes_count[8]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta8 = conta8 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta8 = conta8 + 1\n",
        "     elif class_ == 9 and conta9 < int((classes_count[9]/100)*60) :\n",
        "       if conta == 2 :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         x_train=x_train.append(row_2, ignore_index=True)\n",
        "         conta9 = conta9 + 2\n",
        "       else :\n",
        "         x_train=x_train.append(row_1, ignore_index=True) \n",
        "         conta9 = conta9 + 1\n",
        "     elif class_ == 0 and conta0 >= int((classes_count[0]/100)*60) and conta0 < int((classes_count[0]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta0 = conta0 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta0 = conta0 + 1\n",
        "     elif class_ == 1 and conta1 >= int((classes_count[1]/100)*60) and conta1 < int((classes_count[1]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta1 = conta1 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta1 = conta1 + 1\n",
        "     elif class_ == 2 and conta2 >= int((classes_count[2]/100)*60) and conta2 < int((classes_count[2]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta2 = conta2 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta2 = conta2 + 1\n",
        "     elif class_ == 3 and conta3 >= int((classes_count[3]/100)*60) and conta3 < int((classes_count[3]/100)*80) :\n",
        "        if conta == 2 :\n",
        "          x_val=x_val.append(row_1, ignore_index=True) \n",
        "          x_val=x_val.append(row_2, ignore_index=True)\n",
        "          conta3 = conta3 + 2\n",
        "        else :\n",
        "          x_val=x_val.append(row_1, ignore_index=True) \n",
        "          conta3 = conta3 + 1\n",
        "     elif class_ == 4 and conta4 >= int((classes_count[4]/100)*60) and conta4 < int((classes_count[4]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta4 = conta4 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True)\n",
        "         conta4 = conta4 + 1 \n",
        "     elif class_ == 5 and conta5 >= int((classes_count[5]/100)*60) and conta5 < int((classes_count[5]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta5 = conta5 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta5 = conta5 + 1\n",
        "     elif class_ == 6 and conta6 >= int((classes_count[6]/100)*60) and conta6 < int((classes_count[6]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta6 = conta6 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta6 = conta6 + 1\n",
        "     elif class_ == 7 and conta7 >= int((classes_count[7]/100)*60) and conta7 < int((classes_count[7]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta7 = conta7 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta7 = conta7 + 1\n",
        "     elif class_ == 8 and conta8 >= int((classes_count[8]/100)*60) and conta8 < int((classes_count[8]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta8 = conta8 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta8 = conta8 + 1\n",
        "     elif class_ == 9 and conta9 >= int((classes_count[9]/100)*60) and conta9 < int((classes_count[9]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         x_val=x_val.append(row_2, ignore_index=True)\n",
        "         conta9 = conta9 + 2\n",
        "       else :\n",
        "         x_val=x_val.append(row_1, ignore_index=True) \n",
        "         conta9 = conta9 + 1\n",
        "     elif class_ == 0 and conta0 >= int((classes_count[0]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta0 = conta0 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta0 = conta0 + 1\n",
        "     elif class_ == 1 and conta1 >= int((classes_count[1]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta1 = conta1 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta1 = conta1 + 1\n",
        "     elif class_ == 2 and conta2 >= int((classes_count[2]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta2 = conta2 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta2 = conta2 + 1\n",
        "     elif class_ == 3 and conta3 >= int((classes_count[3]/100)*80) :\n",
        "        if conta == 2 :\n",
        "          x_test=x_test.append(row_1, ignore_index=True) \n",
        "          x_test=x_test.append(row_2, ignore_index=True)\n",
        "          conta3 = conta3 + 2\n",
        "        else :\n",
        "          x_test=x_test.append(row_1, ignore_index=True) \n",
        "          conta3 = conta3 + 1\n",
        "     elif class_ == 4 and conta4 >= int((classes_count[4]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta4 = conta4 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True)\n",
        "         conta4 = conta4 + 1 \n",
        "     elif class_ == 5 and conta5 >= int((classes_count[5]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta5 = conta5 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta5 = conta5 + 1\n",
        "     elif class_ == 6 and conta6 >= int((classes_count[6]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta6 = conta6 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta6 = conta6 + 1\n",
        "     elif class_ == 7 and conta7 >= int((classes_count[7]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta7 = conta7 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta7 = conta7 + 1\n",
        "     elif class_ == 8 and conta8 >= int((classes_count[8]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta8 = conta8 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta8 = conta8 + 1\n",
        "     elif class_ == 9 and conta9 >= int((classes_count[9]/100)*80) :\n",
        "       if conta == 2 :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         x_test=x_test.append(row_2, ignore_index=True)\n",
        "         conta9 = conta9 + 2\n",
        "       else :\n",
        "         x_test=x_test.append(row_1, ignore_index=True) \n",
        "         conta9 = conta9 + 1\n",
        "\n",
        "    \n",
        "  return x_train, x_test, x_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4OSrOanfe7I"
      },
      "source": [
        "## DATA GENERATION "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "    def __init__(self, df, X_col, y_col,\n",
        "                 batch_size,\n",
        "                 input_size = (270, 470),\n",
        "                 shuffle = True,\n",
        "                 class_weights = None):\n",
        "      \n",
        "        self.df = df.copy()\n",
        "        self.X_col = X_col\n",
        "        self.y_col = y_col\n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "        self.shuffle = shuffle\n",
        "        self.class_weights = class_weights\n",
        "        \n",
        "        self.n = len(self.df)\n",
        "        self.n_CLASSE_CALCIO = df[y_col['CLASSE']].nunique()\n",
        "        self.n_SERIE_CALCIO = df[y_col['GEOMETRIA']].nunique()\n",
        "\n",
        "\n",
        "        self.classi_augmented = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}  #to count wich quality classes are augmented during the training on the fly\n",
        "\n",
        "        #D.A. Online\n",
        "        self.augmentor = ImageDataGenerator(\n",
        "            print(\"[INFO] performing 'on the fly' data augmentation\"),\n",
        "            horizontal_flip = True,\n",
        "            vertical_flip = True, \n",
        "            brightness_range = [0.2,0.5],\n",
        "            preprocessing_function = None,\n",
        "            fill_mode = 'constant',\n",
        "            cval = 0.0,\n",
        "        )\n",
        "        \n",
        "        if self.class_weights is not None:\n",
        "          self.df2 = df.copy()\n",
        "          self.class_weights = class_weights.copy()\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    def __get_input(self, path, target_size):\n",
        "      try:\n",
        "        image = tf.keras.preprocessing.image.load_img(path_IMAGES+path, color_mode=\"rgb\" , target_size=(target_size[0],target_size[1]))\n",
        "      except Exception:\n",
        "        print('\\n{}_not found'.format(path))\n",
        "      \n",
        "      image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
        "      image_arr = tf.keras.applications.vgg16.preprocess_input(image_arr)               \n",
        "      image_arr = tf.image.resize(image_arr,(target_size[0], target_size[1])).numpy()\n",
        "\n",
        "      return image_arr\n",
        "    \n",
        "    def __get_output(self, label, num_classes):\n",
        "        return tf.keras.utils.to_categorical(label, num_classes=num_classes)\n",
        "\n",
        "    def __get_output2(self, label, num_series):\n",
        "        return tf.keras.utils.to_categorical(label, num_classes=num_series)\n",
        "    \n",
        "    def __get_data(self, batches):\n",
        "        # Generates data containing batch_size samples\n",
        "        path_batch = batches[self.X_col['PATH_IMG']]  \n",
        "        CLASSE_batch = batches[self.y_col['CLASSE']]\n",
        "        SERIE_batch = batches[self.y_col['GEOMETRIA']]\n",
        "\n",
        "        X_batch = np.asarray([self.__get_input(x, self.input_size) for x in path_batch])\n",
        "        y_batch_ = np.asarray([self.__get_output(y, self.n_CLASSE_CALCIO) for y in CLASSE_batch])\n",
        "        z_batch_ = np.asarray([self.__get_output2(z, self.n_SERIE_CALCIO) for z in SERIE_batch])\n",
        "        y_batch = [y_batch_, z_batch_]  #architecture with multioutput (2 distinct outputs: series, quality_classes), so we need to connect them into a vector \n",
        "        \n",
        "        return X_batch, y_batch\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "                \n",
        "        X, y = self.__get_data(batches)\n",
        "        \n",
        "        if self.class_weights is not None:\n",
        "          sample_weighttt = self.__get_samples_weights_V2(y[0])\n",
        "          X_gen = self.augmentor.flow(X, batch_size=self.batch_size, shuffle=False, sample_weight=sample_weighttt)\n",
        "          return next(X_gen), y                \n",
        "        else:\n",
        "          return X, y                    \n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.n) // self.batch_size\n",
        "\n",
        "\n",
        "    #get weight of batch \n",
        "    def __get_samples_weights_V2(self, y):\n",
        "          labels = []\n",
        "          for x_row in y: \n",
        "            class_array = np.where(x_row == 1)\n",
        "            classe_ = class_array[0]\n",
        "            labels.append(classe_[0])\n",
        "          \n",
        "          labels_batch = np.array(labels)          \n",
        "          \n",
        "          class_weight_present_batch = { your_key: self.class_weights[your_key] for your_key in np.unique(labels_batch)}\n",
        "          \n",
        "          weights = compute_sample_weight(class_weight_present_batch,  labels_batch)\n",
        "\n",
        "          for your_key in np.unique(labels_batch):\n",
        "            self.classi_augmented[your_key] +=1\n",
        "        \n",
        "          weights = np.array(weights)\n",
        "          print(weights)\n",
        "          return weights\n",
        "\n"
      ],
      "metadata": {
        "id": "JZlqpycPq8Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzh70UKnU-fM"
      },
      "source": [
        "## PREPROCESSING IMAGES & DATA FRAME CREATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''verify that same IDs are in the same sub-sets'''\n",
        "\n",
        "'''method for univoque sets'''\n",
        "def check_for_leakage(df1, df2, patient_col):\n",
        "    \"\"\"\n",
        "    Return True if there any patients are in both df1 and df2.\n",
        "\n",
        "    Args:\n",
        "        df1 (dataframe): dataframe describing first dataset\n",
        "        df2 (dataframe): dataframe describing second dataset\n",
        "        patient_col (str): string name of column with patient IDs\n",
        "    \n",
        "    Returns:\n",
        "        leakage (bool): True if there is leakage, otherwise False\n",
        "    \"\"\"\n",
        "    \n",
        "    df1_patients_unique = set(df1[patient_col])\n",
        "    df2_patients_unique = set(df2[patient_col])\n",
        "    \n",
        "    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)\n",
        "\n",
        "    # leakage contains true if there is patient overlap, otherwise false.\n",
        "    leakage = len(patients_in_both_groups) >= 1 # boolean (true if there is at least 1 patient in both groups)\n",
        "        \n",
        "    return leakage"
      ],
      "metadata": {
        "id": "vNqlvLTEefFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGKxig25WSRO"
      },
      "source": [
        "'''PREPROCESSING PHASE OF THE DATAFRAME (CREATIONS OF THE SUBSETS TRAIN/VALIDATION/TEST, CALCULATE WEIGHTS OF ELEMENTS OF THE SUBSETS, VERIFY THAT SAME IDs ARE IN THE SAME SUBSET)'''\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from pandas.compat._optional import import_optional_dependency\n",
        "\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/ProgettoDL')\n",
        "path = os.getcwd()\n",
        "\n",
        "\n",
        "'''reading inforamtions from the CSV'''\n",
        "col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n",
        "dataframe_sx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n",
        "\n",
        "\n",
        "col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n",
        "dataframe_dx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n",
        "\n",
        "\n",
        "'''rename the dataframe columns'''\n",
        "dataframe_sx.columns = ['ID','series', 'filename', 'class']\n",
        "dataframe_dx.columns = ['ID','series', 'filename', 'class']\n",
        "\n",
        "frames = [dataframe_sx, dataframe_dx] \n",
        "result = pd.concat(frames) #concatenate the two dataframes\n",
        "\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"DATAFRAME COMPLETO INIZIALE\")\n",
        "print(\"result\")\n",
        "print(result)\n",
        "\n",
        "\n",
        "'''mapping the values used for the classification into integer values'''\n",
        "#version with 10 classes\n",
        "result[\"class\"] = result[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n",
        "result[\"series\"] = result[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)}) \n",
        "\n",
        "\n",
        "'''identification of NULL values that would bring the execution on failing and eliminate those values'''\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"Number of Null values in column 'quality_classes' : \"+format(result['class'].isnull().sum()))\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "#print(result.loc[result['class'] == '0'])\n",
        "print(\"mostro quegli elementi che hanno valore nullo\")\n",
        "print(result[result['class'].isnull()])\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "\n",
        "'''Remove Null elements to avoid failures during executions (data in not useful!)'''\n",
        "print(\"Rimuovo gli elementi nulli e verifico stampando nuovamente i valori nulli:\")\n",
        "result['class'] = pd.to_numeric(result['class'], errors='coerce')\n",
        "result = result.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n",
        "\n",
        "print(\"elementi nulli rimasti: \"+format(result['class'].isnull().sum()))     #stampo per verifica se ci sono elementi nulli\n",
        "\n",
        "\n",
        "'''verify if images exist in the Google Drive folder, when not present it is eliminated from the dataset aswell'''\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"elimino i file che non sono presenti in Google Drive anche se ci sono nel CSV\")\n",
        "print('CHECK FILE NON PRESENTI NELLA CARTELLA')\n",
        "\n",
        "#--------TORNA QUI ---------\n",
        "os.chdir(path_IMAGES)\n",
        "\n",
        "i = 0; \n",
        "for index, row in result.iterrows():\n",
        "    filename = row['filename']\n",
        "    if os.path.exists(path_IMAGES+filename) == False:\n",
        "      \n",
        "      print('File Non Esiste !!!')\n",
        "    if(os.path.exists(filename) == False):\n",
        "      result = result.drop(result[(result['filename'] == filename)].index)\n",
        "      print('File : {} eliminato'.format(filename))\n",
        "      i = i + 1             \n",
        "print('File Eliminati : {} '.format(i))\n",
        "\n",
        "print('CHECK FILE CON NaN')\n",
        "print(result[result['class'].isnull()])\n",
        "print(result[result['series'].isnull()])\n",
        "print(result[result['filename'].isnull()])\n",
        "print(result[result['ID'].isnull()])\n",
        "result = result[result['class'].notna()]\n",
        "result = result[result['series'].notna()]\n",
        "result = result[result['filename'].notna()]\n",
        "result = result[result['ID'].notna()]\n",
        "\n",
        "'''performing the splitting of the dataframe into sub-sets'''\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"SPLIT DATA\")\n",
        "train_balance_df, test_balance_df, val_balance_df  = split_data(result, 0.2, 0.2, 3)  #CUSTOM SPLIT CON ID IN STESSO SET DI DATI\n",
        "#train_mask, test_mask, validation_mask  = split_data(result2, 0.2, 0.2, 3)           #split per test con immagini con maschere\n",
        "\n",
        "print(\"train_balance_df\")\n",
        "print(train_balance_df)\n",
        "print(\"test_balance_df\")\n",
        "print(test_balance_df)\n",
        "print(\"val_balance_df\")\n",
        "print(val_balance_df)\n",
        "\n",
        "'''verify distibution of classes in the sub-sets and calculate weights of the classes in each sub-set'''\n",
        "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "vals, counts = np.unique(train_balance_df['class'], return_counts=True)\n",
        "print(\"conta del numero di immagini per speicfica classe in set Train\")\n",
        "print(len(train_balance_df))\n",
        "for i in range(0,len(classi)):\n",
        "    print('{}:{}'.format(classi[i], counts[i]))\n",
        "\n",
        "vals2, counts2 = np.unique(val_balance_df['class'], return_counts=True)\n",
        "print(\"conta del numero di immagini per speicfica classe in set Validation\")\n",
        "print(len(val_balance_df))\n",
        "for i in range(0,len(classi)):\n",
        "    print('{}:{}'.format(classi[i], counts2[i]))\n",
        "\n",
        "vals3, counts3 = np.unique(test_balance_df['class'], return_counts=True)\n",
        "print(\"conta del numero di immagini per speicfica classe in set Test\")\n",
        "print(len(test_balance_df))\n",
        "for i in range(0,len(classi)):\n",
        "    print('{}:{}'.format(classi[i], counts3[i]))    \n",
        "\n",
        "\n",
        "class_weights_train = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(train_balance_df['class']),y = train_balance_df['class'])\n",
        "weight_train = {i : round(class_weights_train[i], 2) for i in range(len(classi))} \n",
        "print('Weight train_balance_df')\n",
        "print(weight_train)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(val_balance_df['class']),y = val_balance_df['class'])\n",
        "weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n",
        "print('Weight val_balance_df')\n",
        "print(weight)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(test_balance_df['class']),y = test_balance_df['class'])\n",
        "weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n",
        "print('Weight test_balance_df')\n",
        "print(weight)\n",
        "\n",
        "#--------verifico che stessi ID siano in stesso set--------\n",
        "print(\"test case 1 - train VS validation\")\n",
        "print(f\"Stessi ID in set usati?: {check_for_leakage(train_balance_df, val_balance_df, 'ID')}\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"test case 2 - train VS test\")\n",
        "print(f\"Stessi ID in set usati ?: {check_for_leakage(train_balance_df, test_balance_df, 'ID')}\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"test case 3 - validation VS test\")\n",
        "print(f\"Stessi ID in set usati?: {check_for_leakage(val_balance_df, test_balance_df, 'ID')}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSvo5qiEEeVB"
      },
      "source": [
        "'''Verifica Classi Qualità per ogni Serie'''\n",
        "print(\"Verifica Classi Qualità per ogni Serie\")\n",
        "result_x_ = result.groupby(['series','class']).size()\n",
        "result_class = result.groupby(['class']).size()\n",
        "print(result_class)\n",
        "result_series = result.groupby(['series']).size()\n",
        "print(result_series)\n",
        "\n",
        "print('SOMMA IMG : {}'.format(result_class[0]+result_class[1]+result_class[2]+result_class[3]+result_class[4]+result_class[5]+result_class[6]+result_class[7]+result_class[8]+result_class[9]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss65GPosBd3o"
      },
      "source": [
        "##Weighted Categorical Cross-Entrophy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWDxSeamBcw9"
      },
      "source": [
        "from keras import backend as K\n",
        "class weighted_categorical_crossentropy(object):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "    \n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "    \n",
        "    Usage:\n",
        "        loss = weighted_categorical_crossentropy(weights).loss\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,weights):\n",
        "        self.weights = K.variable(weights)\n",
        "        \n",
        "    def loss(self,y_true, y_pred):\n",
        "        #print('stop')\n",
        "        \n",
        "        # scale preds so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred)\n",
        "        # clip\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "        # calc\n",
        "        \n",
        "        loss = y_true*K.log(y_pred)*self.weights\n",
        "        loss =-K.sum(loss,-1)\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28i0TbSpzt_Z"
      },
      "source": [
        "## NETWORK "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Dc7ahEzpWW"
      },
      "source": [
        "'''NETWORK DEFINITION'''\n",
        "\n",
        "\n",
        "'''build the top model'''\n",
        "model = Sequential()\n",
        "\n",
        "vgg16_conv = VGG16(include_top=False, weights='imagenet', input_shape=(immg_rows, immg_cols, 3))      #pre allenata con immagini di imagenet, e quindi pesi già esistenti dall'inizio, che poi vengono modificati e migliorati\n",
        "for layer in vgg16_conv.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "##NOTA : in alcune alternative ho visto che applicano il Flatten prima della ramificazione.\n",
        "top_model = Flatten(name='flatten')(vgg16_conv.output)\n",
        "\n",
        "'''building the first ramification - quality class classificator'''   \n",
        "#x = Flatten(name='flatten')(vgg16_conv.output)\n",
        "x = Dropout(0.5)(top_model)\n",
        "x = Dense(4096, activation='relu', name='fc1')(x)\n",
        "x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(len(classi), activation='softmax', name='class_output')(x) #-Prediction_QUALITY_CLASS\n",
        "\n",
        "'''building the second ramification - domain adaptator (adversarial debiasing), series class classificator'''\n",
        "#-----------------------\n",
        "#Gradient reversal layer (per invertire il segno della loss e mantenere una forma di Loss complessiva formata solo da somme)\n",
        "\n",
        "@tf.custom_gradient\n",
        "def grad_reverse(x):\n",
        "    y = tf.identity(x)\n",
        "    def custom_grad(dy):\n",
        "        return -dy\n",
        "    return y, custom_grad\n",
        "\n",
        "class GradReverse(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, x):\n",
        "        return grad_reverse(x)\n",
        "\n",
        "#-----------------------\n",
        "#x2 = Flatten(name='flatten2')(vgg16_conv.output)\n",
        "\n",
        "x2 = GradReverse()(top_model)\n",
        "\n",
        "x2 = Dropout(0.5)(x2)\n",
        "x2 = Dense(4096, activation='relu', name='fc1_2')(x2)\n",
        "x2 = Dense(4096, activation='relu', name='fc2_2')(x2)\n",
        "x2 = BatchNormalization()(x2)\n",
        "x2 =  Dense(len(serie), activation='softmax', name='series_output')(x2)   #-Adversarial_Debiasing\n",
        "\n",
        "\n",
        "'''creating the complete architecture'''\n",
        "\n",
        "dot_img_file = '/tmp/model_2.png'     #choose a location where save the image model\n",
        " \n",
        "model = keras.Model(vgg16_conv.input, [x, x2], name=\"quality_recognizer\")   #create the complete model ì, with the input and the 2 outputs ramifications\n",
        "\n",
        "model.summary()    # inspect model in output video\n",
        "\n",
        "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)  #plot the model with the shapes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdnARydQAI-f"
      },
      "source": [
        "##Metrica Balance Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvHBOEvqAL1Z"
      },
      "source": [
        "import keras.backend as K\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "'''\n",
        "Funzione per Balance Accuracy \n",
        "-- Link Utile : https://medium.com/@mostafa.m.ayoub/customize-your-keras-metrics-44ac2e2980bd --\n",
        "-- https://medium.com/analytics-vidhya/custom-metrics-for-keras-tensorflow-ae7036654e05 --- \n",
        "-- https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score --- \n",
        "'''\n",
        "\n",
        "\t\t\n",
        "#https://medium.com/@mostafa.m.ayoub/customize-your-keras-metrics-44ac2e2980bd\n",
        "#https://www.statology.org/balanced-accuracy-python-sklearn/\n",
        "def specificity(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    return tn / (tn + fp + K.epsilon())\n",
        "\n",
        "def sensitivity(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def balanced_accuracy_new (y_true, y_pred):\n",
        "\t\tsensitivity_ = sensitivity(y_true, y_pred)\n",
        "\t\tspecificity_ = specificity(y_true, y_pred)\n",
        "\t\tBA = (sensitivity_ + specificity_) / 2\n",
        "\t\treturn BA "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX2SB5uYzzyB"
      },
      "source": [
        "## Callbacks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvAnOHC2W134"
      },
      "source": [
        "'''CALLBACKS'''\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "#TEST BALANCE ACCURACY \n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "#NB: create sul drive una cartella weights dove salvare i pesi durante l'allenamento\n",
        "path_drive = '/content/drive/My Drive/'\n",
        "path = path_drive+'ProgettoDL/'\n",
        "model_checkpoint_val_bal_acc = ModelCheckpoint( filepath=os.path.join('/content/drive/My Drive/ProgettoDL/weights/model_{}_{}/best_weights.h5'.format(immgs,cnn)), monitor='val_class_output_balanced_accuracy_new', verbose=1, save_best_only=True)\n",
        "model_checkpoint_val_loss = ModelCheckpoint( filepath=os.path.join('/content/drive/My Drive/ProgettoDL/weights/model_{}_{}/best_weights.h5'.format(immgs,cnn)), monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "### MODIFICATO QUA - Implementazione Early Stopping###\n",
        "early_stopping_val_bal_acc = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_class_output_balanced_accuracy_new', #Quantity to be monitored \n",
        "    min_delta=0, #Minimum change in the monitored quantity to qualify as an improvement\n",
        "    patience=10, #Number of epochs with no improvement after which training will be stopped\n",
        "    #verbosity mode, setting verbose 0, 1 or 2 you just say \n",
        "    #how do you want to 'see' the training progress for each epoch.\n",
        "    #verbose=0 will show you nothing (silent)\n",
        "    #verbose=1 will show you an animated progress bar like this: progres_bar\n",
        "    verbose=0, \n",
        "    #Mode = One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity \n",
        "    #monitored has stopped decreasing; in \"max\" mode \n",
        "    #it will stop when the quantity monitored has stopped increasing; \n",
        "    #in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n",
        "    mode=\"max\",\n",
        "    #Training will stop if the model doesn't show improvement over the baseline.\n",
        "    baseline=None,\n",
        "    #Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
        "    restore_best_weights=False\n",
        ")\n",
        "early_stopping_val_loss = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', #Quantity to be monitored \n",
        "    min_delta=0, #Minimum change in the monitored quantity to qualify as an improvement\n",
        "    patience=10, #Number of epochs with no improvement after which training will be stopped\n",
        "    #verbosity mode, setting verbose 0, 1 or 2 you just say \n",
        "    #how do you want to 'see' the training progress for each epoch.\n",
        "    #verbose=0 will show you nothing (silent)\n",
        "    #verbose=1 will show you an animated progress bar like this: progres_bar\n",
        "    verbose=0, \n",
        "    #Mode = One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity \n",
        "    #monitored has stopped decreasing; in \"max\" mode \n",
        "    #it will stop when the quantity monitored has stopped increasing; \n",
        "    #in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n",
        "    mode=\"auto\",\n",
        "    #Training will stop if the model doesn't show improvement over the baseline.\n",
        "    baseline=None,\n",
        "    #Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
        "    restore_best_weights=False\n",
        ")\n",
        "callbacks=[model_checkpoint_val_bal_acc, model_checkpoint_val_loss , early_stopping_val_bal_acc, early_stopping_val_loss ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks3Fly2oUf3g"
      },
      "source": [
        "##HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXwAzoJE_OIP"
      },
      "source": [
        "\n",
        "The **optimization algorithm** (or optimizer) is the main approach used today for training a machine learning model to minimize its error rate. There are *two metrics* to determine the efficacy of an optimizer: **speed of convergence** (the process of reaching a global optimum for gradient descent); and **generalization** (the model’s performance on new data)\n",
        "\n",
        "***SGD*** : Stochastic Gradient Descent \n",
        "\n",
        "Parameters \n",
        "- Learning : learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. ... In setting a learning rate, there is a trade-off between the rate of convergence and overshooting\n",
        "- Momentum : Momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
        "- Decay :  We then set our decay to be the learning rate divided by the total number of epochs we are training the network for (a common rule of thumb) ... lr = (lr_iniziale - (1.0/(1-decay*iterations)))\n",
        "- Nesterov: Nesterov which is set to false by default. Nesterov momentum is a different version of the momentum method which has stronger theoretical converge guarantees for convex functions.\n",
        "\n",
        "[1° LINK](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)\n",
        "\n",
        "[2° LINK](https://www.pyimagesearch.com/2019/07/22/keras-learning-rate-schedules-and-decay/)\n",
        "\n",
        "\n",
        "***ADAM*** :  Adaptive Moment Estimation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "FONTI : Paper for ICLR 2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbjwj4QwW6yx"
      },
      "source": [
        "'''HYPERPARAMETERS DEFINITION'''\n",
        "#per la stampa dei tensori \n",
        "import keras.backend as K   \n",
        "\n",
        "opt = SGD(learning_rate = 0.0001, decay = 1e-4, momentum= 0.9)\n",
        "\n",
        "num_epochs = 100\n",
        "bs = 32 \n",
        "\n",
        "\n",
        "class_weights = weight_train\n",
        "print('Class Weight Train : {} '.format(class_weights))\n",
        "print('Type Class Weight Train : {} '.format(type(class_weights)))\n",
        "\n",
        "data = list(class_weights.values())\n",
        "class_weights = np.array(data)\n",
        "print('Class Weight Train : {} '.format(class_weights))\n",
        "\n",
        "losses = {\n",
        "\t#\"class_output\": \"categorical_crossentropy\", \n",
        "  \"class_output\": weighted_categorical_crossentropy(class_weights).loss,\n",
        "  \"series_output\": \"categorical_crossentropy\" \n",
        "}\n",
        "\n",
        "lossWeights = {\"class_output\": 1.0, \"series_output\": 0.5}\n",
        "\n",
        "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"accuracy\",balanced_accuracy_new])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creazione TrainGen, ValGen, TestGen "
      ],
      "metadata": {
        "id": "HzdfsojWg2Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traingen = CustomDataGen(train_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470), class_weights = weight_train )     \n",
        "testgen = CustomDataGen(test_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470))   \n",
        "valgen = CustomDataGen(val_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470))"
      ],
      "metadata": {
        "id": "24RcC4Hhg31L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfJ3LnbAZhQx"
      },
      "source": [
        "## Testing Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMXAiMAvyom5"
      },
      "source": [
        "'''TRAINING THE MODEL'''\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "history = model.fit(x=traingen,validation_data=valgen, epochs=num_epochs, callbacks = [callbacks], verbose=1)\n",
        "print(history.history.keys()) #---serve per stampare le metriche che ho nel modello "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check D.A. "
      ],
      "metadata": {
        "id": "jzjfKJbF3YWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Conta del numero di immagini per specifica classe in set Train\")\n",
        "print(traingen.classi_augmented)"
      ],
      "metadata": {
        "id": "j_rOgSTx3a6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBGTOuc1VSqq"
      },
      "source": [
        "## PLOT "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQzh_8drJ56m"
      },
      "source": [
        "'''PLOT CURVES'''\n",
        "\n",
        "path = path_drive+'ProgettoDL/'\n",
        "\n",
        "acc_class = history.history['class_output_accuracy']\n",
        "acc_series = history.history['series_output_accuracy']\n",
        "val_acc_class = history.history['val_class_output_accuracy']\n",
        "val_acc_series = history.history['val_series_output_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "class_output_bal_acc = history.history['class_output_balanced_accuracy_new']\n",
        "val_class_output_bal_acc = history.history['val_class_output_balanced_accuracy_new']\n",
        "lista = [acc_class,acc_series,val_acc_class,val_acc_series,loss,val_loss,class_output_bal_acc,val_class_output_bal_acc] #--- modificato \n",
        "     \n",
        "epochs = range(len(acc_class))\n",
        "\n",
        "plt.plot(epochs, acc_class, 'r', label='Training acc Class') \n",
        "plt.plot(epochs, acc_series, 'g', label='Training acc Series') \n",
        "plt.plot(epochs, val_acc_class, 'b', label='Validation acc Class') \n",
        "plt.plot(epochs, val_acc_series, 'y', label='Validation acc Series')\n",
        "plt.title('Training and validation accuracy (IMG)')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(path+'weights/PlotAcc_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "plt.figure()\n",
        " \n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss (IMG)')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(path+'weights/PlotLoss_{}_{}.pdf'.format(immgs,cnn)))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, class_output_bal_acc, 'b', label='Training Balance Accuracy')\n",
        "plt.plot(epochs, val_class_output_bal_acc, 'r', label='Validation Balance Accuracy')\n",
        "plt.title('Training and validation balance accuracy (IMG)')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(path+'weights/PlotLoss_{}_{}.pdf'.format(immgs,cnn)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjI6Y_nefFTj"
      },
      "source": [
        "### SAVE MODEL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApFaEaa6oPu0"
      },
      "source": [
        "#salvataggio modello pesi finali\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "path = path_drive+'ProgettoDL/'\n",
        "model.save(os.path.join(path+'weights/model_{}_{}/Final'.format(immgs,cnn)))\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XefnmV_tsbOw"
      },
      "source": [
        "## LOAD MODEL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VDiBcm-XEOb"
      },
      "source": [
        "'''TEST'''\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "path_drive = '/content/drive/My Drive/'\n",
        "path = path_drive+'ProgettoDL/'\n",
        "\n",
        "path_model = os.path.join(path+'weights/model_{}_{}/Final'.format(immgs,cnn))\n",
        "\n",
        "model = load_model(path_model, custom_objects={'balanced_accuracy_new':balanced_accuracy_new})\n",
        "print('Model Loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQMtb_JBD5Vp"
      },
      "source": [
        "## PREDICTION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDb9juVJfFBc"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from tensorflow.keras import Model\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "test_array = []\n",
        "test_array_series = []\n",
        "\n",
        "for index, row in test_balance_df.iterrows():\n",
        "    class_ = int(row['class'])\n",
        "    series_ = int(row['series'])          #---da qui e nei prossimi, calcolo la ground thruth del shotgun series, ovvero i semplici COD_COMPONENTE (serie) che appartengono al sub-set di test\n",
        "    test_array.append(class_)\n",
        "    test_array_series.append(series_)     #---\n",
        "\n",
        "test_array = np.array(test_array)\n",
        "test_array_series = np.array(test_array_series)   #---\n",
        "\n",
        "y_test = to_categorical(np.unique(test_array, return_inverse=True)[1])\n",
        "y_test_series = to_categorical(np.unique(test_array_series, return_inverse=True)[1])      #---\n",
        "\n",
        "imgs_array = [] \n",
        "\n",
        "\n",
        "for index, row in testgen.df.iterrows():\n",
        "    filename = row['filename'] \n",
        "    image = load_img(os.path.join(path_IMAGES,filename), target_size = (immg_rows, immg_cols))\n",
        "    x = img_to_array(image)\n",
        "    x = preprocess_input(x)  #non dovrebbe servire\n",
        "    imgs_array.append(x)\n",
        "    X_test = np.asarray(imgs_array)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FpZ8K4yZXZ7"
      },
      "source": [
        "######PARTE DA FINIRE DI SISTEMARE\n",
        "y_test_no_argmax = y_test\n",
        "\n",
        "y_test = y_test.argmax(axis=1)\n",
        "y_test_series = y_test_series.argmax(axis=1)      \n",
        "\n",
        "'''predictions'''\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#---------------\n",
        "#metriche nuove senza usare argmax.\n",
        "#---------------\n",
        "y_pred_no_argmax = y_pred[0]            #--- modificato qua aggiungendo y_pred[0] invece di y_pred\n",
        "y_pred = np.argmax(y_pred[0],axis=1)    #--- modificato qua aggiungendo y_pred[0] invece di y_pred\n",
        "\n",
        "print(y_pred)\n",
        "print(y_test)\n",
        "\n",
        "print(y_pred_no_argmax.shape)\n",
        "print(y_test_no_argmax.shape)\n",
        "\n",
        "#y_pred_conf = model.predict(X_test)\n",
        "#index = np.where(np.equal(y_pred, y_test) == False)[0]\n",
        "#print(np.around(y_pred_conf[index], decimals = 2))\n",
        "\n",
        "\"\"\"\n",
        "#alcune verifiche visive sull'intero sub-set di test\n",
        "print(y_test)\n",
        "print(y_test.shape)\n",
        "print(y_test_series)\n",
        "print(y_test_series.shape)\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHMuVIu0_ZFJ"
      },
      "source": [
        "print(y_test)\n",
        "print(y_test.shape)\n",
        "print(y_test_series)\n",
        "print(y_test_series.shape)\n",
        "print(y_pred)\n",
        "print(y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkaCQp4Ndzbe"
      },
      "source": [
        "##SEARCH UNIVOQUE SERIES TO BALANCE SETS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkviN4oh2M6l"
      },
      "source": [
        "'''SEARCHING UNIVOQUE SERIES'''\n",
        "\n",
        "'''serve per fare le stampe delle confusion matrix per ciascuna serie di appartenenza delle immagini del sub-set test'''\n",
        "test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4, test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9, test_array_s10, test_array_s11, test_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4, pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9, pred_array_s10, pred_array_s11, pred_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "i=0\n",
        "for index, row in test_balance_df.iterrows():\n",
        "    \n",
        "    series_ = int(row['series'])\n",
        "    if series_ == 0:\n",
        "      test_array_s0.append(y_test[i])\n",
        "      pred_array_s0.append(y_pred[i])\n",
        "    if series_ == 1:\n",
        "      test_array_s1.append(y_test[i])\n",
        "      pred_array_s1.append(y_pred[i])\n",
        "    if series_ == 2:\n",
        "      test_array_s2.append(y_test[i])\n",
        "      pred_array_s2.append(y_pred[i])\n",
        "    if series_ == 3:\n",
        "      test_array_s3.append(y_test[i])\n",
        "      pred_array_s3.append(y_pred[i])\n",
        "    if series_ == 4:\n",
        "      test_array_s4.append(y_test[i])\n",
        "      pred_array_s4.append(y_pred[i])\n",
        "    if series_ == 5:\n",
        "      test_array_s5.append(y_test[i])\n",
        "      pred_array_s5.append(y_pred[i])\n",
        "    if series_ == 6:\n",
        "      test_array_s6.append(y_test[i])\n",
        "      pred_array_s6.append(y_pred[i])\n",
        "    if series_ == 7:\n",
        "      test_array_s7.append(y_test[i])\n",
        "      pred_array_s7.append(y_pred[i])\n",
        "    if series_ == 8:\n",
        "      test_array_s8.append(y_test[i])\n",
        "      pred_array_s8.append(y_pred[i])\n",
        "    if series_ == 9:\n",
        "      test_array_s9.append(y_test[i])\n",
        "      pred_array_s9.append(y_pred[i])\n",
        "    if series_ == 10:\n",
        "      test_array_s10.append(y_test[i])\n",
        "      pred_array_s10.append(y_pred[i])\n",
        "    if series_ == 11:\n",
        "      test_array_s11.append(y_test[i])\n",
        "      pred_array_s11.append(y_pred[i])\n",
        "    if series_ == 12:\n",
        "      test_array_s12.append(y_test[i])\n",
        "      pred_array_s12.append(y_pred[i])\n",
        "\n",
        "    i=i+1\n",
        "\n",
        "print(test_array_s0)\n",
        "print(pred_array_s0)\n",
        "\n",
        "from functools import reduce\n",
        "reduced = reduce(np.union1d, (pred_array_s0, test_array_s0))\n",
        "print(reduced)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8WKlbpqKgAP"
      },
      "source": [
        "## METRICHE MASK & IMG "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHSjdjJcgc1Y"
      },
      "source": [
        "'''METRICHE'''\n",
        "print('--------------Metrice IMG----------------')\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score,classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "a = accuracy_score(y_test, y_pred)                                              # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
        "print(\"test accuracy: {0:0.4f}\".format(a))\n",
        "print(\"precision: {0:0.4f}\".format(precision_score(y_test, y_pred , average=\"macro\")))         # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html?highlight=precision_score#sklearn.metrics.precision_score\n",
        "print(\"recall: {0:0.4f}\".format(recall_score(y_test, y_pred , average=\"macro\")))                # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall_score#sklearn.metrics.recall_score\n",
        "print(\"f1_score: {0:0.4f}\".format(f1_score(y_test, y_pred , average=\"macro\")))                # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score\n",
        "print('---------------------------------------')\n",
        "print('classification report:')\n",
        "print(classification_report(y_test, y_pred))  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJjt39sGAfQF"
      },
      "source": [
        "Per quanto riguarda la funzione np_quadratic_weighted_kappa abbiamo avuto alcune difficoltà implementative e quindi abbiamo cercato un codice online che ci calcolasse la stessa metrica \n",
        "\n",
        "[Link Utilizzato](https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in2Vpij3AHgB"
      },
      "source": [
        "\n",
        "# The following 3 functions have been taken from Ben Hamner's github repository\n",
        "# https://github.com/benhamner/Metrics\n",
        "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the confusion matrix between rater's ratings\n",
        "    \"\"\"\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(rater_a + rater_b)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(rater_a + rater_b)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    conf_mat = [[0 for i in range(num_ratings)]\n",
        "                for j in range(num_ratings)]\n",
        "    for a, b in zip(rater_a, rater_b):\n",
        "        conf_mat[a - min_rating][b - min_rating] += 1\n",
        "    return conf_mat\n",
        "\n",
        "\n",
        "def histogram(ratings, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the counts of each type of rating that a rater made\n",
        "    \"\"\"\n",
        "    if min_rating is None:\n",
        "        min_rating = min(ratings)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(ratings)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    hist_ratings = [0 for x in range(num_ratings)]\n",
        "    for r in ratings:\n",
        "        hist_ratings[r - min_rating] += 1\n",
        "    return hist_ratings\n",
        "\n",
        "def quadratic_weighted_kappa(y, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the quadratic weighted kappa\n",
        "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    rater_a = y\n",
        "    rater_b = y_pred\n",
        "    min_rating=1 # era None abbiamo messo 0\n",
        "    max_rating=9 # era None abbiamo messo 9\n",
        "    rater_a = np.array(rater_a, dtype=int)\n",
        "    rater_b = np.array(rater_b, dtype=int)\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(min(rater_a), min(rater_b))\n",
        "    if max_rating is None:\n",
        "        max_rating = max(max(rater_a), max(rater_b))\n",
        "    conf_mat = Cmatrix(rater_a, rater_b,\n",
        "                                min_rating, max_rating)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return (1.0 - numerator / denominator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jydD-lsObD4e"
      },
      "source": [
        "path_drive = '/content/drive/My Drive/'\n",
        "path = path_drive+'ProgettoDL/'\n",
        "\n",
        "os.chdir(path)\n",
        "\n",
        "from metrics import np_quadratic_weighted_kappa, minimum_sensitivity\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "#errore no graph before run \n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "def compute_metrics(y_true, y_pred, num_classes):\n",
        "  #run function minimum_sensitivity\n",
        "  # Calculate metric\n",
        "  sess = keras.backend.get_session()\n",
        "\n",
        "  #qwk = np_quadratic_weighted_kappa(np.argmax(y_true, axis=0), np.argmax(y_pred, axis=0), 0,\n",
        "\t#\t\t\t\t\t\t\t\t\tnum_classes - 1)\n",
        "  \n",
        "  qwk = quadratic_weighted_kappa(y_true, y_pred)\n",
        "  ms = minimum_sensitivity(y_test_no_argmax, y_pred_no_argmax)\n",
        "  mae = sess.run(K.mean(keras.losses.mean_absolute_error(y_test_no_argmax, y_pred_no_argmax)))\n",
        "  \n",
        "  metrics = {\n",
        "\t\t'QWK': qwk,\n",
        "\t\t'MS': ms,\n",
        "\t\t'MAE': mae}\n",
        "  \n",
        "  return metrics\n",
        "\n",
        "def print_metrics(metrics):\n",
        "\tprint('QWK: {:.4f}'.format(metrics['QWK']))\n",
        "\tprint('MS: {:.4f}'.format(metrics['MS']))\n",
        "\tprint('MAE: {:.4f}'.format(metrics['MAE']))    \n",
        "\n",
        "\n",
        "#-----codice------\n",
        "num_classi = 10\n",
        "metrics = compute_metrics(y_test, y_pred,num_classi)\n",
        "print_metrics(metrics)\n",
        "\n",
        "with open(\"metrics.txt\", \"w\") as text_file:\n",
        "    print(print_metrics, file=text_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnlbf22bBpIt"
      },
      "source": [
        "***Metrice Ottenute***\n",
        "\n",
        "**K Cohen**   https://it.vvikipedla.com/wiki/Cohen%27s_kappa\n",
        "Il Kappa di Cohen è un coefficiente statistico che rappresenta il grado di accuratezza e affidabilità in una classificazione statistica; è un indice di concordanza che tiene conto della probabilità di concordanza casuale; l'indice calcolato in base al rapporto tra l'accordo in eccesso rispetto alla probabilità di concordanza casuale e l'eccesso massimo ottenibile. Attraverso la matrice di confusione è possibile valutare questo parametro. In particolare ... Esistono diversi \"gradi di concordanza\", in base ai quali possiamo definire se Kappa di Cohen è scarso o ottimo:\n",
        "\n",
        "- se k assume valori inferiori a 0, allora non c'è concordanza;\n",
        "- se k assume valori compresi tra 0-0,4, allora la concordanza è scarsa;\n",
        "- se k assume valori compresi tra 0,4-0,6, allora la concordanza è discreta;\n",
        "- se k assume valori compresi tra 0,6-0,8, la concordanza è buona;\n",
        "- se k assume valori compresi tra 0,8-1, la concordanza è ottima.\n",
        "\n",
        "**QWK**: 0.7849\n",
        "\n",
        "BLA BLA BLA \n",
        "\n",
        "**MS**: 1.0000\n",
        "\n",
        "\n",
        "In statistics, **mean absolute error (MAE)** is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. \n",
        "\n",
        "**MAE**: 0.0000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z25yiN0zKZ5Y"
      },
      "source": [
        "## PLOT CONFUSION MATRIX FUNCTION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsYoP-3hgfY5"
      },
      "source": [
        "'''METHOD FOR PLOT CONFUSION MATRIX'''\n",
        "#Confusion Matrix - CROP\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i in range (cm.shape[0]):\n",
        "      for j in range (cm.shape[1]):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    axes = plt.gca()\n",
        "    bottom, top = axes.get_ylim()\n",
        "    axes.set_ylim(bottom + 0.5, top - 0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0JFOHg2KUnT"
      },
      "source": [
        "## PLOT CONFUSION MATRIX "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpj28QpDKTkz"
      },
      "source": [
        "'''PLOT ENTIRE CONFUSION MATRIX OF THE SUB-SET TEST'''\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "cm = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "fig = plot_confusion_matrix(cm,\n",
        "                      target_names = classi,\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix IMG \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_{}_{}.pdf'.format(immgs,cnn))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy4ylbat-r51"
      },
      "source": [
        "##PLOT CONFUSION MATRIX PER CIASCUNA SERIE DEL CALCIO "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEruoZcb-q_S"
      },
      "source": [
        "'''PLOT CONFUSION MATRIX FOR EACH DISTINCT SERIES OF PRODUCTION PRESENT INTO THE SUB-SET TEST'''\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "from functools import reduce\n",
        "#serie 0\n",
        "cm0 = metrics.confusion_matrix(y_true=test_array_s0, y_pred=pred_array_s0)\n",
        "fig = plot_confusion_matrix(cm0,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s0, test_array_s0)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 0 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie0_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 1\n",
        "cm1 = metrics.confusion_matrix(y_true=test_array_s1, y_pred=pred_array_s1)\n",
        "fig = plot_confusion_matrix(cm1,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s1, test_array_s1)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 1 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie1_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 2\n",
        "cm2 = metrics.confusion_matrix(y_true=test_array_s2, y_pred=pred_array_s2)\n",
        "fig = plot_confusion_matrix(cm2,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s2, test_array_s2)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 2 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie2_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 3\n",
        "cm3 = metrics.confusion_matrix(y_true=test_array_s3, y_pred=pred_array_s3)\n",
        "fig = plot_confusion_matrix(cm3,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s3, test_array_s3)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 3 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie3_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 4\n",
        "cm4 = metrics.confusion_matrix(y_true=test_array_s4, y_pred=pred_array_s4)\n",
        "fig = plot_confusion_matrix(cm4,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s4, test_array_s4)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 4 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie4_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 5\n",
        "cm5 = metrics.confusion_matrix(y_true=test_array_s5, y_pred=pred_array_s5)\n",
        "fig = plot_confusion_matrix(cm5,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s5, test_array_s5)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 5 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie5_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 6\n",
        "cm6 = metrics.confusion_matrix(y_true=test_array_s6, y_pred=pred_array_s6)\n",
        "fig = plot_confusion_matrix(cm6,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s6, test_array_s6)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 6 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie6_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 7\n",
        "cm7 = metrics.confusion_matrix(y_true=test_array_s7, y_pred=pred_array_s7)\n",
        "fig = plot_confusion_matrix(cm7,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s7, test_array_s7)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 7 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie7_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 8\n",
        "cm8 = metrics.confusion_matrix(y_true=test_array_s8, y_pred=pred_array_s8)\n",
        "fig = plot_confusion_matrix(cm8,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s8, test_array_s8)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 8 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie8_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 9 \n",
        "cm9 = metrics.confusion_matrix(y_true=test_array_s9, y_pred=pred_array_s9)\n",
        "fig = plot_confusion_matrix(cm9,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s9, test_array_s9)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 9 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie9_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 10\n",
        "cm10 = metrics.confusion_matrix(y_true=test_array_s10, y_pred=pred_array_s10)\n",
        "fig = plot_confusion_matrix(cm10,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s10, test_array_s10)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 10 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie10_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 11\n",
        "cm11 = metrics.confusion_matrix(y_true=test_array_s11, y_pred=pred_array_s11)\n",
        "fig = plot_confusion_matrix(cm11,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s11, test_array_s11)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 11 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie11_{}_{}.pdf'.format(immgs,cnn))) \n",
        "\n",
        "#serie 12\n",
        "cm12 = metrics.confusion_matrix(y_true=test_array_s12, y_pred=pred_array_s12)\n",
        "fig = plot_confusion_matrix(cm12,\n",
        "                      target_names = reduce(np.union1d, (pred_array_s12, test_array_s12)),\n",
        "                      normalize    = False,\n",
        "                      title        = \"Confusion Matrix Series 12 \")\n",
        "plt.savefig(os.path.join(path+'weights/CM_serie12_{}_{}.pdf'.format(immgs,cnn))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUnAYcXVclfD"
      },
      "source": [
        "## CRAMER V CORRELATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''CRAMER V CORRELATION MEASUREMENT'''\n",
        "\n",
        "'''PRIMA VERSIONE'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as ss\n",
        "import seaborn as sns\n",
        "\n",
        "def cramers_v(confusion_matrix):\n",
        "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
        "        uses correction from Bergsma and Wicher,\n",
        "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
        "    \"\"\"\n",
        "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    rcorr = r - ((r-1)**2)/(n-1)\n",
        "    kcorr = k - ((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
        "\n",
        "confusion_matrix = pd.crosstab(y_test, y_pred)\n",
        "cramer1 = cramers_v(confusion_matrix.values)\n",
        "print('cramer correlation tra predizioni delle classi, e le classi effettive : {:.4f}'.format(cramer1))\n",
        "\n",
        "confusion_matrix2 = pd.crosstab(y_test_series, y_pred)\n",
        "cramer2 = cramers_v(confusion_matrix2.values)\n",
        "print('cramer correlation tra predizioni delle classi e le ground thruth di shotgun series : {:.4f}'.format(cramer2))\n"
      ],
      "metadata": {
        "id": "PNmA7QZSp4I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqvgi3VKck3o"
      },
      "source": [
        "'''CRAMER V CORRELATION MEASUREMENT'''\n",
        "'''SECONDA VERSIONE.        https://www.youtube.com/watch?v=eTnLTJer_Oo'''\n",
        "contTable = pd.crosstab(y_test_series, y_pred)\n",
        "print(contTable)\n",
        "\n",
        "!pip install researchpy\n",
        "\n",
        "import researchpy\n",
        "\n",
        "crosstab, res = researchpy.crosstab(pd.Series(y_test_series), pd.Series(y_pred), test='chi-square')\n",
        "print(\"\\n{}\".format(res))\n",
        "\n",
        "df = min(contTable.shape[0], contTable.shape[1]) - 1\n",
        "print(\"\\ndf = {}\".format(df))\n",
        "\n",
        "V = res.iloc[2,1]\n",
        "print(\"V = {}\".format(V))\n",
        "\n",
        "if df == 1:\n",
        "    if V < 0.10:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.30:\n",
        "        qual = 'small'\n",
        "    elif V < 0.50:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "elif df == 2:\n",
        "    if V < 0.07:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.21:\n",
        "        qual = 'small'\n",
        "    elif V < 0.35:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "elif df == 3:\n",
        "    if V < 0.06:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.17:\n",
        "        qual = 'small'\n",
        "    elif V < 0.29:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "elif df == 4:\n",
        "    if V < 0.05:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.15:\n",
        "        qual = 'small'\n",
        "    elif V < 0.25:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "else:\n",
        "    if V < 0.05:\n",
        "        qual = 'negligible'\n",
        "    elif V < 0.13:\n",
        "        qual = 'small'\n",
        "    elif V < 0.22:\n",
        "        qual = 'medium'\n",
        "    else:\n",
        "        qual = 'large'\n",
        "\n",
        "print(\"\\nquality classification of the correlation is:   {}\".format(qual))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHab1JLzqkKx"
      },
      "source": [
        "To indicate the strength of the association between two nominal variables, Cramér's V (Cramér, 1946) is often used.\n",
        "\n",
        "As for the interpretation for Cramér's V various rules of thumb exist but one of them is from Cohen (1988, pp. 222, 224, 225) who let's the interpretation depend on the degrees of freedom, shown in the table below.\n",
        "\n",
        "|df*|negligible|small|medium|large|\n",
        "|-------|---|---|---|---|\n",
        "|1|0 < .10|.10 < .30|.30 < .50|.50 or more|\n",
        "|2|0 < .07|.07 < .21|.21 < .35|.35 or more|\n",
        "|3|0 < .06|.06 < .17|.17 < .29|.29 or more|\n",
        "|4|0 < .05|.05 < .15|.15 < .25|.25 or more|\n",
        "|5|0 < .05|.05 < .13|.13 < .22|.22 or more|\n",
        "\n",
        "The degrees of freedom (df*) is for Cramér's V the minimum of the number of rows, or number of columns, then minus one.\n",
        "\n",
        "Lets see how to obtain Cramér's V with Python, using an example.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**A SECONDA DEI RISULTATI E CONFRONTANDOLI CON LA TABELLA RIUSCIAMO A CAPIRE L'INTENSITA' DEL BIAS TRA DIVERSE VARIABILI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSi_lI_xuvO"
      },
      "source": [
        "## Metriche Nuove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7H7AQn_u3pW"
      },
      "source": [
        "## Alcune Definizioni \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **True Positives** (TP): Items where the true label is positive and whose class is correctly predicted to be positive.\n",
        "*  **False Positives** (FP): Items where the true label is negative and whose class is incorrectly predicted to be positive\n",
        "*  **True Negatives** (N): Items where the true label is negative and whose class is correctly predicted to be negative.\n",
        "*  **False Negatives** (FN): Items where the true label is positive and whose class is incorrectly predicted to be negative.\n",
        "\n",
        "* **False Positive Rate**, or *Type I Error*: Number of items wrongly identified as positive out of the total actual negatives — FP/(FP+TN) - This error means that an image not containing a particular parasite egg is incorrectly labeled as having it\n",
        "* **False Negative Rate**, or *Type II Error*: Number of items wrongly identified as negative out of the total actual positives — FN/(FN+TP). This metric is especially important to us, as it tells us the frequency with which a particular parasite egg is not classified correctly\n",
        "\n",
        "-------------\n",
        "\n",
        "* **Statistical Parity Difference**\n",
        "This measure is based on the following formula :\n",
        "𝑃𝑟(𝑌=1|𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑)−𝑃𝑟(𝑌=1|𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑) Here the bias or statistical imparity is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1. So it has to be close to 0 so it will be fair.\n",
        "\n",
        "*  **Equal Opportunity Difference** This metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula - 𝑇𝑃𝑅𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑−𝑇𝑃𝑅𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑 Same as the previous metric we need it to be close to 0.\n",
        "\n",
        "* **demographic parity** A fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n",
        "\n",
        "* **equality of opportunity** A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership."
      ],
      "metadata": {
        "id": "Yn0Ddk19pBcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codice Metriche Nuove"
      ],
      "metadata": {
        "id": "47nJxdrJppg9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7dJJO7ixuJ7"
      },
      "source": [
        "!pip install fairlearn \n",
        "from fairlearn.metrics import selection_rate\n",
        "from fairlearn.metrics import true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate\n",
        "from fairlearn.metrics import equalized_odds_difference\n",
        "\n",
        "import sklearn as sk\n",
        "\n",
        "\n",
        "#---- metriche lisa ----#\n",
        "y_true = testgen.df['class'].to_numpy()\n",
        "SR = selection_rate(y_true, y_pred, pos_label=1, sample_weight=None)\n",
        "print('selection_rate : {}' . format(SR))\n",
        "\n",
        "\n",
        "#Per quanto riguarda AO come metrica, potremo utilizzare i risultati della confusion matrix ?\n",
        "#LINK : https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "#LINK : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n",
        "#print('Unique Element Y_test : {}'.format(np.unique(y_test)))\n",
        "#print('Unique Element Y_pred : {}'.format(np.unique(y_pred)))\n",
        "#print('True_Positive_Rate : {}'.format(true_positive_rate(y_true, y_pred)))\n",
        "\n",
        "FP = cm.sum(axis=0) - np.diag(cm)  \n",
        "FN = cm.sum(axis=1) - np.diag(cm)\n",
        "TP = np.diag(cm)\n",
        "TN = cm.sum() - (FP + FN + TP)\n",
        "\n",
        "# Sensitivity, hit rate, recall, or true positive rate\n",
        "TPR = TP/(TP+FN)\n",
        "print('TPR : {}'.format(TPR))\n",
        "# Specificity or true negative rate\n",
        "TNR = TN/(TN+FP) \n",
        "print('TNR : {}'.format(TNR))\n",
        "# Precision or positive predictive value\n",
        "PPV = TP/(TP+FP)\n",
        "print('PPV : {}'.format(PPV))\n",
        "# Negative predictive value\n",
        "NPV = TN/(TN+FN)\n",
        "print('NPV : {}'.format(NPV))\n",
        "# Fall out or false positive rate\n",
        "FPR = FP/(FP+TN)\n",
        "print('FPR : {}'.format(FPR))\n",
        "# False negative rate\n",
        "FNR = FN/(TP+FN)\n",
        "print('FNR : {}'.format(FNR))\n",
        "# False discovery rate\n",
        "FDR = FP/(TP+FP)\n",
        "print('FDR : {}'.format(FDR))\n",
        "\n",
        "# Overall accuracy\n",
        "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "print('Accuracy : {}'.format(ACC))\n",
        "\n",
        "\n",
        "AO = 0.5*(\n",
        "    (TPR[0] + FPR[0]) - \n",
        "    (TPR[1] + FPR[1]) + \n",
        "    (TPR[2] + FPR[2]) - \n",
        "    (TPR[3] + FPR[3]) +\n",
        "    (TPR[4] + FPR[4]) -\n",
        "    (TPR[5] + FPR[5]) +\n",
        "    (TPR[6] + FPR[6]) -\n",
        "    (TPR[7] + FPR[7]) +\n",
        "    (TPR[8] + FPR[8]) -\n",
        "    (TPR[9] + FPR[9]))\n",
        "\n",
        "print('AO : {}'.format(AO))\n",
        "#y_true= y_true.reshape(1,-1)\n",
        "#y_pred= y_pred.reshape(-1,1)\n",
        "#print(y_true.shape)\n",
        "#print(y_pred.shape)\n",
        "\n",
        "\n",
        "'''FORSE QUA RIUSCIAMO A TROVARE UN ESEMPIO DI APPLICAZIONE DEL METODO'''\n",
        "'''https://deepnote.com/@Machine-Learning-2/Miniproject-z523fGqWSSu7QV34n_u7OA'''\n",
        "'''https://fairlearn.org/main/user_guide/assessment.html'''\n",
        "\n",
        "\n",
        "EO =(TPR[0] - TPR[1] + TPR[2] - TPR[3] + TPR[4] - TPR[5] + TPR[6] - TPR[7] + TPR[8] - FPR[9]) \n",
        "print('EO : {}' . format(EO))\n",
        "\n",
        "\n",
        "#Demographic parity\n",
        "'''\n",
        "Demographic parity is one of the most popular fairness indicators in the literature. \n",
        "Demographic parity is achieved if the absolute number of positive predictions \n",
        "in the subgroups are close to each other. This measure does not take true class into\n",
        "consideration and only depends on the model predictions. In some literature, \n",
        "demographic parity is also referred to as statsictal parity or independence.\n",
        "'''\n",
        "DP = (TP + FP)\n",
        "print('Demographic parity : {}' . format(DP))\n",
        "\n",
        "#Equalized odds\n",
        "'''\n",
        "Equalized odds, also known as separation, are achieved if the sensitivities in the \n",
        "subgroups are close to each other. The group-specific sensitivities \n",
        "indicate the number of the true positives divided by the total \n",
        "number of positives in that group.\n",
        "'''\n",
        "Equalized_Odds = TP / (TP + FN)\n",
        "print('Equalized Odds : {}' . format(Equalized_Odds))\n",
        "\n",
        "\n",
        "##---- Link Riccardo ----##\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n",
        "\n",
        "\n",
        "Balanced_Accuracy = sk.metrics.balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n",
        "print('Balanced Accuracy Generale : {0:0.4f}' . format(Balanced_Accuracy))\n",
        "\n",
        "\n",
        "#####----------- PER CIASCUNA SERIE BALANCED ACCURACY -----------####\n",
        "test_array_series = [\n",
        "                     test_array_s0, \n",
        "                     test_array_s1,\n",
        "                     test_array_s2,\n",
        "                     test_array_s3,\n",
        "                     test_array_s4,\n",
        "                     test_array_s5,\n",
        "                     test_array_s6,\n",
        "                     test_array_s7,\n",
        "                     test_array_s8,\n",
        "                     test_array_s9,\n",
        "                     test_array_s10,\n",
        "                     test_array_s11,\n",
        "                     test_array_s12\n",
        "                     ]\n",
        "\n",
        "pred_array_series = [\n",
        "                     pred_array_s0, \n",
        "                     pred_array_s1,\n",
        "                     pred_array_s2,\n",
        "                     pred_array_s3,\n",
        "                     pred_array_s4,\n",
        "                     pred_array_s5,\n",
        "                     pred_array_s6,\n",
        "                     pred_array_s7,\n",
        "                     pred_array_s8,\n",
        "                     pred_array_s9,\n",
        "                     pred_array_s10,\n",
        "                     pred_array_s11,\n",
        "                     pred_array_s12\n",
        "                     ]\n",
        "\n",
        "SUM_BA = 0\n",
        "for ba_i in range (13):\n",
        "  BA = sk.metrics.balanced_accuracy_score(test_array_series[ba_i], pred_array_series[ba_i], sample_weight=None, adjusted=False)\n",
        "  SUM_BA += BA\n",
        "  print('Balanced Accuracy Series {0} : {1:0.4f}' . format(ba_i, BA))\n",
        "\n",
        "\n",
        "#----------- MEDIA DELLE BALANCED ACCURACY ---------------\n",
        "Average = SUM_BA/12\n",
        "print('Average Balanced Accuracy : {0:0.4f}' . format(Average))\n",
        " \n",
        "\n",
        "##---- Wodsworth et Al ----# \n",
        "#HIGH_RISK_GAP = SP #modulo o cardinalità \n",
        "\n",
        "#FN_GAP = false_negative (s1) - false negative (s2) \n",
        "#FN_GAP = (false_negative_rate(y_true, y_pred) - false_negative_rate(y_true, y_pred))  #modulo o cardinalità\n",
        "  \n",
        "#FN_GAP = false_negative (s1) - false negative (s2) \n",
        "#FP_GAP = (false_positive_rate(y_true, y_pred) - false_positive_rate(y_true, y_pred))  #modulo o cardinalità\n",
        "\n",
        "### LINK UTILE ####\n",
        "#https://www.kaggle.com/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}