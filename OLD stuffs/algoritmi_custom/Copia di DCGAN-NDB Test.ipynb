{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"Copia di DCGAN-NDB Test.ipynb","provenance":[{"file_id":"1fGrFl5UzYc3upShr25Hv8VfqyzhZOPTM","timestamp":1651573021029}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"RT_dDz2Y1FdN"},"source":["CelebA dataset: https://www.kaggle.com/jessicali9530/celeba-dataset\n","\n","Note: The dataset and notebook can be directly loaded into a Kaggle kernel."]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"_D64r_1t0xY8"},"source":["from __future__ import print_function\n","#%matplotlib inline\n","import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import time\n","import gc\n","\n","# Set random seed for reproducibility\n","manualSeed = 999\n","#manualSeed = random.randint(1, 10000) # use if you want new results\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","np.random.seed(999)\n","\n","#Install + import Weights and Biases\n","!pip install --upgrade wandb\n","import wandb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"d_aIM8g_0xY_"},"source":["import os\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from scipy.stats import norm\n","from matplotlib import pyplot as plt\n","import pickle as pkl\n","\n","class NDB:\n","    def __init__(self, training_data=None, number_of_bins=100, significance_level=0.05, z_threshold=None,\n","                 whitening=False, max_dims=None, cache_folder=None):\n","        \"\"\"\n","        NDB Evaluation Class\n","        :param training_data: Optional - the training samples - array of m x d floats (m samples of dimension d)\n","        :param number_of_bins: Number of bins (clusters) default=100\n","        :param significance_level: The statistical significance level for the two-sample test\n","        :param z_threshold: Allow defining a threshold in terms of difference/SE for defining a bin as statistically different\n","        :param whitening: Perform data whitening - subtract mean and divide by per-dimension std\n","        :param max_dims: Max dimensions to use in K-means. By default derived automatically from d\n","        :param bins_file: Optional - file to write / read-from the clusters (to avoid re-calculation)\n","        \"\"\"\n","        \n","        #Added\n","        self.original_bin_centers = None\n","        self.count = None\n","        self.bin_order = None\n","        \n","        #Original\n","        self.number_of_bins = number_of_bins\n","        self.significance_level = significance_level\n","        self.z_threshold = z_threshold\n","        self.whitening = whitening\n","        self.ndb_eps = 1e-6\n","        self.training_mean = 0.0\n","        self.training_std = 1.0\n","        self.max_dims = max_dims\n","        self.cache_folder = cache_folder\n","        self.bin_centers = None\n","        self.bin_proportions = None\n","        self.ref_sample_size = None\n","        self.used_d_indices = None\n","        self.results_file = None\n","        self.test_name = 'ndb_{}_bins_{}'.format(self.number_of_bins, 'whiten' if self.whitening else 'orig')\n","        self.cached_results = {}\n","        if self.cache_folder:\n","            self.results_file = os.path.join(cache_folder, self.test_name+'_results.pkl')\n","            if os.path.isfile(self.results_file):\n","                # print('Loading previous results from', self.results_file, ':')\n","                self.cached_results = pkl.load(open(self.results_file, 'rb'))\n","                # print(self.cached_results.keys())\n","        if training_data is not None or cache_folder is not None:\n","                bins_file = None\n","                if cache_folder:\n","                    os.makedirs(cache_folder, exist_ok=True)\n","                    bins_file = os.path.join(cache_folder, self.test_name+'.pkl')\n","                self.construct_bins(training_data, bins_file)\n","\n","    def construct_bins(self, training_samples, bins_file):\n","        \"\"\"\n","        Performs K-means clustering of the training samples\n","        :param training_samples: An array of m x d floats (m samples of dimension d)\n","        \"\"\"\n","\n","        if self.__read_from_bins_file(bins_file):\n","            return\n","        n, d = training_samples.shape\n","        k = self.number_of_bins\n","        if self.whitening:\n","            self.training_mean = np.mean(training_samples, axis=0)\n","            self.training_std = np.std(training_samples, axis=0) + self.ndb_eps\n","\n","        if self.max_dims is None and d > 1000:\n","            # To ran faster, perform binning on sampled data dimension (i.e. don't use all channels of all pixels)\n","            self.max_dims = d//6\n","\n","        whitened_samples = (training_samples-self.training_mean)/self.training_std\n","        d_used = d if self.max_dims is None else min(d, self.max_dims)\n","        self.used_d_indices = np.random.choice(d, d_used, replace=False)\n","\n","        print('Performing K-Means clustering of {} samples in dimension {} / {} to {} clusters ...'.format(n, d_used, d, k))\n","        print('Can take a couple of minutes...')\n","        if n//k > 1000:\n","            print('Training data size should be ~500 times the number of bins (for reasonable speed and accuracy)')\n","\n","        clusters = KMeans(n_clusters=k, max_iter=100, n_jobs=-1).fit(whitened_samples[:, self.used_d_indices])\n","\n","        bin_centers = np.zeros([k, d])\n","        for i in range(k):\n","            bin_centers[i, :] = np.mean(whitened_samples[clusters.labels_ == i, :], axis=0)\n","        \n","        self.original_bin_centers = bin_centers\n","        #print(\"Bin centers: \", bin_centers.shape)\n","        # Organize bins by size (largest bin -> smallest bin)\n","        label_vals, label_counts = np.unique(clusters.labels_, return_counts=True)\n","        self.count = list(zip(label_vals, label_counts))\n","        self.count.sort(key=lambda tup: tup[1], reverse=True)\n","        bin_order = np.argsort(-label_counts)\n","        self.bin_order = bin_order\n","        self.bin_proportions = label_counts[bin_order] / np.sum(label_counts)\n","        self.bin_centers = bin_centers[bin_order, :]\n","        self.ref_sample_size = n\n","        self.__write_to_bins_file(bins_file)\n","        print('Done.')\n","\n","    def evaluate(self, query_samples, model_label=None):\n","        \"\"\"\n","        Assign each sample to the nearest bin center (in L2). Pre-whiten if required. and calculate the NDB\n","        (Number of statistically Different Bins) and JS divergence scores.\n","        :param query_samples: An array of m x d floats (m samples of dimension d)\n","        :param model_label: optional label string for the evaluated model, allows plotting results of multiple models\n","        :return: results dictionary containing NDB and JS scores and array of labels (assigned bin for each query sample)\n","        \"\"\"\n","        n = query_samples.shape[0]\n","        query_bin_proportions, query_bin_assignments = self.__calculate_bin_proportions(query_samples)\n","        # print(query_bin_proportions)\n","        different_bins = NDB.two_proportions_z_test(self.bin_proportions, self.ref_sample_size, query_bin_proportions,\n","                                                    n, significance_level=self.significance_level,\n","                                                    z_threshold=self.z_threshold)\n","        ndb = np.count_nonzero(different_bins)\n","        js = NDB.jensen_shannon_divergence(self.bin_proportions, query_bin_proportions)\n","        results = {'NDB': ndb,\n","                   'JS': js,\n","                   'Proportions': query_bin_proportions,\n","                   'N': n,\n","                   'Bin-Assignment': query_bin_assignments,\n","                   'Different-Bins': different_bins}\n","\n","        if model_label:\n","            print('Results for {} samples from {}: '.format(n, model_label), end='')\n","            self.cached_results[model_label] = results\n","            if self.results_file:\n","                # print('Storing result to', self.results_file)\n","                pkl.dump(self.cached_results, open(self.results_file, 'wb'))\n","\n","        print('NDB =', ndb, 'NDB/K =', ndb/self.number_of_bins, ', JS =', js)\n","        return results\n","\n","    def print_results(self):\n","        print('NSB results (K={}{}):'.format(self.number_of_bins, ', data whitening' if self.whitening else ''))\n","        for model in sorted(list(self.cached_results.keys())):\n","            res = self.cached_results[model]\n","            print('%s: NDB = %d, NDB/K = %.3f, JS = %.4f' % (model, res['NDB'], res['NDB']/self.number_of_bins, res['JS']))\n","\n","    def plot_results(self, models_to_plot=None):\n","        \"\"\"\n","        Plot the binning proportions of different methods\n","        :param models_to_plot: optional list of model labels to plot\n","        \"\"\"\n","        K = self.number_of_bins\n","        w = 1.0 / (len(self.cached_results)+1)\n","        assert K == self.bin_proportions.size\n","        assert self.cached_results\n","\n","        # Used for plotting only\n","        def calc_se(p1, n1, p2, n2):\n","            p = (p1 * n1 + p2 * n2) / (n1 + n2)\n","            return np.sqrt(p * (1 - p) * (1/n1 + 1/n2))\n","\n","        if not models_to_plot:\n","            models_to_plot = sorted(list(self.cached_results.keys()))\n","\n","        # Visualize the standard errors using the train proportions and size and query sample size\n","        train_se = calc_se(self.bin_proportions, self.ref_sample_size,\n","                           self.bin_proportions, self.cached_results[models_to_plot[0]]['N'])\n","        plt.bar(np.arange(0, K)+0.5, height=train_se*2.0, bottom=self.bin_proportions-train_se,\n","                width=1.0, label='Train$\\pm$SE', color='gray')\n","\n","        ymax = 0.0\n","        for i, model in enumerate(models_to_plot):\n","            results = self.cached_results[model]\n","            label = '%s (%i : %.4f)' % (model, results['NDB'], results['JS'])\n","            ymax = max(ymax, np.max(results['Proportions']))\n","            if K <= 70:\n","                plt.bar(np.arange(0, K)+(i+1.0)*w, results['Proportions'], width=w, label=label)\n","            else:\n","                plt.plot(np.arange(0, K)+0.5, results['Proportions'], '--*', label=label)\n","        plt.legend(loc='best')\n","        plt.ylim((0.0, min(ymax, np.max(self.bin_proportions)*4.0)))\n","        plt.grid(True)\n","        plt.title('Binning Proportions Evaluation Results for {} bins (NDB : JS)'.format(K))\n","        plt.show()\n","\n","    def __calculate_bin_proportions(self, samples):\n","        if self.bin_centers is None:\n","            print('First run construct_bins on samples from the reference training data')\n","        assert samples.shape[1] == self.bin_centers.shape[1]\n","        n, d = samples.shape\n","        k = self.bin_centers.shape[0]\n","        D = np.zeros([n, k], dtype=samples.dtype)\n","\n","        print('Calculating bin assignments for {} samples...'.format(n))\n","        whitened_samples = (samples-self.training_mean)/self.training_std\n","        for i in range(k):\n","            print('.', end='', flush=True)\n","            D[:, i] = np.linalg.norm(whitened_samples[:, self.used_d_indices] - self.bin_centers[i, self.used_d_indices],\n","                                     ord=2, axis=1)\n","        print()\n","        labels = np.argmin(D, axis=1)\n","        probs = np.zeros([k])\n","        label_vals, label_counts = np.unique(labels, return_counts=True)\n","        probs[label_vals] = label_counts / n\n","        return probs, labels\n","\n","    def __read_from_bins_file(self, bins_file):\n","        if bins_file and os.path.isfile(bins_file):\n","            print('Loading binning results from', bins_file)\n","            bins_data = pkl.load(open(bins_file,'rb'))\n","            self.bin_proportions = bins_data['proportions']\n","            self.bin_centers = bins_data['centers']\n","            self.ref_sample_size = bins_data['n']\n","            self.training_mean = bins_data['mean']\n","            self.training_std = bins_data['std']\n","            self.used_d_indices = bins_data['d_indices']\n","            return True\n","        return False\n","\n","    def __write_to_bins_file(self, bins_file):\n","        if bins_file:\n","            print('Caching binning results to', bins_file)\n","            bins_data = {'proportions': self.bin_proportions,\n","                         'centers': self.bin_centers,\n","                         'n': self.ref_sample_size,\n","                         'mean': self.training_mean,\n","                         'std': self.training_std,\n","                         'd_indices': self.used_d_indices}\n","            pkl.dump(bins_data, open(bins_file, 'wb'))\n","\n","    @staticmethod\n","    def two_proportions_z_test(p1, n1, p2, n2, significance_level, z_threshold=None):\n","        # Per http://stattrek.com/hypothesis-test/difference-in-proportions.aspx\n","        # See also http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/binotest.htm\n","        p = (p1 * n1 + p2 * n2) / (n1 + n2)\n","        se = np.sqrt(p * (1 - p) * (1/n1 + 1/n2))\n","        z = (p1 - p2) / se\n","        # Allow defining a threshold in terms as Z (difference relative to the SE) rather than in p-values.\n","        if z_threshold is not None:\n","            return abs(z) > z_threshold\n","        p_values = 2.0 * norm.cdf(-1.0 * np.abs(z))    # Two-tailed test\n","        return p_values < significance_level\n","\n","    @staticmethod\n","    def jensen_shannon_divergence(p, q):\n","        \"\"\"\n","        Calculates the symmetric Jensen–Shannon divergence between the two PDFs\n","        \"\"\"\n","        m = (p + q) * 0.5\n","        return 0.5 * (NDB.kl_divergence(p, m) + NDB.kl_divergence(q, m))\n","\n","    @staticmethod\n","    def kl_divergence(p, q):\n","        \"\"\"\n","        The Kullback–Leibler divergence.\n","        Defined only if q != 0 whenever p != 0.\n","        \"\"\"\n","        assert np.all(np.isfinite(p))\n","        assert np.all(np.isfinite(q))\n","        assert not np.any(np.logical_and(p != 0, q == 0))\n","\n","        p_pos = (p > 0)\n","        return np.sum(p[p_pos] * np.log(p[p_pos] / q[p_pos]))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"p7OFjE7O0xZD"},"source":["def test_ndb():  \n","    dim=100\n","    k=50\n","    n_train = k*100\n","    n_test = k*10\n","\n","    train_samples = np.random.uniform(size=[n_train, dim])\n","    ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\n","\n","    test_samples = np.random.uniform(high=1.0, size=[n_test, dim])\n","    results = ndb.evaluate(test_samples, model_label='Test')\n","    print(results['Bin-Assignment'])\n","    print\n","    print(ndb.bin_order)\n","    print()\n","    print(ndb.count)\n","    print()\n","    unique, counts = np.unique(results[\"Bin-Assignment\"], return_counts=True)\n","    zipped = list(zip(unique, counts))\n","    zipped.sort(key=lambda tup: tup[1], reverse=True)\n","    print(zipped)\n","\n","    \n","test_ndb()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"0Zk6S1ZR0xZG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"GdqSJMK70xZJ"},"source":["wandb.init(entity=\"authors\", project=\"DCGAN-ndb-test\")\n","\n","# Root directory for dataset\n","dataroot = \"/kaggle/input/celeba-dataset/img_align_celeba\"\n","\n","# Number of workers for dataloader\n","workers = 2\n","\n","# Batch size during training\n","batch_size = 128\n","\n","# Spatial size of training images. All images will be resized to this\n","#   size using a transformer.\n","image_size = 64\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector (i.e. size of generator input)\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 64\n","\n","# Size of feature maps in discriminator\n","ndf = 64\n","\n","# Number of training epochs\n","num_epochs = 5\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# Beta1 hyperparam for Adam optimizers\n","beta1 = 0.5\n","\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1\n","\n","#NBD Scoring\n","\n","numTrainBatches = 156  \n","\n","numTestBatches = 39\n","\n","#number of bins for NBD\n","k = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"iiLJqwje0xZM"},"source":["dataset = dset.ImageFolder(root=dataroot,\n","                           transform=transforms.Compose([\n","                               transforms.Resize(image_size),\n","                               transforms.CenterCrop(image_size),\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                           ]))\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                         shuffle=True, num_workers=workers)\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","\n","# Plot some training images\n","real_batch = next(iter(dataloader))\n","plt.figure(figsize=(8,8))\n","plt.axis(\"off\")\n","plt.title(\"Training Images\")\n","plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"QNOKv2Qn0xZP"},"source":["#real_batch[0].permute(1,2,0).cpu().numpy().astype(np.float64)\n","img = real_batch[0][1].permute(1,2,0).cpu().numpy().astype(np.float64)\n","image = ((img * 0.5) + 0.5)\n","# plt.imshow(image)\n","# plt.show()\n","# wdb_img = wandb.Image(image)\n","# wandb.log({\"test\": wdb_img})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"pf6_hpEL0xZU"},"source":["# custom weights initialization called on netG and netD\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"h9B69Y-B0xZW"},"source":["# Generator Code\n","\n","class Generator(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Generator, self).__init__()\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","            # input is Z, going into a convolution\n","            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            # state size. (ngf*8) x 4 x 4\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            # state size. (ngf*4) x 8 x 8\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            # state size. (ngf*2) x 16 x 16\n","            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            # state size. (ngf) x 32 x 32\n","            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","            # state size. (nc) x 64 x 64\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Rc3W2tWd0xZY"},"source":["# Create the generator\n","netG = Generator(ngpu).to(device)\n","\n","# Handle multi-gpu if desired\n","if (device.type == 'cuda') and (ngpu > 1):\n","    netG = nn.DataParallel(netG, list(range(ngpu)))\n","\n","# Apply the weights_init function to randomly initialize all weights\n","#  to mean=0, stdev=0.2.\n","netG.apply(weights_init)\n","\n","# Print the model\n","print(netG)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"yIAcCut-0xZa"},"source":["class Discriminator(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Discriminator, self).__init__()\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","            # input is (nc) x 64 x 64\n","            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf) x 32 x 32\n","            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*2) x 16 x 16\n","            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*4) x 8 x 8\n","            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*8) x 4 x 4\n","            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"6GYWM-Y90xZc"},"source":["# Create the Discriminator\n","netD = Discriminator(ngpu).to(device)\n","\n","# Handle multi-gpu if desired\n","if (device.type == 'cuda') and (ngpu > 1):\n","    netD = nn.DataParallel(netD, list(range(ngpu)))\n","\n","# Apply the weights_init function to randomly initialize all weights\n","#  to mean=0, stdev=0.2.\n","netD.apply(weights_init)\n","\n","# Print the model\n","print(netD)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"kmUfu95h0xZf"},"source":["# Initialize BCELoss function\n","criterion = nn.BCELoss()\n","\n","# Create batch of latent vectors that we will use to visualize\n","#  the progression of the generator\n","fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n","\n","# Establish convention for real and fake labels during training\n","real_label = 1\n","fake_label = 0\n","\n","# Setup Adam optimizers for both G and D\n","optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"-O83Ws-F0xZh"},"source":["def real_samples(numTrainBatches, dataloader):\n","    #Get real samples (to reduce training time take 40% of original data - 80000 samples)\n","\n","    real_batches = []\n","    for i, data in enumerate(dataloader, 0):\n","\n","        if(i >= numTrainBatches):\n","            break\n","\n","        real = data[0].numpy()\n","        real_batches.append(real)\n","\n","    real_batches = np.array(real_batches)\n","\n","    print(len(real_batches))\n","    #Display a sample\n","    plt.imshow(np.transpose(real_batches[0][0], (1,2,0)))\n","\n","    real_combined = real_batches.reshape(numTrainBatches*batch_size, nc, image_size, image_size)\n","    del real_batches\n","    gc.collect()\n","    \n","    return real_combined\n","\n","def generated_fakes(numTestBatches, netG):\n","    #Generate fake samples for testing (about 10% of total data - 20000 samples)\n","\n","    generated_batches = []\n","    for i in range(numTestBatches):\n","        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n","        # Generate fake image batch with G\n","        fake = netG(noise).detach().cpu().numpy()\n","        generated_batches.append(fake)\n","\n","    generated_batches = np.array(generated_batches)\n","\n","    print(len(generated_batches))\n","    #Display a sample\n","    plt.imshow(np.transpose(generated_batches[0][0], (1,2,0)))\n","\n","\n","    # gen_combined = generated_batches[0]\n","    # for i in range(1,len(generated_batches)):\n","    #     gen_combined = np.concatenate((gen_combined, generated_batches[i]))\n","    gen_combined = generated_batches.reshape(numTestBatches*batch_size, nc, image_size, image_size)\n","    del generated_batches\n","    gc.collect()\n","    \n","    return gen_combined"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"qZSiGrxK0xZj"},"source":["real_combined = real_samples(numTrainBatches, dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"D_Q9VDoA0xZl"},"source":["# Training Loop\n","\n","# Lists to keep track of progress\n","img_list = []\n","G_losses = []\n","D_losses = []\n","metrics = [] \n","ndb_scores = []\n","iters = 0\n","\n","start = time.time()\n","print(\"Starting Training Loop...\")\n","# For each epoch\n","for epoch in range(num_epochs):\n","    # For each batch in the dataloader\n","    for i, data in enumerate(dataloader, 0):\n","            \n","        ############################\n","        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","        ###########################\n","        ## Train with all-real batch\n","        netD.zero_grad()\n","        # Format batch\n","        real_cpu = data[0].to(device)\n","        b_size = real_cpu.size(0) \n","        label = torch.full((b_size,), real_label, device=device)\n","        # Forward pass real batch through D\n","        output = netD(real_cpu).view(-1)\n","        # Calculate loss on all-real batch\n","        errD_real = criterion(output, label)\n","        # Calculate gradients for D in backward pass\n","        errD_real.backward()\n","        D_x = output.mean().item()\n","\n","        ## Train with all-fake batch\n","        # Generate batch of latent vectors\n","        noise = torch.randn(b_size, nz, 1, 1, device=device)\n","        # Generate fake image batch with G\n","        fake = netG(noise)\n","        label.fill_(fake_label)\n","        # Classify all fake batch with D\n","        output = netD(fake.detach()).view(-1)\n","        # Calculate D's loss on the all-fake batch\n","        errD_fake = criterion(output, label)\n","        # Calculate the gradients for this batch\n","        errD_fake.backward()\n","        D_G_z1 = output.mean().item()\n","        # Add the gradients from the all-real and all-fake batches\n","        errD = errD_real + errD_fake\n","        # Update D\n","        optimizerD.step()\n","\n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        netG.zero_grad()\n","        label.fill_(real_label)  # fake labels are real for generator cost\n","        # Since we just updated D, perform another forward pass of all-fake batch through D\n","        output = netD(fake).view(-1)\n","        # Calculate G's loss based on this output\n","        errG = criterion(output, label)\n","        # Calculate gradients for G\n","        errG.backward()\n","        D_G_z2 = output.mean().item()\n","        # Update G\n","        optimizerG.step()\n","\n","        # Output training stats\n","        if i % 50 == 0:\n","            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n","                  % (epoch, num_epochs, i, len(dataloader),\n","                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n","\n","        # Save Losses for plotting later\n","        G_losses.append(errG.item())\n","        D_losses.append(errD.item())\n","\n","        # Check how the generator is doing by saving G's output on fixed_noise\n","        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n","            with torch.no_grad():\n","                fake = netG(fixed_noise).detach().cpu()\n","            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n","            \n","            gen_combined = generated_fakes(numTestBatches, netG)\n","            \n","            #Calculate NDB\n","            train_size = numTrainBatches*batch_size\n","            test_size = numTestBatches*batch_size\n","            dim = image_size*image_size*nc\n","\n","            train_samples = real_combined.reshape(train_size, dim)\n","            test_samples = gen_combined.reshape(test_size, dim)\n","            \n","            ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\n","            results = ndb.evaluate(test_samples)\n","                \n","            ndb_k = float(results[\"NDB\"])/ndb.number_of_bins\n","            wandb.log({\"ndb_k\" : ndb_k, \"JS\": results[\"JS\"]})\n","            \n","            \n","            #specifically track NDB\n","            metrics.append(results)\n","            ndb_scores.append(ndb_k)\n","            print(\"NDB_K: \", ndb_k)\n","\n","\n","        iters += 1\n","\n","end = time.time()\n","print(\"Time taken in seconds: \", end - start)\n","\n","torch.save(netD.state_dict(), 'netD.pth')\n","wandb.save('netD.pth')\n","torch.save(netG.state_dict(), 'netG.pth')\n","wandb.save('netG.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n-LKuNRT0xZn"},"source":["# After Training\n"]},{"cell_type":"code","metadata":{"trusted":true,"id":"gQY7hwpe0xZn"},"source":["\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","plt.plot(G_losses,label=\"G\")\n","plt.plot(D_losses,label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()\n","\n","#%%capture\n","fig = plt.figure(figsize=(8,8))\n","plt.axis(\"off\")\n","ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n","ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n","\n","HTML(ani.to_jshtml())\n","\n","# Grab a batch of real images from the dataloader\n","real_batch = next(iter(dataloader))\n","\n","# Plot the real images\n","plt.figure(figsize=(15,15))\n","plt.subplot(1,2,1)\n","plt.axis(\"off\")\n","plt.title(\"Real Images\")\n","plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n","\n","# Plot the fake images from the last epoch\n","plt.subplot(1,2,2)\n","plt.axis(\"off\")\n","plt.title(\"Fake Images\")\n","plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TjcWiXOq0xZq"},"source":["Load Models (if already trained)"]},{"cell_type":"code","metadata":{"trusted":true,"id":"8LUiIMq30xZq"},"source":["#Load Models\n","# netD = Discriminator(ngpu)\n","# netD.load_state_dict(torch.load('../input/dcgan-celeba-good/netD.pth'))\n","# netD = netD.to(device)\n","\n","# netG = Generator(ngpu)\n","# netG.load_state_dict(torch.load('../input/dcgan-celeba-good/netG.pth'))\n","# netG = netG.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TybKe3R80xZs"},"source":["**NBD Testing**"]},{"cell_type":"code","metadata":{"trusted":true,"id":"VjUP-mCa0xZt"},"source":["#Delete models to save Kaggle/Colab memory\n","del dataloader\n","del netD\n","torch.cuda.empty_cache()\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"IXmLLMoQ0xZv"},"source":["gen_combined = generated_fakes(numTestBatches, netG)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"mjAMw9pu0xZy"},"source":["del netG\n","torch.cuda.empty_cache()\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"lu9XjPhB0xZ0"},"source":["\n","print(real_combined.shape)\n","print(gen_combined.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"E62mwpco0xZ1"},"source":["#put into format to be fed into NBD\n","train_size = numTrainBatches*batch_size\n","test_size = numTestBatches*batch_size\n","dim = image_size*image_size*nc\n","\n","train_samples = real_combined.reshape(train_size, dim)\n","test_samples = gen_combined.reshape(test_size, dim)\n","print(train_samples.shape)\n","print(test_samples.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"9HU8ae3a0xZ4"},"source":["ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\n","results = ndb.evaluate(test_samples, model_label='Test')\n","ndb.plot_results(models_to_plot=['Test'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"IFggctb10xZ8"},"source":["\n","#Print/log out generated images in a bin\n","def get_generated(bin_id, results, num_imgs):\n","    indices = [i for i, x in enumerate(results[\"Bin-Assignment\"]) if x == bin_id]\n","    for i in range(len(indices[:num_imgs])):\n","        img = test_samples[indices[i]]\n","        img = img.reshape((nc, image_size, image_size))\n","        img = np.transpose(img,(1,2,0)).astype(np.float64)\n","\n","        plt.imshow(img)\n","        plt.show()\n","\n","#         wdb_img = wandb.Image(img)\n","#         wandb.log({\"bin_\" + str(bin_id): wdb_img})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"8Iesx2VX0xZ-"},"source":["\n","#print out smallest bin centroid\n","#center\n","small_center = ndb.original_bin_centers[ndb.bin_order[-1]]\n","small_center = small_center.reshape((nc, image_size, image_size))\n","small_center = np.transpose(small_center,(1,2,0)).astype(np.float64)\n","small_center = ((small_center * 0.5) + 0.5)\n","wdb_img = wandb.Image(small_center)\n","wandb.log({\"smallest_bin_center\": wdb_img})\n","plt.imshow(small_center)\n","plt.show()\n","\n","#print out generated images in smallest bin\n","print(\"Smallest Bin Generated Images:\")\n","get_generated(ndb.bin_order[-1], results, 10)\n","\n","\n","#print out largest bin centroid\n","#center\n","large_center = ndb.original_bin_centers[ndb.bin_order[0]]\n","large_center = large_center.reshape((nc, image_size, image_size))\n","large_center = np.transpose(large_center,(1,2,0)).astype(np.float64)\n","large_center = ((large_center * 0.5) + 0.5)\n","wdb_img = wandb.Image(large_center)\n","wandb.log({\"largest_bin_center\": wdb_img})\n","plt.imshow(large_center)\n","plt.show()\n","\n","#print out generated images in largest bin\n","print(\"Largest Bin Generated Images:\")\n","get_generated(ndb.bin_order[0], results, 10)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"DZNrl7Ij0xaA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"wbsFhITI0xaC"},"source":[""],"execution_count":null,"outputs":[]}]}